{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "zHBd9aAbdoOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BJ5hFeMWHm9",
        "outputId": "06054061-67c5-41d3-f70f-ac5ed310cbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 30 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.13.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=6a31b070c7645a6244fe2d2ceeee90a2d239e2acdde8f350a9a630d0c919bf57\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ],
      "source": [
        "# pip install 창! 그냥 실행시키면 됩니다.\n",
        "\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install tensorflow==1.15"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "%load_ext tensorboard\n",
        "#텐서보드 로드 명령어\n"
      ],
      "metadata": {
        "id": "HoqcFOlxWQ9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글드라이브를 파일 로드 기본경로로 만드는 명령어\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiE9B6bVWXe5",
        "outputId": "b33af630-02dd-404b-ef50-486cc1a51070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개념정리\n",
        "- CNN : 이미지 분류가 Main! 다만, 최근에 자연어 처리에서도 성능을 발휘한단 논문 많이 발표되고 있어\n",
        "- 이미지에서 필요한 데이터와 필드를 수집하는 레이어\n",
        "- 앞서 학습하는 레이어는 FC로 명명\n",
        "- CNN은 두 가지 레이어로 학습하는 것이라 이해하면 된다.\n",
        "\n",
        "## Stride\n",
        "- stride : 이미지 필터가 이동하는 간격을 의미하며, 학습할 이미지의 전체 크기를 결정한다.\n",
        "- 이미지 크기가 항상 가로, 세로가 동일해야한다. 다르다면? 이미지의 크기를 맞춰줘야한다.\n",
        "- image size = 이미지 전체 픽셀 크기 - 이미지 필터 크기 / stride + 1\n",
        "\n",
        "## Padding\n",
        "- Stride로 인해 이미지가 너무 작아지는 것을 방지하는 것\n",
        "- 원리는 이미지 밖 테두리에 0값을 넣어\u001c사이즈를 키워두는 것.\n",
        "- 그러면 스트라이드를 이미지 밖에서부터 진행하므로 이미지 사이즈 축소를 방지할 수 있다.\n",
        "- 테두리와 끝을 쉽게 파악할 수 있는 것도 장점\n",
        "\n",
        "## Max Pooling\n",
        "- single depth slice에 필터 내 가장 값이 큰 것만 뽑아서 하는 것\n",
        "\n",
        "## Process\n",
        "- Stride + Padding -> Conv Layer -> Relu -> Max Pooling -> FC(full connected)\n",
        "- FC는 앞서 우리가 했던 답을 주고 훈련시키는 것을 의미한다.\n",
        "- CNN은 FC 앞전 단계에서 기계가 스스로 분류하도록 만드는 과정이 존재한다. "
      ],
      "metadata": {
        "id": "5IfIGnNBWiIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xxHvNXZ-iPek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기본예제\n",
        "\n",
        "+ 3 * 3 * 1 * 1\n",
        "+ 크기(3*3), 색상(1), 갯수(1)\n",
        "+ filter = 2 * 2 * 1"
      ],
      "metadata": {
        "id": "R14FqUITn2_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = np.array([[[[1], [2], [3]],\n",
        "                     [[4], [5], [6]],\n",
        "                     [[7], [8], [9]]]], dtype=np.float32)\n",
        "\n",
        "print(image.shape)\n",
        "plt.imshow(image.reshape(3, 3), cmap=\"Greys\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "BYxmbj1BWlt9",
        "outputId": "8ebffdc2-35cb-4949-eaee-c7fe23b5398a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 3, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe0c86d3d10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter = tf.constant([[[[1.]], [[1.]]],\n",
        "                       [[[1.]], [[1.]]]])\n",
        "\n",
        "print(filter.shape)\n",
        "\n",
        "conv2d = tf.nn.conv2d(image, filter, strides=[1,1,1,1], padding=\"VALID\")\n",
        "#strides=[1,1,1,1]에서 양 끝 1과 1은 의미 X. 중요한건 가운데 1,1. 앞은 가로, 뒤는 세로 이동 크기 의미한다.\n",
        "\n",
        "sess = tf.Session()\n",
        "conv2d_img = sess.run(conv2d)\n",
        "print(conv2d_img.shape)\n",
        "\n",
        "#(2,2,2,2)이면 뒤에서부터 1차원에 2개의 값이 있는 것이 2차원으로 2개 있고 3차원으로 2개 있고 4차원으로 2개 있다. 이렇게 해석하면 됨\n",
        "\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "  print(one_img.reshape(2,2))\n",
        "  plt.subplot(1,2,i+1)\n",
        "  plt.imshow(one_img.reshape(2,2), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Um85bItCpJsJ",
        "outputId": "952e744d-5224-4229-9870-d8a79eddce6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2, 1, 1)\n",
            "(1, 2, 2, 1)\n",
            "[[12. 16.]\n",
            " [24. 28.]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding\n",
        "- 이미지 테두리에 숫자를 채우는 것\n",
        "- 이미지 크기가 줄어드는 것을 방지하기 위해"
      ],
      "metadata": {
        "id": "ah9giVm3i3aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#padding\n",
        "\n",
        "filter = tf.constant([[[[1.]], [[1.]]],\n",
        "                       [[[1.]], [[1.]]]])\n",
        "\n",
        "print(filter.shape)\n",
        "\n",
        "conv2d = tf.nn.conv2d(image, filter, strides=[1,1,1,1], padding=\"SAME\")\n",
        "#strides=[1,1,1,1]에서 양 끝 1과 1은 의미 X. 중요한건 가운데 1,1. 앞은 가로, 뒤는 세로 이동 크기 의미한다.\n",
        "\n",
        "sess = tf.Session()\n",
        "conv2d_img = sess.run(conv2d)\n",
        "print(conv2d_img.shape)\n",
        "\n",
        "#(2,2,2,2)이면 뒤에서부터 1차원에 2개의 값이 있는 것이 2차원으로 2개 있고 3차원으로 2개 있고 4차원으로 2개 있다. 이렇게 해석하면 됨\n",
        "\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "  print(one_img.reshape(3,3))\n",
        "  plt.subplot(1,2,i+1)\n",
        "  plt.imshow(one_img.reshape(3,3), cmap='gray')\n",
        "\n",
        "# 패딩을 했으니, 사이즈 3,3으로 지정할 수 있다."
      ],
      "metadata": {
        "id": "Cgx_7iKsq5Wh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "bb725d9f-1051-467b-9438-1d6d266c0e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2, 1, 1)\n",
            "(1, 3, 3, 1)\n",
            "[[12. 16.  9.]\n",
            " [24. 28. 15.]\n",
            " [15. 17.  9.]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 주제 선정 방법\n",
        "\n",
        "#하향식 :: 주제 -> 데이터\n",
        "#상향식 :: 데이터 -> 주제\n",
        "\n",
        "\n",
        "#### CNN 등의 딥러닝은 원리를 이해하는게 중요. 그래야 조합해서 쓸 수 있기 때문.\n",
        "#### 사용방법도 중요하지만, 왜 하는지를 정확히 이해하고 넘어가자! 그래야 적절하게 조합해 러닝모델을 만들 수 있다."
      ],
      "metadata": {
        "id": "NbwGTmg2db9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter\n",
        "- 필터를 여러개 쓰는 방법에 대해\n",
        "- 필터를 많이 쓰면 특징을 조금 더 잘 뽑을 수 있다.\n",
        "- 결과물로 필터 수만큼 이미지 수가 생긴다"
      ],
      "metadata": {
        "id": "vOtRzuR0jAuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이번엔 filter를 3개 만들어보자. 필터를 세개 만들면 이미지를 세번 스캐닝하므로 세 개의 이미지가 만들어진다. 학습이 더 잘될 가능성이 높음\n",
        "# filter -> (2,2,1,3) 필터크기:2x2, 색상:흑백, 갯수:3개\n",
        "\n",
        "filter = tf.constant([[[[1., 10, -1]], [[1., 10, -1]]],\n",
        "                       [[[1., 10, -1]], [[1., 10, -1]]]])\n",
        "# 필터에 지정한 값은 학습을 통해 스스로 찾는 것. 여기선 그냥 임의로 지정해준 것일 뿐이다.\n",
        "\n",
        "print(filter.shape)\n",
        "\n",
        "conv2d = tf.nn.conv2d(image, filter, strides=[1,1,1,1], padding=\"SAME\")\n",
        "#strides=[1,1,1,1]에서 양 끝 1과 1은 의미 X. 중요한건 가운데 1,1. 앞은 가로, 뒤는 세로 이동 크기 의미한다.\n",
        "\n",
        "sess = tf.Session()\n",
        "conv2d_img = sess.run(conv2d)\n",
        "print(conv2d_img.shape)\n",
        "#(1,3,3,3) 색상/크기/갯수\n",
        "\n",
        "#(2,2,2,2)이면 뒤에서부터 1차원에 2개의 값이 있는 것이 2차원으로 2개 있고 3차원으로 2개 있고 4차원으로 2개 있다. 이렇게 해석하면 됨\n",
        "\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "  print(one_img.reshape(3,3))\n",
        "  plt.subplot(1,3,i+1) #이미지 갯수에 맞게. 3개가 나올테니 subplot 공간도 3개\n",
        "  plt.imshow(one_img.reshape(3,3), cmap='gray')\n",
        "\n",
        "# 패딩을 했으니, 사이즈 3,3으로 지정할 수 있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "nek1U_QOfB6y",
        "outputId": "c27840bc-21b0-4b9f-e427-ddbbe5b932cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2, 1, 3)\n",
            "(1, 3, 3, 3)\n",
            "[[12. 16.  9.]\n",
            " [24. 28. 15.]\n",
            " [15. 17.  9.]]\n",
            "[[120. 160.  90.]\n",
            " [240. 280. 150.]\n",
            " [150. 170.  90.]]\n",
            "[[-12. -16.  -9.]\n",
            " [-24. -28. -15.]\n",
            " [-15. -17.  -9.]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Max Pooling\n",
        "- 필터 통해 본 이미지 픽셀 값 중 최대값만 뽑아오는 것을 의미\n",
        "- conv2d 사이사이 섞어주면 성능이 올라가더라~\n",
        " "
      ],
      "metadata": {
        "id": "yk2-uJsjjMnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Max Pooling : 2*2 필터\n",
        "# padding은 없음\n",
        "\n",
        "image2 = tf.constant([[[[4], [3]],\n",
        "                       [[2], [1]]]])\n",
        "\n",
        "print(image2.shape)\n",
        "\n",
        "pool = tf.nn.max_pool(image2, ksize=[1,2,2,1], strides=[1,1,1,1], padding='VALID')\n",
        "# conv2d, 컨벌루션은 필터값이 가중치를 의미하기 때문에 그 값이 딥러닝 성능을 결정하는 요인\n",
        "# 그러나, 여기서 ksize는 가중치 없는 단순 프레임을 의미한다.\n",
        "\n",
        "sess = tf.Session()\n",
        "p = sess.run(pool)\n",
        "print(p.shape)\n",
        "print(p)\n",
        "# 1. 패딩을 안했으니 이미지 크기는 줄었다.\n",
        "# 2. 4,3,2,1 중 가장 큰 값인 4를 꺼냈다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLI2gK6tgXZu",
        "outputId": "b17f1eb0-c2fa-4df1-fdcb-aa056e7a6f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2, 2, 1)\n",
            "(1, 1, 1, 1)\n",
            "[[[[4]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Max Pooling : 2*2 필터 + padding\n",
        "\n",
        "image2 = tf.constant([[[[4], [3]],\n",
        "                       [[2], [1]]]])\n",
        "\n",
        "print(image2.shape)\n",
        "\n",
        "pool = tf.nn.max_pool(image2, ksize=[1,2,2,1], strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "sess = tf.Session()\n",
        "p = sess.run(pool)\n",
        "print(p.shape)\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoTSAxZIj71s",
        "outputId": "acf2121c-815d-4b6d-8682-a59940128ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2, 2, 1)\n",
            "(1, 2, 2, 1)\n",
            "[[[[4]\n",
            "   [3]]\n",
            "\n",
            "  [[2]\n",
            "   [1]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 응용예제 : MNIST를 활용한 simple CNN"
      ],
      "metadata": {
        "id": "_cvRQlpcrW26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting Data!\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/content/data/mnist/\", one_hot = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l91fLYPSpvL-",
        "outputId": "a5993cf1-b47b-40ca-eec6-cb073d516e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-5f133e0c162f>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /content/data/mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /content/data/mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /content/data/mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /content/data/mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "AE5cZTp-6uDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "#### 0. Convolution Layer\n",
        "# - 크기 : 3x3\n",
        "# - 갯수 : 32\n",
        "# - 색상수 : 1\n",
        "# - 2차원 원본 이미지를 4차원으로 먼저 바꿔줘야\n",
        "\n",
        "\n",
        "\n",
        "#### 1. 원본 이미지 준비\n",
        "origin_X = tf.reshape(X, [-1, 28, 28, 1])\n",
        "# (1,1,1,1) 형태로 바꿔줘야 한다. 갯수/크기/색상 순이다.\n",
        "# None과 같은 의미로 -1이 있다. 둘 중 하나\n",
        "\n",
        "\n",
        "\n",
        "#### 2. 필터 준비\n",
        "# - max_pooling이 아니므로 필터는 가중치 역할을 한다.\n",
        "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
        "# 필터도 4차원으로 들어가줘야한다.\n",
        "# 여기서 4차원 순서는 크기/색상/갯수\n",
        "# stddev = 0.01은 난수 뽑을 때 그 값들의 표준편차를 0.01로 정하고 뽑으란 의미\n",
        "\n",
        "\n",
        "#### 3. Convolution Layer : stride 1칸, padding 있음\n",
        "layer1 = tf.nn.conv2d(origin_X, W1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "print(layer1)\n",
        "# Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
        "# 필터가 32개이니 32개의 레이어가 생겼다. 55000 x 32의 이미지가 생겨날 것\n",
        "\n",
        "\n",
        "#### 4. ReLU 사용\n",
        "layer1 = tf.nn.relu(layer1)\n",
        "\n",
        "\n",
        "#### 5. max_pooling\n",
        "# - 크기 : 2x2\n",
        "# - stride : 2칸\n",
        "# - padding : Yes\n",
        "layer1 = tf.nn.max_pool(layer1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "print(layer1)\n",
        "# Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
        "# 크기가 14x14로 줄었다. padding 했지만 strides가 2칸씩으로 설정되어있기 때문\n",
        "\n",
        "\n",
        "#### 6. 위의 것을 한번 더 반복하면 Feature_Extracting 단계 완료\n",
        "# - 필터크기 = 3x3\n",
        "# - 필터갯수 = 64\n",
        "# - 색상 수 = 1\n",
        "\n",
        "# Filter\n",
        "W2 = tf.Variable(tf.random_normal([3,3,32,64], stddev=0.01))\n",
        "# 두번째부터 색상 정보는 의미가 없어 안받는다. 그래서 여기선 크기/받을 이미지 갯수/만들 이미지 갯수다. \n",
        "# Convolution Layer\n",
        "layer2 = tf.nn.conv2d(layer1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "# ReLU\n",
        "layer2 = tf.nn.relu(layer2)\n",
        "# max_pooling\n",
        "layer2 = tf.nn.max_pool(layer2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "print(layer2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA3xrm_Js081",
        "outputId": "6353c6f4-93b8-4bbf-dec6-5cb44c83e11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
            "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
            "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FC, Fully Connected Layer\n",
        "- **Dense Layer"
      ],
      "metadata": {
        "id": "U9kBeH-c6ycD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 1. Hyper Parameter 준비\n",
        "\n",
        "lr = 0.001\n",
        "#러닝레이트\n",
        "training_epochs = 20\n",
        "batch_size = 500\n",
        "\n",
        "\n",
        "\n",
        "#### 2. 입력값을 지정해주자\n",
        "# 입력해야할 layer2는 4차원, 그런데, FC에선 2차원으로 받아야한다.\n",
        "# reshape 필요\n",
        "# FC에선 [갯수, 크기]를 의미한다.\n",
        "\n",
        "train_X = tf.reshape(layer2, [-1, 7*7*64])\n",
        "#print(layer2) 결과 참고\n",
        "\n",
        "W1 = tf.get_variable(\"W1\", shape=[7*7*64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([10]), name = \"bias1\") # 출력값도 맞춰줘야 한다.\n",
        "\n",
        "# 비용\n",
        "logit = tf.matmul(train_X, W1) + b1\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
        "\n",
        "# 최저비용\n",
        "train = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
        "\n",
        "# batch\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "  total_batch = int(mnist.train.num_examples / batch_size)\n",
        "  avg_cost = 0\n",
        "\n",
        "  for i in range(total_batch):\n",
        "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "    \n",
        "    _, c = sess.run([train, cost], feed_dict = {X:batch_x, y:batch_y})\n",
        "    #prob값이 0.7이면 70% 남기고 30%는 버린다는 의미\n",
        "\n",
        "    avg_cost += c / total_batch\n",
        "\n",
        "  print(epoch, avg_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcjyKZ4qyDnK",
        "outputId": "d70adfe4-e4f2-4feb-cc33-b3d2c1524ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.0138822005553683\n",
            "1 0.23155568282712588\n",
            "2 0.13670509322123098\n",
            "3 0.10038444040851165\n",
            "4 0.08375339623201981\n",
            "5 0.07477945411069828\n",
            "6 0.06379712911492043\n",
            "7 0.0607537778941068\n",
            "8 0.054617119563574144\n",
            "9 0.049680400419641606\n",
            "10 0.04699406154792419\n",
            "11 0.04306638384745879\n",
            "12 0.04169839999045838\n",
            "13 0.038723489354279904\n",
            "14 0.03448338605124841\n",
            "15 0.0325040609575808\n",
            "16 0.03286313671110706\n",
            "17 0.02949000108055769\n",
            "18 0.030234354040162125\n",
            "19 0.02584497594189916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = tf.argmax(logit, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "sess.run(accuracy, feed_dict = {X:mnist.test.images, y:mnist.test.labels})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS7ayBog-LOz",
        "outputId": "622c6ff2-a2fa-43af-8330-024330385c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9874"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "YyrxVheg_OwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep & Wide FC"
      ],
      "metadata": {
        "id": "B_DyZQqm_yOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "\n",
        "\n",
        "#### 1. 원본 이미지 준비\n",
        "origin_X = tf.reshape(X, [-1, 28, 28, 1])\n",
        "\n",
        "#### 2. 필터 준비\n",
        "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
        "\n",
        "#### 3. Convolution Layer : stride 1칸, padding 있음\n",
        "layer1 = tf.nn.conv2d(origin_X, W1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "\n",
        "#### 4. ReLU 사용\n",
        "layer1 = tf.nn.relu(layer1)\n",
        "\n",
        "#### 5. max_pooling\n",
        "layer1 = tf.nn.max_pool(layer1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "#### 6. 위의 것을 한번 더 반복하면 Feature_Extracting 단계 완료\n",
        "W2 = tf.Variable(tf.random_normal([3,3,32,64], stddev=0.01))\n",
        "layer2 = tf.nn.conv2d(layer1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "layer2 = tf.nn.relu(layer2)\n",
        "layer2 = tf.nn.max_pool(layer2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 1. Hyper Parameter 준비\n",
        "\n",
        "lr = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "prob = tf.placeholder(tf.float32)\n",
        "\n",
        "#### 2. 입력값을 지정해주자\n",
        "train_X = tf.reshape(layer2, [-1, 7*7*64])\n",
        "\n",
        "#### 3. hidden layer\n",
        "# 첫번째 히든 계층\n",
        "with tf.name_scope(\"layer1\"):\n",
        "  W1 = tf.get_variable(\"W1\", shape=[7*7*64,128], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  b1 = tf.Variable(tf.random_normal([128]), name = \"bias1\") # 출력값도 맞춰줘야 한다.\n",
        "  hypot1 = tf.nn.relu(tf.matmul(train_X, W1) + b1)\n",
        "  hypot1 = tf.nn.dropout(hypot1, keep_prob=prob)\n",
        "\n",
        "  tf.summary.histogram(\"weight1\", W1)\n",
        "  tf.summary.histogram(\"bias1\", b1)\n",
        "  tf.summary.histogram(\"hypot1\", hypot1)\n",
        "\n",
        "# 두번째 히든 계층\n",
        "with tf.name_scope(\"layer2\"):\n",
        "  W2 = tf.get_variable(\"W2\", shape=[128,128], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  b2 = tf.Variable(tf.random_normal([128]), name = \"bias2\") # 출력값도 맞춰줘야 한다.\n",
        "  hypot2 = tf.nn.relu(tf.matmul(hypot1, W2) + b2)\n",
        "  hypot2 = tf.nn.dropout(hypot2, keep_prob=prob)\n",
        "\n",
        "  tf.summary.histogram(\"weight2\", W2)\n",
        "  tf.summary.histogram(\"bias2\", b2)\n",
        "  tf.summary.histogram(\"hypot2\", hypot2)\n",
        "\n",
        "# 마지막 히든 계층\n",
        "with tf.name_scope(\"layer\"):\n",
        "  W = tf.get_variable(\"W\", shape=[128,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  b = tf.Variable(tf.random_normal([10]), name = \"bias\") # 출력값도 맞춰줘야 한다.\n",
        "  logit = tf.nn.relu(tf.matmul(hypot2, W) + b)\n",
        "  hypot = tf.nn.softmax(logit)\n",
        "  hypot = tf.nn.dropout(hypot, keep_prob=prob)\n",
        "\n",
        "  tf.summary.histogram(\"weight\", W)\n",
        "  tf.summary.histogram(\"bias\", b)\n",
        "  tf.summary.histogram(\"hypot\", hypot)\n",
        "\n",
        "\n",
        "# 비용\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
        "\n",
        "# 최저비용\n",
        "train = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
        "\n",
        "# batch\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "  total_batch = int(mnist.train.num_examples / batch_size)\n",
        "  avg_cost = 0\n",
        "\n",
        "  for i in range(total_batch):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        _, c = sess.run([train, cost], \n",
        "                        feed_dict={X:batch_x, y:batch_y, prob:0.7})\n",
        "\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "  print(epoch, avg_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "MNKGgWp3_15P",
        "outputId": "8fadc2e0-88eb-40d2-c9c8-41d0d9f91023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1120\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1121\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3606\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3607\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3685\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3687\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder_2:0\", dtype=float32) is not an element of this graph.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-212f88f21099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         _, c = sess.run([train, cost], \n\u001b[0;32m---> 97\u001b[0;31m                         feed_dict={X:batch_x, y:batch_y, prob:0.7})\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1121\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: ' +\n\u001b[0;32m-> 1123\u001b[0;31m                             e.args[0])\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_2:0\", dtype=float32) is not an element of this graph."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = tf.argmax(logit, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "sess.run(accuracy, feed_dict = {X:mnist.test.images, y:mnist.test.labels, prob:1.0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vp2sndYDJvK",
        "outputId": "c116a7d0-dc35-4e2f-b844-a428e37adc88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9829"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "_RMhS4o_DK4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "\n",
        "\n",
        "#### 1. 원본 이미지 준비\n",
        "origin_X = tf.reshape(X, [-1, 28, 28, 1])\n",
        "\n",
        "#### 2. 필터 준비\n",
        "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
        "\n",
        "#### 3. Convolution Layer : stride 1칸, padding 있음\n",
        "layer1 = tf.nn.conv2d(origin_X, W1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "\n",
        "#### 4. ReLU 사용\n",
        "layer1 = tf.nn.relu(layer1)\n",
        "\n",
        "#### 5. max_pooling\n",
        "layer1 = tf.nn.max_pool(layer1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "#### 6. 위의 것을 한번 더 반복하면 Feature_Extracting 단계 완료\n",
        "W2 = tf.Variable(tf.random_normal([3,3,32,64], stddev=0.01))\n",
        "layer2 = tf.nn.conv2d(layer1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "layer2 = tf.nn.relu(layer2)\n",
        "layer2 = tf.nn.max_pool(layer2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### Hyper Parameter 준비\n",
        "lr = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "prob = tf.placeholder(tf.float32)\n",
        "\n",
        "##### Tensorflow Graph\n",
        "train_X = tf.reshape(layer2, [-1, 7*7*64])\n",
        "\n",
        "\n",
        "# Layer1\n",
        "W1 = tf.get_variable(\"W1\", shape=[7*7*64, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([128]))\n",
        "logit1 = tf.matmul(train_X, W1) + b1\n",
        "train_X = tf.nn.relu(logit1)\n",
        "l1 = tf.nn.dropout(train_X, keep_prob=prob)\n",
        "\n",
        "# Layer2\n",
        "W2 = tf.get_variable(\"W2\", shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([128]))\n",
        "logit2 = tf.matmul(l1, W2) + b2\n",
        "l2 = tf.nn.relu(logit2)\n",
        "l2 = tf.nn.dropout(l2, keep_prob=prob)\n",
        "\n",
        "# Layer3\n",
        "W_out = tf.get_variable(\"W_out\", shape=[128, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b_out = tf.Variable(tf.random_normal([10]))\n",
        "logit = tf.matmul(l2, W_out) + b_out\n",
        "\n",
        "# 비용\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
        "\n",
        "# 최저 비용\n",
        "train = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
        "\n",
        "##### Tensor Graph 실행\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    avg_cost = 0\n",
        "    \n",
        "    for i in range(total_batch):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        _, c = sess.run([train, cost], \n",
        "                        feed_dict={X:batch_x, y:batch_y, prob:0.7})\n",
        "        \n",
        "        avg_cost += c / total_batch\n",
        "        \n",
        "    print(epoch+1, avg_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exf6_PChInnf",
        "outputId": "500181f0-93e2-492a-85be-7758094bc156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 0.47291190317408616\n",
            "2 0.13728828584267327\n",
            "3 0.10307766818695455\n",
            "4 0.07962189983999862\n",
            "5 0.06730421524198556\n",
            "6 0.05887102808380003\n",
            "7 0.05181923394176096\n",
            "8 0.04598868594949388\n",
            "9 0.04168113912201736\n",
            "10 0.03887187231861226\n",
            "11 0.03370104429560757\n",
            "12 0.0317030080776154\n",
            "13 0.030377448891945727\n",
            "14 0.029989277886872324\n",
            "15 0.026381749580190923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = tf.argmax(logit, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "sess.run(accuracy, feed_dict = {X:mnist.test.images, y:mnist.test.labels, prob:1.0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnBRMZPuLXqx",
        "outputId": "a5992588-b165-4f7b-ce83-b6259e3b9f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9906"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "Jo6hfd0jPFeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실전 응용 예제\n",
        "  - https://benchmark.ini.rub.de/gtsrb_dataset.html\n",
        "  - 교통표지판 CNN\n",
        "  - 독일 교통표지판\n",
        "\n",
        "+ simple CNN\n",
        "  - 이미지(32*32) -> conv layer1(pooling) -> conv layer2 -> FC"
      ],
      "metadata": {
        "id": "qrXL8R-vfVBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 이미지 전처리"
      ],
      "metadata": {
        "id": "5eO-3xSQubTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 1. 이미지 전처리\n",
        "#### - 이미지를 크기를 똑같이 맞춰줘야 한다. 데이터 보면 크기 제각각\n",
        "\n",
        "import glob\n",
        "from skimage.color import rgb2lab\n",
        "from skimage.transform import resize\n",
        "from collections import namedtuple\n",
        "np.random.seed(101)"
      ],
      "metadata": {
        "id": "61w2STLsPH_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. 상수 정의\n",
        "#### - 변수인데 값을 바꿀 수 없는 것을 의미\n",
        "#### 예)N_CLASSES = 43 선언하면 N_CLASSES는 43으로 사용 가능.\n",
        "#### 예)파이썬에선 상수 지원을 하지 않음\n",
        "#### 예)암묵적으로 상수는 대문자로 선언\n",
        "\n",
        "N_CLASSES = 43\n",
        "# 교통표지판 종류 수\n",
        "RESIZED_IMAGE = (32,32)"
      ],
      "metadata": {
        "id": "MgliwrQAkMyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = namedtuple(\"Dataset\", [\"X\", \"y\"])\n",
        "# 데이터 보호를 목적으로 X, y는 바뀌지 않게 만들고 싶어\n",
        "# 우리가 namedtuple을 import한 이유"
      ],
      "metadata": {
        "id": "rKZNJeZDmeGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_tf_format(imgs):\n",
        "  return np.stack([img[:, :, np.newaxis] for img in imgs], axis=0).astype(np.float32)\n",
        "\n",
        "def read_ppm(data, n_labels, resize_to):\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  for c in range(n_labels):\n",
        "    full_path = data + \"/\" + \"/\" + format(c, '05d') + \"/\"\n",
        "    print(full_path)\n",
        "\n",
        "    for img_name in glob.glob(full_path + \"*.ppm\"):\n",
        "      img = plt.imread(img_name).astype(np.float32)\n",
        "\n",
        "      img = rgb2lab(img/255.0)[:, :, 0]\n",
        "      #rgb 색상은 255가지. 이걸 나눈 값을 받아서 하나의 색상값을 빼준다.\n",
        "      #흑백색상으로 만들어주는 것으로 이해하면 된다\n",
        "\n",
        "      img = resize(img, resize_to, mode = 'reflect')\n",
        "\n",
        "      label = np.zeros((n_labels,), dtype=np.float32)\n",
        "      label[c] = 1.0\n",
        "\n",
        "      images.append(img.astype(np.float32))\n",
        "      labels.append(label)\n",
        "\n",
        "  return Dataset(X=to_tf_format(images), y=np.array(labels))\n",
        "  # X, y는 넘파이 자료가 리스트로 담겨져있다. 바로 쓰려면 최소 numpy 배열로.\n",
        "  # X, y 넘파이로 변환하도록 만들어야\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "ds = read_ppm('/content/drive/MyDrive/GTSRB/Final_Training/Images', N_CLASSES, RESIZED_IMAGE)\n",
        "print(ds.X.shape)\n",
        "print(ds.y.shape)"
      ],
      "metadata": {
        "id": "0Br8PcPanCLm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "aed7c593-7980-41cc-ed2a-918625eb5402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GTSRB/Final_Training/Images//00000/\n",
            "/content/drive/MyDrive/GTSRB/Final_Training/Images//00001/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d5c1cee6a923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# ------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_ppm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/GTSRB/Final_Training/Images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRESIZED_IMAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d5c1cee6a923>\u001b[0m in \u001b[0;36mread_ppm\u001b[0;34m(data, n_labels, resize_to)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m#흑백색상으로 만들어주는 것으로 이해하면 된다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'reflect'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mtform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffineTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Make sure the transform is exactly metric, to ensure fast warping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/transform/_geometric.py\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(self, src, dst)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;31m# De-center and de-normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_matrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0msrc_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_tf_format(imgs):\n",
        "    return np.stack([img[:, :, np.newaxis] for img in imgs], axis=0).astype(np.float32)\n",
        "\n",
        "def read_ppm(data, n_labels, resize_to):\n",
        "    images = []\n",
        "    labels = []\n",
        "    \n",
        "    for c in range(n_labels):\n",
        "        full_path = data + \"/\" + format(c, '05d') + \"/\"\n",
        "        \n",
        "        for img_name in glob.glob(full_path + \"*.ppm\"):\n",
        "            img = plt.imread(img_name).astype(np.float32)\n",
        "\n",
        "            img = rgb2lab(img/255.0)[:, :, 0]\n",
        "            \n",
        "            img = resize(img, resize_to, mode=\"reflect\")\n",
        "            \n",
        "            label = np.zeros((n_labels,), dtype=np.float32)\n",
        "            label[c] = 1.0\n",
        "            \n",
        "            images.append(img.astype(np.float32))\n",
        "            labels.append(label)\n",
        "            \n",
        "    return Dataset(X=to_tf_format(images), y=np.array(labels))\n",
        "# --------------------------------------------------\n",
        "\n",
        "ds = read_ppm(\"/content/drive/MyDrive/GTSRB/Final_Training/Images\", N_CLASSES, RESIZED_IMAGE)\n",
        "print(ds.X.shape)\n",
        "print(ds.y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVPq_vazz6wr",
        "outputId": "9a3df1d2-d534-483c-cbc7-5834eeaef0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(29449, 32, 32, 1)\n",
            "(29449, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 전처리된 이미지 확인\n",
        "# X의 첫번째 이미지 확인\n",
        "plt.imshow(ds.X[0, :, :, :].reshape(RESIZED_IMAGE))\n",
        "# y의 첫번째 값 확인 \n",
        "print(ds.y[0, :])\n",
        "\n",
        "plt.imshow(ds.X[-1, :, :, :].reshape(RESIZED_IMAGE))\n",
        "print(ds.y[-1, :])\n"
      ],
      "metadata": {
        "id": "EHMx7eDi1hUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "1b7e0837-d8b4-4cf8-d70d-62f23095f8b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 훈련데이터와 테스트데이터의 분리\n",
        "# - 이미 나눠진 데이터를 굳이 또 한번 나누는 이유는 훈련 자체의 검증을 편리하게 하기 위해서다\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# 원래 했던 방식\n",
        "# X_train, X_test, \n",
        "\n",
        "\n",
        "\n",
        "idx_train, idx_test = train_test_split(range(ds.X.shape[0]), test_size=0.25, random_state=101)\n",
        "print(len(idx_train))\n",
        "print(idx_train[:10])\n",
        "\n",
        "X_train = ds.X[idx_train, :, :, :]\n",
        "print(X_train.shape)\n",
        "\n",
        "X_test = ds.X[idx_test, :, :, :]\n",
        "print(X_test.shape)\n",
        "\n",
        "y_train = ds.y[idx_train, :]\n",
        "print(y_train.shape)\n",
        "\n",
        "y_test = ds.y[idx_test, :]\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "id": "lk5evMesny-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18855f2d-f1f8-4bf5-e980-2fdb25284f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22086\n",
            "[26806, 18881, 5898, 11680, 15523, 12373, 3110, 6318, 23539, 7137]\n",
            "(22086, 32, 32, 1)\n",
            "(7363, 32, 32, 1)\n",
            "(22086, 43)\n",
            "(7363, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 훈련을 위한 함수 준비"
      ],
      "metadata": {
        "id": "TSwazKebuVIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 미니배치 준비\n",
        "\n",
        "def minibatcher(X, y, batch_size, shuffle):\n",
        "  # X와 y의 숫자가 같아야한다. X=문제 / y=정답\n",
        "  # TDD 방식 : test코드를 먼저 만들고 코드 진행마다해서 적용해 빠르게 대응하는 방법\n",
        "  # 이 방식이라면 테스트는 간단해야 한다.\n",
        "  assert X.shape[0] == y.shape[0]\n",
        "\n",
        "  n_samples = X.shape[0]\n",
        "\n",
        "  if shuffle:\n",
        "    idx = np.random.permutation(n_samples)\n",
        "  else:\n",
        "    idx = list(range(n_samples))\n",
        "\n",
        "  for i in range(int(np.ceil(n_samples/batch_size))):\n",
        "    from_idx = i*batch_size\n",
        "    to_idx = (i+1)*batch_size\n",
        "\n",
        "    # 스레드 : 작업을 나눠서 병렬처리. 끝날 때까지\n",
        "    # 사례로 예를 들면 3번에 나눠서 데이터를 전달해줘야하니, 3번의 작업을 끝날 때까지 이 함수를 끝내지 않겠단 의미\n",
        "    # 그냥 return을 하면 뱃치로 나눈 데이터 중 첫번째 덩어리만 던져주고 끝나버린다.\n",
        "    yield X[idx[from_idx:to_idx], :, :, :], y[idx[from_idx:to_idx], :]\n"
      ],
      "metadata": {
        "id": "sPovlSKrpBIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 미니배치 테스트\n",
        "for i in minibatcher(X_train, y_train, 10000, True):\n",
        "  print(i[0].shape, i[1].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_s-A0OpxiNZ",
        "outputId": "9b0f440a-5561-4346-fbf3-b7ed8e54906c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 32, 32, 1) (10000, 43)\n",
            "(10000, 32, 32, 1) (10000, 43)\n",
            "(2086, 32, 32, 1) (2086, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### CNN 모듈화\n",
        "def fc_no_activation_layer(in_tensors, n_units):\n",
        "  W = tf.get_variable(\"fc_W\", [in_tensors.get_shape()[1], n_units],\n",
        "                      initializer=tf.contrib.layers.xavier_initializer())\n",
        "  b = tf.get_variable(\"fc_b\", [n_units],\n",
        "                      initializer=tf.constant_initializer(0.0))\n",
        "  \n",
        "  return tf.matmul(in_tensors, W) + b\n",
        "\n",
        "\n",
        "def fc_layer(in_tensors, n_units):\n",
        "  return tf.nn.leaky_relu(fc_no_activation_layer(in_tensors, n_units))\n",
        "\n",
        "def conv_layer(in_tensors, kernel_size, n_units):\n",
        "  W = tf.get_variable(\"conv_W\", [kernel_size, kernel_size, in_tensors.get_shape()[3], n_units],\n",
        "                      initializer=tf.contrib.layers.xavier_initializer())\n",
        "  b = tf.get_variable(\"conv_b\", [n_units], initializer=tf.constant_initializer(0.0))\n",
        "  conv = tf.nn.conv2d(in_tensors, W, [1,1,1,1], \"SAME\")\n",
        "\n",
        "  return tf.nn.leaky_relu(conv + b)\n",
        "\n",
        "def maxpool_layer(in_tensors, sampling):\n",
        "  return tf.nn.max_pool(in_tensors, [1, sampling, sampling, 1], [1, sampling, sampling, 1], \"SAME\")\n",
        "\n",
        "def dropout(in_tensors, keep_proba, is_training):\n",
        "  #훈련 중일 때와 아닐 때를 나눠서. 조건문 써야하는데 tf에서 간단하게 할 수 있는 함수 제공\n",
        "  return tf.cond(is_training, lambda:tf.nn.dropout(in_tensors, keep_proba), lambda:in_tensors)"
      ],
      "metadata": {
        "id": "j7SUhc253pSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 구현\n",
        "\n",
        "+ spec\n",
        "  - 2차원 conv : 5*5 32필터\n",
        "  - 2차원 conv : 5*5 64필터\n",
        "  - 평면화 계층 : flat layer\n",
        "  - Full Connected layer, 1024개의 unit\n",
        "  - dropout 40%\n",
        "  - softmax"
      ],
      "metadata": {
        "id": "PgdsFFUsFrA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(in_tensors, is_training):\n",
        "  #훈련중인지 테스트중인지 파악하기 위한 함수\n",
        "  #fitst layer = 5*5 2d convolution, 32 filter, 2x maxpool, drop=20%(전체 20%)\n",
        "  with tf.variable_scope(\"L1\"):\n",
        "    l1 = maxpool_layer(conv_layer(in_tensors, 5, 32), 2)\n",
        "    l1_out = dropout(l1, 0.9, is_training)\n",
        "  \n",
        "  #second layer = 5*5 2d convolution, 64 filter, 2x maxpool, drop=20%(전체 20%)\n",
        "  with tf.variable_scope(\"L2\"):\n",
        "    l2 = maxpool_layer(conv_layer(l1_out, 5, 64), 2)\n",
        "    l2_out = dropout(l2, 0.9, is_training)\n",
        "\n",
        "  #flat layer = FC에서 훈련할 수 있게 차원을 바꿔준다. reshape보다 훨씬 편함\n",
        "  with tf.variable_scope(\"Flatten\"):\n",
        "    l2_out_flat = tf.layers.flatten(l2_out)\n",
        "  \n",
        "  # FC layer = 1024 neurons, 30% dropout\n",
        "  with tf.variable_scope(\"L4\"):\n",
        "    l3 = fc_layer(l2_out_flat, 1024)\n",
        "    l3_out = dropout(l3, 0.6, is_training)\n",
        "\n",
        "  # Result\n",
        "  with tf.variable_scope(\"L5\"):\n",
        "    out_tensors = fc_no_activation_layer(l3_out, 43)\n",
        "  \n",
        "\n",
        "  return out_tensors"
      ],
      "metadata": {
        "id": "dCE8Z3x2Fp81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 모델"
      ],
      "metadata": {
        "id": "pSYaGKJBQuQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "def train_model(X_train, y_train, learning_rate, max_epochs, batch_size):\n",
        "  in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\n",
        "  #shape = [-1, X_train.shape[1], X_train.shape[2], X_train.shape[3]]\n",
        "  #shape = 29406, 32, 32, 1\n",
        "\n",
        "  in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASSES))\n",
        "  is_training = tf.placeholder(tf.bool)\n",
        "  #shape = 29406, 43\n",
        "\n",
        "  logit = model(in_X_tensors_batch, is_training)\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=in_y_tensors_batch))\n",
        "  train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "      print(\"Epoch=\", epoch)\n",
        "      tf_scores=[]\n",
        "\n",
        "      for mb in minibatcher(X_train, y_train, batch_size, shuffle=True):\n",
        "        _, c = sess.run([train, cost], feed_dict={in_X_tensors_batch:mb[0], in_y_tensors_batch:mb[1], is_training:True})\n",
        "        tf_scores.append(c)\n",
        "\n",
        "      print(\"train loss score:\", np.mean(tf_scores))\n",
        "    \n",
        "    #### 훈련 끝나고 테스트\n",
        "    print(\"TEST SET PERFORMANCE\")\n",
        "\n",
        "    out_y_pred = tf.nn.softmax(logit)\n",
        "    y_test_pred, test_cost = sess.run([out_y_pred, cost], feed_dict={in_X_tensors_batch:X_test, in_y_tensors_batch:y_test, is_training:False})\n",
        "\n",
        "\n",
        "    print(\"test_loss_score=\", test_cost)\n",
        "    y_test_pred_classified = np.argmax(y_test_pred, axis=1).astype(np.int32)\n",
        "    y_test_true_classified = np.argmax(y_test, axis=1).astype(np.int32)\n",
        "    print(classification_report(y_test_true_classified, y_test_pred_classified))"
      ],
      "metadata": {
        "id": "pLyuHGAAQyba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "train_model(X_train, y_train, 0.001, 10, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NMM8P26Q8c5",
        "outputId": "a86293ab-7b7f-4f88-fae5-3fb77283144d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch= 0\n",
            "train loss score: 2.832918\n",
            "Epoch= 1\n",
            "train loss score: 0.45240867\n",
            "Epoch= 2\n",
            "train loss score: 0.23218405\n",
            "Epoch= 3\n",
            "train loss score: 0.14784008\n",
            "Epoch= 4\n",
            "train loss score: 0.10357764\n",
            "Epoch= 5\n",
            "train loss score: 0.08258565\n",
            "Epoch= 6\n",
            "train loss score: 0.062499534\n",
            "Epoch= 7\n",
            "train loss score: 0.057499688\n",
            "Epoch= 8\n",
            "train loss score: 0.05491186\n",
            "Epoch= 9\n",
            "train loss score: 0.03927221\n",
            "TEST SET PERFORMANCE\n",
            "test_loss_score= 0.0635813\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.98      0.99       537\n",
            "           2       0.99      0.99      0.99       562\n",
            "           3       0.98      0.95      0.97       358\n",
            "           4       0.98      0.99      0.99       500\n",
            "           5       0.99      0.95      0.97       466\n",
            "           6       1.00      1.00      1.00        92\n",
            "          10       1.00      0.99      0.99       499\n",
            "          11       1.00      0.99      0.99       324\n",
            "          12       0.99      0.99      0.99       313\n",
            "          13       1.00      0.99      0.99       547\n",
            "          14       0.97      1.00      0.99       208\n",
            "          15       0.99      0.99      0.99       175\n",
            "          16       1.00      1.00      1.00       113\n",
            "          17       1.00      1.00      1.00       265\n",
            "          18       0.97      1.00      0.99       308\n",
            "          19       0.96      0.98      0.97        51\n",
            "          20       0.98      0.97      0.97        88\n",
            "          21       0.97      1.00      0.98        83\n",
            "          22       0.99      0.99      0.99       107\n",
            "          23       0.98      0.98      0.98       126\n",
            "          24       1.00      0.99      0.99        70\n",
            "          25       0.98      0.99      0.99       349\n",
            "          26       1.00      0.95      0.98       147\n",
            "          27       1.00      1.00      1.00        56\n",
            "          28       0.96      1.00      0.98       129\n",
            "          29       0.99      0.93      0.96        72\n",
            "          32       1.00      0.91      0.95        65\n",
            "          33       1.00      0.98      0.99       188\n",
            "          34       0.98      0.99      0.99       111\n",
            "          35       0.90      1.00      0.95       301\n",
            "          40       0.99      1.00      1.00       103\n",
            "          42       1.00      0.98      0.99        50\n",
            "\n",
            "    accuracy                           0.98      7363\n",
            "   macro avg       0.99      0.98      0.98      7363\n",
            "weighted avg       0.99      0.98      0.98      7363\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. 일부 CNN 계층과 FC 계층을 추가해서 성능이 어떻게 되는지 확인해보기\n",
        "2. dropout의 비율을 변경해 보면서 결과가 과소적합 또는 과대적합인지 확인해보기\n",
        "3. 전체 epoch수와 batch_size도 변경해서 결과 확인\n",
        "4. 실제 테스트 이미지를 통해 사용할 수 있는 간단한 프로그램 작성\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "g3LEOB72u3sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. 모델 구현\n",
        "# - conv 레이어가 수가 많고 필터 수가 크면 성능이 더 좋을까?\n",
        "# - conv 레이어 수로 특징을 잘 잡게 만들면 FC 레이어의 출력값 수가 작아도 성적이 잘 나오지 않을까?\n",
        "# - dropout도 낮춰서 더 많은 데이터를 훈련하게 해보자(과대적합 비교)\n",
        "\n",
        "# + spec\n",
        "#   - 2차원 conv : 5*5 32필터\n",
        "#   - 2차원 conv : 5*5 64필터\n",
        "#   - 2차원 conv : 5*5 64필터\n",
        "#   - 평면화 계층 : flat layer\n",
        "#   - Full Connected layer, 512개의 unit\n",
        "#   - dropout 30%(CONV & FC)\n",
        "#   - softmax"
      ],
      "metadata": {
        "id": "3KqDP3Pd2OZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(in_tensors, is_training):\n",
        "  #훈련중인지 테스트중인지 파악하기 위한 함수\n",
        "  #fitst layer = 5*5 2d convolution, 32 filter, 2x maxpool, drop=10%(전체 30%)\n",
        "  with tf.variable_scope(\"L1\"):\n",
        "    l1 = maxpool_layer(conv_layer(in_tensors, 5, 32), 2)\n",
        "    l1_out = dropout(l1, 0.9, is_training)\n",
        "  \n",
        "  #second layer = 5*5 2d convolution, 64 filter, 2x maxpool, drop=10%(전체 30%)\n",
        "  with tf.variable_scope(\"L2\"):\n",
        "    l2 = maxpool_layer(conv_layer(l1_out, 5, 64), 2)\n",
        "    l2_out = dropout(l2, 0.9, is_training)\n",
        "\n",
        "  #third layer = 5*5 2d convolution, 64 filter, 2x maxpool, drop=10%(전체 30%)\n",
        "  with tf.variable_scope(\"L3\"):\n",
        "    l3 = maxpool_layer(conv_layer(l2_out, 5, 64), 2)\n",
        "    l3_out = dropout(l3, 0.9, is_training)\n",
        "\n",
        "  #flat layer = FC에서 훈련할 수 있게 차원을 바꿔준다. reshape보다 훨씬 편함\n",
        "  with tf.variable_scope(\"Flatten\"):\n",
        "    l3_out_flat = tf.layers.flatten(l3_out)\n",
        "  \n",
        "  # FC layer = 1512 neurons, 30% dropout\n",
        "  with tf.variable_scope(\"L4\"):\n",
        "    l4 = fc_layer(l3_out_flat, 512)\n",
        "    l4_out = dropout(l4, 0.7, is_training)\n",
        "\n",
        "  # Result\n",
        "  with tf.variable_scope(\"L5\"):\n",
        "    out_tensors = fc_no_activation_layer(l4_out, 43)\n",
        "  \n",
        "\n",
        "  return out_tensors"
      ],
      "metadata": {
        "id": "hHXfzX-Y5Nlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "def train_model(X_train, y_train, learning_rate, max_epochs, batch_size):\n",
        "  in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\n",
        "  #shape = [-1, X_train.shape[1], X_train.shape[2], X_train.shape[3]]\n",
        "  #shape = 29406, 32, 32, 1\n",
        "\n",
        "  in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASSES))\n",
        "  is_training = tf.placeholder(tf.bool)\n",
        "  #shape = 29406, 43\n",
        "\n",
        "  logit = model(in_X_tensors_batch, is_training)\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=in_y_tensors_batch))\n",
        "  train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "      print(\"Epoch=\", epoch)\n",
        "      tf_scores=[]\n",
        "\n",
        "      for mb in minibatcher(X_train, y_train, batch_size, shuffle=True):\n",
        "        _, c = sess.run([train, cost], feed_dict={in_X_tensors_batch:mb[0], in_y_tensors_batch:mb[1], is_training:True})\n",
        "        tf_scores.append(c)\n",
        "\n",
        "      print(\"train loss score:\", np.mean(tf_scores))\n",
        "    \n",
        "    #### 훈련 끝나고 테스트\n",
        "    print(\"TEST SET PERFORMANCE\")\n",
        "\n",
        "    out_y_pred = tf.nn.softmax(logit)\n",
        "    y_test_pred, test_cost = sess.run([out_y_pred, cost], feed_dict={in_X_tensors_batch:X_test, in_y_tensors_batch:y_test, is_training:False})\n",
        "\n",
        "\n",
        "    print(\"test_loss_score=\", test_cost)\n",
        "    y_test_pred_classified = np.argmax(y_test_pred, axis=1).astype(np.int32)\n",
        "    y_test_true_classified = np.argmax(y_test, axis=1).astype(np.int32)\n",
        "    print(classification_report(y_test_true_classified, y_test_pred_classified))"
      ],
      "metadata": {
        "id": "PW_Q35GJ8MAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력값은 같다.\n",
        "tf.reset_default_graph()\n",
        "train_model(X_train, y_train, 0.001, 10, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ag8MtDO8qVe",
        "outputId": "e102fe3f-b10a-4f23-b82d-a3a644e59913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-12-b202f53bfa2a>:27: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-13-f9e0b8fc4b8e>:20: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "Epoch= 0\n",
            "train loss score: 3.0414162\n",
            "Epoch= 1\n",
            "train loss score: 0.81293267\n",
            "Epoch= 2\n",
            "train loss score: 0.32590693\n",
            "Epoch= 3\n",
            "train loss score: 0.17751853\n",
            "Epoch= 4\n",
            "train loss score: 0.11883142\n",
            "Epoch= 5\n",
            "train loss score: 0.085906\n",
            "Epoch= 6\n",
            "train loss score: 0.058726612\n",
            "Epoch= 7\n",
            "train loss score: 0.051545147\n",
            "Epoch= 8\n",
            "train loss score: 0.04628155\n",
            "Epoch= 9\n",
            "train loss score: 0.043476637\n",
            "TEST SET PERFORMANCE\n",
            "test_loss_score= 0.03555752\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.99      0.99       537\n",
            "           2       0.99      1.00      0.99       562\n",
            "           3       1.00      0.98      0.99       358\n",
            "           4       1.00      0.99      1.00       500\n",
            "           5       0.97      1.00      0.99       466\n",
            "           6       0.99      1.00      0.99        92\n",
            "          10       1.00      0.99      0.99       499\n",
            "          11       1.00      0.99      1.00       324\n",
            "          12       0.98      0.99      0.99       313\n",
            "          13       1.00      1.00      1.00       547\n",
            "          14       1.00      0.99      0.99       208\n",
            "          15       0.98      0.98      0.98       175\n",
            "          16       0.99      0.99      0.99       113\n",
            "          17       0.99      1.00      0.99       265\n",
            "          18       0.98      1.00      0.99       308\n",
            "          19       0.98      0.94      0.96        51\n",
            "          20       1.00      0.99      0.99        88\n",
            "          21       1.00      1.00      1.00        83\n",
            "          22       0.99      1.00      1.00       107\n",
            "          23       0.98      0.98      0.98       126\n",
            "          24       1.00      0.99      0.99        70\n",
            "          25       0.99      0.99      0.99       349\n",
            "          26       0.99      0.96      0.98       147\n",
            "          27       1.00      1.00      1.00        56\n",
            "          28       0.98      1.00      0.99       129\n",
            "          29       0.99      0.96      0.97        72\n",
            "          32       1.00      1.00      1.00        65\n",
            "          33       0.99      0.99      0.99       188\n",
            "          34       1.00      1.00      1.00       111\n",
            "          35       1.00      1.00      1.00       301\n",
            "          40       0.99      0.99      0.99       103\n",
            "          42       1.00      1.00      1.00        50\n",
            "\n",
            "    accuracy                           0.99      7363\n",
            "   macro avg       0.99      0.99      0.99      7363\n",
            "weighted avg       0.99      0.99      0.99      7363\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### dropout의 비율을 10%로 확 낮춰보자. 그리고 과적합 비율 비교.\n",
        "#### cost는 줄어든다. 그런데 성적은 코스트 대비 별 차이가 없다. 과적합 가능성 높다.\n",
        "\n",
        "def model(in_tensors, is_training):\n",
        "  #훈련중인지 테스트중인지 파악하기 위한 함수\n",
        "  #fitst layer = 5*5 2d convolution, 32 filter, 2x maxpool, drop=10%(전체 30%)\n",
        "  with tf.variable_scope(\"L1\"):\n",
        "    l1 = maxpool_layer(conv_layer(in_tensors, 5, 32), 2)\n",
        "    l1_out = dropout(l1, 0.95, is_training)\n",
        "  \n",
        "  #second layer = 5*5 2d convolution, 64 filter, 2x maxpool, drop=10%(전체 30%)\n",
        "  with tf.variable_scope(\"L2\"):\n",
        "    l2 = maxpool_layer(conv_layer(l1_out, 5, 64), 2)\n",
        "    l2_out = dropout(l2, 0.95, is_training)\n",
        "\n",
        "  #flat layer = FC에서 훈련할 수 있게 차원을 바꿔준다. reshape보다 훨씬 편함\n",
        "  with tf.variable_scope(\"Flatten\"):\n",
        "    l2_out_flat = tf.layers.flatten(l2_out)\n",
        "  \n",
        "  # FC layer = 1512 neurons, 30% dropout\n",
        "  with tf.variable_scope(\"L4\"):\n",
        "    l3 = fc_layer(l2_out_flat, 512)\n",
        "    l3_out = dropout(l3, 0.9, is_training)\n",
        "\n",
        "  # Result\n",
        "  with tf.variable_scope(\"L5\"):\n",
        "    out_tensors = fc_no_activation_layer(l3_out, 43)\n",
        "  \n",
        "\n",
        "  return out_tensors"
      ],
      "metadata": {
        "id": "oI7GDaO2862W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "def train_model(X_train, y_train, learning_rate, max_epochs, batch_size):\n",
        "  in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\n",
        "  #shape = [-1, X_train.shape[1], X_train.shape[2], X_train.shape[3]]\n",
        "  #shape = 29406, 32, 32, 1\n",
        "\n",
        "  in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASSES))\n",
        "  is_training = tf.placeholder(tf.bool)\n",
        "  #shape = 29406, 43\n",
        "\n",
        "  logit = model(in_X_tensors_batch, is_training)\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=in_y_tensors_batch))\n",
        "  train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "      print(\"Epoch=\", epoch)\n",
        "      tf_scores=[]\n",
        "\n",
        "      for mb in minibatcher(X_train, y_train, batch_size, shuffle=True):\n",
        "        _, c = sess.run([train, cost], feed_dict={in_X_tensors_batch:mb[0], in_y_tensors_batch:mb[1], is_training:True})\n",
        "        tf_scores.append(c)\n",
        "\n",
        "      print(\"train loss score:\", np.mean(tf_scores))\n",
        "    \n",
        "    #### 훈련 끝나고 테스트\n",
        "    print(\"TEST SET PERFORMANCE\")\n",
        "\n",
        "    out_y_pred = tf.nn.softmax(logit)\n",
        "    y_test_pred, test_cost = sess.run([out_y_pred, cost], feed_dict={in_X_tensors_batch:X_test, in_y_tensors_batch:y_test, is_training:False})\n",
        "\n",
        "\n",
        "    print(\"test_loss_score=\", test_cost)\n",
        "    y_test_pred_classified = np.argmax(y_test_pred, axis=1).astype(np.int32)\n",
        "    y_test_true_classified = np.argmax(y_test, axis=1).astype(np.int32)\n",
        "    print(classification_report(y_test_true_classified, y_test_pred_classified))"
      ],
      "metadata": {
        "id": "1jF2lsDVC-bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력값은 같다.\n",
        "tf.reset_default_graph()\n",
        "train_model(X_train, y_train, 0.001, 10, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxJxyd_aDOfl",
        "outputId": "a180fa2f-729a-4e92-c471-586b41f87fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch= 0\n",
            "train loss score: 3.366077\n",
            "Epoch= 1\n",
            "train loss score: 0.44661817\n",
            "Epoch= 2\n",
            "train loss score: 0.19122347\n",
            "Epoch= 3\n",
            "train loss score: 0.1110337\n",
            "Epoch= 4\n",
            "train loss score: 0.0726982\n",
            "Epoch= 5\n",
            "train loss score: 0.044522107\n",
            "Epoch= 6\n",
            "train loss score: 0.038845304\n",
            "Epoch= 7\n",
            "train loss score: 0.024207573\n",
            "Epoch= 8\n",
            "train loss score: 0.025083497\n",
            "Epoch= 9\n",
            "train loss score: 0.015780186\n",
            "TEST SET PERFORMANCE\n",
            "test_loss_score= 0.06110932\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.99      0.99       537\n",
            "           2       0.97      0.99      0.98       562\n",
            "           3       1.00      0.93      0.96       358\n",
            "           4       0.99      0.99      0.99       500\n",
            "           5       0.97      0.98      0.97       466\n",
            "           6       1.00      1.00      1.00        92\n",
            "          10       1.00      1.00      1.00       499\n",
            "          11       1.00      0.99      0.99       324\n",
            "          12       0.98      1.00      0.99       313\n",
            "          13       1.00      0.99      0.99       547\n",
            "          14       1.00      1.00      1.00       208\n",
            "          15       0.99      0.99      0.99       175\n",
            "          16       1.00      1.00      1.00       113\n",
            "          17       1.00      1.00      1.00       265\n",
            "          18       0.99      1.00      0.99       308\n",
            "          19       0.98      0.92      0.95        51\n",
            "          20       1.00      0.93      0.96        88\n",
            "          21       0.95      1.00      0.98        83\n",
            "          22       0.99      0.97      0.98       107\n",
            "          23       0.98      0.98      0.98       126\n",
            "          24       0.97      0.99      0.98        70\n",
            "          25       0.97      0.99      0.98       349\n",
            "          26       0.99      0.96      0.98       147\n",
            "          27       0.95      1.00      0.97        56\n",
            "          28       0.94      0.99      0.97       129\n",
            "          29       0.99      0.94      0.96        72\n",
            "          32       0.97      0.98      0.98        65\n",
            "          33       0.99      0.98      0.99       188\n",
            "          34       0.98      0.99      0.99       111\n",
            "          35       0.98      1.00      0.99       301\n",
            "          40       0.98      0.98      0.98       103\n",
            "          42       1.00      1.00      1.00        50\n",
            "\n",
            "    accuracy                           0.99      7363\n",
            "   macro avg       0.98      0.98      0.98      7363\n",
            "weighted avg       0.99      0.99      0.99      7363\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 전체 epoch 수와 batch_size 수 변경\n",
        "#### 앞서 성적이 좋았던 conv2d를 3개로 dropout은 30%로\n",
        "#### 처음부터 필터를 64개로 가져가보자. 시간이 너무 오래걸린다.\n",
        "#### epoch 수는 up하고, batch_size는 up&down해서 반복횟수를 많이 가져가거나 반대로 한번 훈련할 때 양을 늘려보는 시도해보자.\n",
        "\n",
        "def model(in_tensors, is_training):\n",
        "  #훈련중인지 테스트중인지 파악하기 위한 함수\n",
        "  #fitst layer = 5*5 2d convolution, 64 filter, 2x maxpool, drop=10%(전체 30%)\n",
        "  with tf.variable_scope(\"L1\"):\n",
        "    l1 = maxpool_layer(conv_layer(in_tensors, 5, 32), 2)\n",
        "    l1_out = dropout(l1, 0.9, is_training)\n",
        "  \n",
        "  #second layer = 5*5 2d convolution, 64 filter, 2x maxpool, drop=10%(전체 30%)\n",
        "  with tf.variable_scope(\"L2\"):\n",
        "    l2 = maxpool_layer(conv_layer(l1_out, 5, 64), 2)\n",
        "    l2_out = dropout(l2, 0.9, is_training)\n",
        "\n",
        "  #third layer = 5*5 2d convolution, 64 filter, 2x maxpool, drop=10%(전체 30%)\n",
        "  with tf.variable_scope(\"L3\"):\n",
        "    l3 = maxpool_layer(conv_layer(l2_out, 5, 64), 2)\n",
        "    l3_out = dropout(l3, 0.9, is_training)\n",
        "\n",
        "  #flat layer = FC에서 훈련할 수 있게 차원을 바꿔준다. reshape보다 훨씬 편함\n",
        "  with tf.variable_scope(\"Flatten\"):\n",
        "    l3_out_flat = tf.layers.flatten(l3_out)\n",
        "  \n",
        "  # FC layer = 1512 neurons, 30% dropout\n",
        "  with tf.variable_scope(\"L4\"):\n",
        "    l4 = fc_layer(l3_out_flat, 512)\n",
        "    l4_out = dropout(l4, 0.7, is_training)\n",
        "\n",
        "  # Result\n",
        "  with tf.variable_scope(\"L5\"):\n",
        "    out_tensors = fc_no_activation_layer(l4_out, 43)\n",
        "  \n",
        "\n",
        "  return out_tensors"
      ],
      "metadata": {
        "id": "ZWzvVKglFkJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "def train_model(X_train, y_train, learning_rate, max_epochs, batch_size):\n",
        "  in_X_tensors_batch = tf.placeholder(tf.float32, shape=(None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\n",
        "  #shape = [-1, X_train.shape[1], X_train.shape[2], X_train.shape[3]]\n",
        "  #shape = 29406, 32, 32, 1\n",
        "\n",
        "  in_y_tensors_batch = tf.placeholder(tf.float32, shape=(None, N_CLASSES))\n",
        "  is_training = tf.placeholder(tf.bool)\n",
        "  #shape = 29406, 43\n",
        "\n",
        "  logit = model(in_X_tensors_batch, is_training)\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=in_y_tensors_batch))\n",
        "  train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "      print(\"Epoch=\", epoch)\n",
        "      tf_scores=[]\n",
        "\n",
        "      for mb in minibatcher(X_train, y_train, batch_size, shuffle=True):\n",
        "        _, c = sess.run([train, cost], feed_dict={in_X_tensors_batch:mb[0], in_y_tensors_batch:mb[1], is_training:True})\n",
        "        tf_scores.append(c)\n",
        "\n",
        "      print(\"train loss score:\", np.mean(tf_scores))\n",
        "    \n",
        "    #### 훈련 끝나고 테스트\n",
        "    print(\"TEST SET PERFORMANCE\")\n",
        "\n",
        "    out_y_pred = tf.nn.softmax(logit)\n",
        "    y_test_pred, test_cost = sess.run([out_y_pred, cost], feed_dict={in_X_tensors_batch:X_test, in_y_tensors_batch:y_test, is_training:False})\n",
        "\n",
        "\n",
        "    print(\"test_loss_score=\", test_cost)\n",
        "    y_test_pred_classified = np.argmax(y_test_pred, axis=1).astype(np.int32)\n",
        "    y_test_true_classified = np.argmax(y_test, axis=1).astype(np.int32)\n",
        "    print(classification_report(y_test_true_classified, y_test_pred_classified))"
      ],
      "metadata": {
        "id": "Ab_gG3-4F26_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter 64 -> 64 -> 64, epoch 20, batch_down\n",
        "# 시간 너무 오래걸림 + 튐\n",
        "# 그래서 20분 돌리고 중지\n",
        "tf.reset_default_graph()\n",
        "train_model(X_train, y_train, 0.001, 20, 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "mCy-6HENF6__",
        "outputId": "1f2fa1af-fce2-4a35-ac63-0a25b0c1a4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch= 0\n",
            "train loss score: 2.1625824\n",
            "Epoch= 1\n",
            "train loss score: 0.34607714\n",
            "Epoch= 2\n",
            "train loss score: 0.1544654\n",
            "Epoch= 3\n",
            "train loss score: 0.094667464\n",
            "Epoch= 4\n",
            "train loss score: 0.081064135\n",
            "Epoch= 5\n",
            "train loss score: 0.06968354\n",
            "Epoch= 6\n",
            "train loss score: 0.05896504\n",
            "Epoch= 7\n",
            "train loss score: 0.07485195\n",
            "Epoch= 8\n",
            "train loss score: 0.10274172\n",
            "Epoch= 9\n",
            "train loss score: 0.12020548\n",
            "Epoch= 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-117c93172836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# epoch 20, batch_up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-96b0f2e2ca9f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, learning_rate, max_epochs, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mmb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0min_X_tensors_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_y_tensors_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtf_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "8N5dUDAJM-dc",
        "outputId": "aabaef14-0e48-4321-8189-1024dd815278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e1e23f5baa4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter 32 -> 64 -> 64, epoch 20, batch_up\n",
        "tf.reset_default_graph()\n",
        "train_model(X_train, y_train, 0.001, 20, 1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6itUTDb4G7RN",
        "outputId": "f25e75c4-856f-4dc0-e673-7f0cb8715281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch= 0\n",
            "train loss score: 4.3631616\n",
            "Epoch= 1\n",
            "train loss score: 2.6768935\n",
            "Epoch= 2\n",
            "train loss score: 1.564356\n",
            "Epoch= 3\n",
            "train loss score: 0.89327216\n",
            "Epoch= 4\n",
            "train loss score: 0.5160455\n",
            "Epoch= 5\n",
            "train loss score: 0.34358948\n",
            "Epoch= 6\n",
            "train loss score: 0.2568567\n",
            "Epoch= 7\n",
            "train loss score: 0.18521892\n",
            "Epoch= 8\n",
            "train loss score: 0.15110825\n",
            "Epoch= 9\n",
            "train loss score: 0.12670219\n",
            "Epoch= 10\n",
            "train loss score: 0.09866645\n",
            "Epoch= 11\n",
            "train loss score: 0.08111558\n",
            "Epoch= 12\n",
            "train loss score: 0.0757952\n",
            "Epoch= 13\n",
            "train loss score: 0.062446713\n",
            "Epoch= 14\n",
            "train loss score: 0.05596503\n",
            "Epoch= 15\n",
            "train loss score: 0.048023023\n",
            "Epoch= 16\n",
            "train loss score: 0.044385344\n",
            "Epoch= 17\n",
            "train loss score: 0.040167026\n",
            "Epoch= 18\n",
            "train loss score: 0.03838925\n",
            "Epoch= 19\n",
            "train loss score: 0.033597726\n",
            "TEST SET PERFORMANCE\n",
            "test_loss_score= 0.039409544\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.99      0.99       537\n",
            "           2       0.98      0.99      0.99       562\n",
            "           3       1.00      0.98      0.99       358\n",
            "           4       1.00      1.00      1.00       500\n",
            "           5       0.98      0.98      0.98       466\n",
            "           6       1.00      1.00      1.00        92\n",
            "          10       1.00      0.99      0.99       499\n",
            "          11       0.99      0.99      0.99       324\n",
            "          12       0.99      1.00      0.99       313\n",
            "          13       1.00      1.00      1.00       547\n",
            "          14       1.00      1.00      1.00       208\n",
            "          15       0.99      1.00      1.00       175\n",
            "          16       0.99      1.00      1.00       113\n",
            "          17       1.00      1.00      1.00       265\n",
            "          18       0.97      1.00      0.99       308\n",
            "          19       0.98      0.98      0.98        51\n",
            "          20       0.98      0.99      0.98        88\n",
            "          21       0.99      1.00      0.99        83\n",
            "          22       1.00      1.00      1.00       107\n",
            "          23       1.00      0.97      0.98       126\n",
            "          24       1.00      1.00      1.00        70\n",
            "          25       0.99      1.00      0.99       349\n",
            "          26       0.99      0.95      0.97       147\n",
            "          27       1.00      1.00      1.00        56\n",
            "          28       0.98      0.99      0.99       129\n",
            "          29       0.99      0.96      0.97        72\n",
            "          32       1.00      1.00      1.00        65\n",
            "          33       0.99      1.00      0.99       188\n",
            "          34       1.00      0.99      1.00       111\n",
            "          35       1.00      1.00      1.00       301\n",
            "          40       1.00      0.99      1.00       103\n",
            "          42       0.98      1.00      0.99        50\n",
            "\n",
            "    accuracy                           0.99      7363\n",
            "   macro avg       0.99      0.99      0.99      7363\n",
            "weighted avg       0.99      0.99      0.99      7363\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YOsoYmAUPtRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}