{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_선형회귀와로지스틱회귀.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Colab 사전 준비"
      ],
      "metadata": {
        "id": "V9UQvXXjVBDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2he9D-DGAMwn",
        "outputId": "e35b747d-712d-4d69-f640-a60f16acd95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.42.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.13.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=6526c1520ecd295afe74b9ade3c6fe0e3ecc32d1ba0a050a6b63757c3ed870c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install tensorflow==1.15"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "metadata": {
        "id": "mCiy7wfzAXx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "R3SIuLT_U-LK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047b3723-d558-4259-9b2f-6c438a624a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 선형회귀분석\n",
        "- 단순선형회귀분석 모델의 비용 구하기"
      ],
      "metadata": {
        "id": "Ong9aiVPB-Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 데이터 준비\n",
        "X_train = [1,2,3]\n",
        "y_train = [1,2,3]\n",
        "\n",
        "W = tf.placeholder(tf.float32)\n",
        "#### 가설 준비(y=wx) : 지금은 데이터가 쉬워서 가중치 알 수 있으나, 원래는 이거 찾는데 엄청 고생\n",
        "hypot = W * X_train\n",
        "\n",
        "#### 비용? 잔차제곱합을 의미. 즉 잔차제곱합(평균제곱오차)가 최소가 되도록 만드는 값, 직선을 찾아야하는데 일단 비용부터 구해보자\n",
        "cost = tf.reduce_mean(tf.square(hypot-y_train))\n",
        "#mean((sum(blabla~)^2)) 이렇게 안하는 이유는 이건 바로 계산이 되어 그래프로 나타낼 수 없다.\n",
        "#tf로 하면 그래프로 나타낼 수 있다.\n",
        "\n",
        "# =============== 그래프 완성! ======================\n",
        "\n",
        "#### 이제 w값을 찾아보자\n",
        "sess = tf.Session()\n",
        "\n",
        "cost_val = []\n",
        "for i in range(-30, 50):\n",
        "  result = sess.run(cost, feed_dict={W:i})\n",
        "  cost_val.append(result)\n",
        "\n",
        "plt.plot(cost_val)\n",
        "sess.close()\n",
        "# 결과를 보면 convex graph로\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "kfAbDOomB1xY",
        "outputId": "12c7709a-844d-4257-8302-a2539186c374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 경사하강 알고리즘을 이용해 가중치 학습시키기\n",
        "# 앞에서 데이터 가져오기\n",
        "X_train = [1,2,3]\n",
        "y_train = [1,2,3]\n",
        "W = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
        "# b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
        "hypot = W * X_train\n",
        "cost = tf.reduce_mean(tf.square(hypot-y_train))\n",
        "\n",
        "\n",
        "#### 최저비용학습을 위한 경사하강알고리즘(gradient decent) :: 미분 개념\n",
        "# x의 순간변화값과 y값의 차이를 구하는 것?\n",
        "gradient = tf.reduce_mean((hypot-y_train)*X_train)\n",
        "learning_rate = 0.01\n",
        "descent = W - (learning_rate * gradient)\n",
        "# 원래는 W = W - (learning_rate * gradient). 그런데 왜 못하냐면 갱신이 안되서. 코드상으로 구현 불가\n",
        "# 그래서 assign이란 함수를 쓴다.\n",
        "\n",
        "updata = W.assign(descent)\n",
        "\n",
        "# ================================ Gradient Descent 그래프 완성\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# learning_rate는 내려가는 보폭 / 반복문 range는 훈련 횟수.\n",
        "# 적절한 값 분포가 필요하다. 예)learning_rate를 너무 줄이면 훈련 횟수를 엄청 늘려야 한다.\n",
        "\n",
        "for step in range(100):\n",
        "  _, weight, c = sess.run([updata, W, cost])\n",
        "  print(step, weight, c)\n",
        "  #여러개의 노드를 한번에 묶어 진행 가능 []로 묶어줘\n",
        "\n",
        "sess.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtsQjprcFvV6",
        "outputId": "7be76d9a-1a7e-4a2b-ef55-233b370a2a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [1.3590611] 0.6619941\n",
            "1 [1.342305] 0.6016494\n",
            "2 [1.3263307] 0.5468057\n",
            "3 [1.3111019] 0.49696127\n",
            "4 [1.2965838] 0.45166054\n",
            "5 [1.2827432] 0.41048908\n",
            "6 [1.2695485] 0.37307072\n",
            "7 [1.2569696] 0.33906332\n",
            "8 [1.2449777] 0.30815575\n",
            "9 [1.2335454] 0.28006572\n",
            "10 [1.2226466] 0.25453624\n",
            "11 [1.2122564] 0.23133366\n",
            "12 [1.2023511] 0.21024637\n",
            "13 [1.192908] 0.19108117\n",
            "14 [1.1839057] 0.17366306\n",
            "15 [1.1753235] 0.15783285\n",
            "16 [1.1671417] 0.1434455\n",
            "17 [1.1593417] 0.13036959\n",
            "18 [1.1519058] 0.11848558\n",
            "19 [1.1448169] 0.10768503\n",
            "20 [1.1380588] 0.097869\n",
            "21 [1.131616] 0.08894775\n",
            "22 [1.125474] 0.08083957\n",
            "23 [1.1196185] 0.07347069\n",
            "24 [1.1140363] 0.06677347\n",
            "25 [1.1087146] 0.060686648\n",
            "26 [1.1036413] 0.05515468\n",
            "27 [1.0988047] 0.05012706\n",
            "28 [1.0941938] 0.045557734\n",
            "29 [1.0897981] 0.041404862\n",
            "30 [1.0856075] 0.037630606\n",
            "31 [1.0816125] 0.034200363\n",
            "32 [1.0778039] 0.031082757\n",
            "33 [1.074173] 0.028249383\n",
            "34 [1.0707116] 0.025674274\n",
            "35 [1.0674118] 0.023333952\n",
            "36 [1.0642658] 0.021206943\n",
            "37 [1.0612668] 0.019273812\n",
            "38 [1.0584077] 0.01751687\n",
            "39 [1.055682] 0.01592011\n",
            "40 [1.0530834] 0.014468901\n",
            "41 [1.0506063] 0.013149965\n",
            "42 [1.0482446] 0.011951298\n",
            "43 [1.0459932] 0.010861869\n",
            "44 [1.0438468] 0.00987174\n",
            "45 [1.0418006] 0.008971881\n",
            "46 [1.0398499] 0.008154038\n",
            "47 [1.0379902] 0.0074107354\n",
            "48 [1.0362173] 0.0067352043\n",
            "49 [1.0345272] 0.006121252\n",
            "50 [1.032916] 0.005563248\n",
            "51 [1.0313798] 0.0050561377\n",
            "52 [1.0299155] 0.004595242\n",
            "53 [1.0285194] 0.004176367\n",
            "54 [1.0271885] 0.00379566\n",
            "55 [1.0259198] 0.0034496777\n",
            "56 [1.0247102] 0.0031352274\n",
            "57 [1.0235571] 0.0028494336\n",
            "58 [1.0224577] 0.002589693\n",
            "59 [1.0214097] 0.0023536347\n",
            "60 [1.0204107] 0.0021390945\n",
            "61 [1.0194582] 0.0019441145\n",
            "62 [1.0185502] 0.0017668913\n",
            "63 [1.0176845] 0.001605839\n",
            "64 [1.0168592] 0.0014594538\n",
            "65 [1.0160724] 0.0013264188\n",
            "66 [1.0153223] 0.0012055056\n",
            "67 [1.0146073] 0.0010956143\n",
            "68 [1.0139257] 0.0009957395\n",
            "69 [1.0132759] 0.00090498355\n",
            "70 [1.0126563] 0.00082249305\n",
            "71 [1.0120656] 0.00074752234\n",
            "72 [1.0115026] 0.0006793728\n",
            "73 [1.0109658] 0.0006174455\n",
            "74 [1.010454] 0.0005611634\n",
            "75 [1.0099663] 0.0005100051\n",
            "76 [1.0095012] 0.00046352003\n",
            "77 [1.0090579] 0.00042127477\n",
            "78 [1.0086352] 0.00038287535\n",
            "79 [1.0082322] 0.00034797692\n",
            "80 [1.007848] 0.00031626062\n",
            "81 [1.0074818] 0.00028742693\n",
            "82 [1.0071326] 0.0002612285\n",
            "83 [1.0067998] 0.0002374169\n",
            "84 [1.0064825] 0.00021577669\n",
            "85 [1.0061799] 0.00019610378\n",
            "86 [1.0058916] 0.00017822855\n",
            "87 [1.0056167] 0.00016198232\n",
            "88 [1.0053545] 0.00014721899\n",
            "89 [1.0051047] 0.00013379891\n",
            "90 [1.0048665] 0.000121603174\n",
            "91 [1.0046394] 0.00011051781\n",
            "92 [1.0044229] 0.00010044492\n",
            "93 [1.0042166] 9.128967e-05\n",
            "94 [1.0040197] 8.2969105e-05\n",
            "95 [1.0038321] 7.540534e-05\n",
            "96 [1.0036533] 6.853002e-05\n",
            "97 [1.0034828] 6.2283725e-05\n",
            "98 [1.0033203] 5.6606787e-05\n",
            "99 [1.0031654] 5.1449082e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 경사하강 알고리즘을 이용해 가중치 학습시키기\n",
        "#### 실제론 아래와 같은 방법을 사용한다.\n",
        "#### placeholder로 지정 후 실행할 때 feed_dict로 데이터값을 넘겨받는 형식\n",
        "# 앞에서 데이터 가져오기\n",
        "X_train = tf.placeholder(tf.float32, shape=[None]) #None 입력 : shape의 크기에 영향을 받지 않고 값을 받아준다.\n",
        "y_train = tf.placeholder(tf.float32, shape=[None]) #pandas에서 (-1, -1)과 같은\n",
        "W = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
        "#b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
        "hypot = W * X_train\n",
        "cost = tf.reduce_mean(tf.square(hypot-y_train))\n",
        "\n",
        "\n",
        "#### 최저비용학습을 위한 경사하강알고리즘(gradient decent) :: 미분 개념\n",
        "# x의 순간변화값과 y값의 차이를 구하는 것?\n",
        "gradient = tf.reduce_mean((hypot-y_train)*X_train)\n",
        "learning_rate = 0.05\n",
        "descent = W - (learning_rate * gradient)\n",
        "# 원래는 W = W - (learning_rate * gradient). 그런데 왜 못하냐면 갱신이 안되서. 코드상으로 구현 불가\n",
        "# 그래서 assign이란 함수를 쓴다.\n",
        "\n",
        "updata = W.assign(descent)\n",
        "\n",
        "# ================================ Gradient Descent 그래프 완성\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# learning_rate는 내려가는 보폭 / 반복문 range는 훈련 횟수.\n",
        "# 적절한 값 분포가 필요하다. 예)learning_rate를 너무 줄이면 훈련 횟수를 엄청 늘려야 한다.\n",
        "\n",
        "for step in range(40):\n",
        "  _, weight, c = sess.run([updata, W, cost], feed_dict={X_train:[1,2,3,4,5], y_train:[1,2,3,4,5]})\n",
        "  print(step, weight, c)\n",
        "  #여러개의 노드를 한번에 묶어 진행 가능 []로 묶어줘\n",
        "\n",
        "sess.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWDe_8KZgDNd",
        "outputId": "8ae46b80-ec1f-45df-b169-f544add80461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [0.9186523] 0.3594663\n",
            "1 [0.9633935] 0.0727919\n",
            "2 [0.98352706] 0.01474039\n",
            "3 [0.99258715] 0.0029849322\n",
            "4 [0.9966642] 0.00060445076\n",
            "5 [0.9984989] 0.0001224011\n",
            "6 [0.9993245] 2.4785764e-05\n",
            "7 [0.999696] 5.019262e-06\n",
            "8 [0.9998632] 1.0166119e-06\n",
            "9 [0.9999384] 2.0577586e-07\n",
            "10 [0.9999723] 4.1728107e-08\n",
            "11 [0.99998754] 8.461942e-09\n",
            "12 [0.9999944] 1.7124002e-09\n",
            "13 [0.9999975] 3.4584957e-10\n",
            "14 [0.99999887] 6.87038e-11\n",
            "15 [0.99999946] 1.4163249e-11\n",
            "16 [0.99999976] 3.4027891e-12\n",
            "17 [0.9999999] 5.2295944e-13\n",
            "18 [0.99999994] 1.5063507e-13\n",
            "19 [1.] 7.176482e-14\n",
            "20 [1.] 0.0\n",
            "21 [1.] 0.0\n",
            "22 [1.] 0.0\n",
            "23 [1.] 0.0\n",
            "24 [1.] 0.0\n",
            "25 [1.] 0.0\n",
            "26 [1.] 0.0\n",
            "27 [1.] 0.0\n",
            "28 [1.] 0.0\n",
            "29 [1.] 0.0\n",
            "30 [1.] 0.0\n",
            "31 [1.] 0.0\n",
            "32 [1.] 0.0\n",
            "33 [1.] 0.0\n",
            "34 [1.] 0.0\n",
            "35 [1.] 0.0\n",
            "36 [1.] 0.0\n",
            "37 [1.] 0.0\n",
            "38 [1.] 0.0\n",
            "39 [1.] 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 실제로 사용할 땐 만들어져 있는 모델을 사용합니다.\n",
        "X_train = tf.placeholder(tf.float32, shape=[None])#None 입력 : shape의 크기에 영향을 받지 않고 값을 받아준다.\n",
        "y_train = tf.placeholder(tf.float32, shape=[None])#pandas에서 (-1, -1)과 같은\n",
        "W = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
        "b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
        "hypot = W * X_train + b\n",
        "cost = tf.reduce_mean(tf.square(hypot-y_train))\n",
        "\n",
        "\n",
        "#### 최저비용학습을 위한 경사하강알고리즘(gradient decent) :: 미분 개념\n",
        "update = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "# ================================ Gradient Descent 그래프 완성\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# learning_rate는 내려가는 보폭 / 반복문 range는 훈련 횟수.\n",
        "# 적절한 값 분포가 필요하다. 예)learning_rate를 너무 줄이면 훈련 횟수를 엄청 늘려야 한다.\n",
        "\n",
        "for step in range(40):\n",
        "  _, weight, c, bias = sess.run([update, W, cost, b], feed_dict={X_train:[1,2,3,4,5], y_train:[2.1,3.1,4.1,5.1,6.1]})\n",
        "  print(step, weight, c, bias)\n",
        "#여러개의 노드를 한번에 묶어 진행 가능 []로 묶어줘\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03679dG2lCpm",
        "outputId": "69a31cdc-7f73-48d0-ee23-8149f6bd45cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [-1.2100062] 92.28515 [1.0941459]\n",
            "1 [-0.72345364] 53.803062 [1.2268634]\n",
            "2 [-0.35190564] 31.377447 [1.3277333]\n",
            "3 [-0.06815043] 18.308756 [1.404293]\n",
            "4 [0.14858508] 10.692808 [1.4622961]\n",
            "5 [0.31415856] 6.2544527 [1.5061351]\n",
            "6 [0.44067556] 3.6678429 [1.5391629]\n",
            "7 [0.5373772] 2.1603389 [1.5639391]\n",
            "8 [0.6113179] 1.2816851 [1.5824177]\n",
            "9 [0.66788286] 0.76949567 [1.5960903]\n",
            "10 [0.7111832] 0.47086444 [1.6060956]\n",
            "11 [0.74435717] 0.29668593 [1.6133027]\n",
            "12 [0.7698004] 0.19503239 [1.6183752]\n",
            "13 [0.7893418] 0.13564461 [1.6218196]\n",
            "14 [0.80437744] 0.10088787 [1.6240227]\n",
            "15 [0.81597304] 0.080485746 [1.6252797]\n",
            "16 [0.8249422] 0.06845016 [1.6258156]\n",
            "17 [0.83190596] 0.06129091 [1.6258028]\n",
            "18 [0.83733845] 0.056974374 [1.6253723]\n",
            "19 [0.8416016] 0.05431541 [1.6246245]\n",
            "20 [0.8449718] 0.05262343 [1.6236359]\n",
            "21 [0.8476598] 0.05149579 [1.6224649]\n",
            "22 [0.84982675] 0.050697982 [1.621156]\n",
            "23 [0.8515955] 0.050093494 [1.6197432]\n",
            "24 [0.8530599] 0.049602475 [1.6182526]\n",
            "25 [0.85429156] 0.049178563 [1.616704]\n",
            "26 [0.8553452] 0.048794676 [1.6151124]\n",
            "27 [0.8562625] 0.048435055 [1.6134895]\n",
            "28 [0.8570754] 0.048090473 [1.611844]\n",
            "29 [0.8578082] 0.04775553 [1.6101825]\n",
            "30 [0.85847944] 0.047427196 [1.6085104]\n",
            "31 [0.8591033] 0.04710353 [1.6068314]\n",
            "32 [0.85969067] 0.04678353 [1.6051486]\n",
            "33 [0.8602498] 0.046466548 [1.6034641]\n",
            "34 [0.86078703] 0.046152163 [1.6017798]\n",
            "35 [0.8613071] 0.045840204 [1.600097]\n",
            "36 [0.8618137] 0.045530517 [1.5984166]\n",
            "37 [0.8623097] 0.04522305 [1.5967394]\n",
            "38 [0.8627972] 0.044917714 [1.5950661]\n",
            "39 [0.86327785] 0.04461443 [1.5933969]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### testing\n",
        "\n",
        "sess.run(hypot, feed_dict={X_train:[27]})"
      ],
      "metadata": {
        "id": "jPTgU5l-qTAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "IpUANOqPtjZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 응용예제\n",
        "- X = [1, 2, 3, 4, 5, 6, 7] :: 시간\n",
        "- y = [25000, 55000, 75000, 110000, 128000, 1550000, 180000] :: 입금받은 돈\n",
        "- 그럼 8시간 일하면 얼마벌 수 있어?"
      ],
      "metadata": {
        "id": "4RRAGSkYu6m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.placeholder(tf.float32, shape=[None])#None 입력 : shape의 크기에 영향을 받지 않고 값을 받아준다.\n",
        "y_train = tf.placeholder(tf.float32, shape=[None])#pandas에서 (-1, -1)과 같은\n",
        "W = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
        "b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
        "hypot = W * X_train + b\n",
        "cost = tf.reduce_mean(tf.square(hypot-y_train))\n",
        "\n",
        "\n",
        "#### 최저비용학습을 위한 경사하강알고리즘(gradient decent) :: 미분 개념\n",
        "update = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "# ================================ Gradient Descent 그래프 완성\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# learning_rate는 내려가는 보폭 / 반복문 range는 훈련 횟수.\n",
        "# 적절한 값 분포가 필요하다. 예)learning_rate를 너무 줄이면 훈련 횟수를 엄청 늘려야 한다.\n",
        "\n",
        "for step in range(2000):\n",
        "  _, c = sess.run([update, cost], feed_dict={X_train:[1,2,3,4,5,6,7], y_train:[25000, 55000, 75000, 110000, 128000, 155000, 180000]})\n",
        "  if step % 100 == 0:\n",
        "    print(step, c)\n",
        "#여러개의 노드를 한번에 묶어 진행 가능 []로 묶어줘"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WL7cJg9vRTA",
        "outputId": "bc3d36f2-11e3-4e9f-883a-e341335cbfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 13455193000.0\n",
            "100 9835551.0\n",
            "200 9189372.0\n",
            "300 8890315.0\n",
            "400 8751903.0\n",
            "500 8687842.0\n",
            "600 8658195.0\n",
            "700 8644476.0\n",
            "800 8638122.0\n",
            "900 8635186.0\n",
            "1000 8633823.0\n",
            "1100 8633194.0\n",
            "1200 8632904.0\n",
            "1300 8632770.0\n",
            "1400 8632705.0\n",
            "1500 8632679.0\n",
            "1600 8632659.0\n",
            "1700 8632661.0\n",
            "1800 8632664.0\n",
            "1900 8632652.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.run(hypot, feed_dict={X_train:[8]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgFnENTQvdFm",
        "outputId": "e0c94133-5c01-44b9-9706-61012f58bc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([206570.42], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "CHMmwEj6wkoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 다중선형회귀"
      ],
      "metadata": {
        "id": "ev52AJRrkAua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1_data = [73., 93., 89., 96., 73.]\n",
        "x2_data = [80., 88., 91., 98., 66.]\n",
        "x3_data = [75., 93., 90., 100., 70.]\n",
        "y_data = [152., 185., 180., 196., 142.]"
      ],
      "metadata": {
        "id": "SWw3hbTzxEb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. 단순하게 처리하는 경우\n",
        "- 그냥 공식 대입해 해결"
      ],
      "metadata": {
        "id": "bfdujiv4mJ_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = tf.placeholder(tf.float32, shape=[None])\n",
        "x2 = tf.placeholder(tf.float32, shape=[None])\n",
        "x3 = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "Y = tf.placeholder(tf.float32, shape=[None])"
      ],
      "metadata": {
        "id": "A9IXmUFFmFV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 가설설정 : 다중선형회귀분석\n",
        "#### y = w1x1 + w2x2 + w3x3 + bias\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([1]))\n",
        "W2 = tf.Variable(tf.random_normal([1]))\n",
        "W3 = tf.Variable(tf.random_normal([1]))\n",
        "b = tf.Variable(tf.random_normal([1]))\n",
        "\n",
        "hypot = (W1*x1) + (W2*x2) + (W3*x3) + b\n",
        "\n",
        "#목표 설정 완료. 비용 계산\n",
        "cost = tf.reduce_mean(tf.square(hypot - Y))\n",
        "\n",
        "#최소비용계산\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(cost)\n",
        "\n",
        "# 훈련시작\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(20000):\n",
        "  _, c, h = sess.run([train, cost, hypot], feed_dict={x1:x1_data, x2:x2_data, x3:x3_data, Y:y_data})\n",
        "  if step % 100 == 0:\n",
        "    print(step, c, h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g48k3-Z0mRAk",
        "outputId": "4687dd89-8b51-40e2-9fb4-60ad986f488c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 105763.516 [-136.1466  -162.69337 -160.53018 -176.94412 -122.29204]\n",
            "100 0.61021423 [152.40242 184.14342 181.20216 195.19809 142.25911]\n",
            "200 0.6046623 [152.3907  184.15117 181.19821 195.19801 142.26694]\n",
            "300 0.5993197 [152.37927 184.15874 181.19435 195.19801 142.27454]\n",
            "400 0.59419286 [152.36812 184.16614 181.1906  195.19804 142.2819 ]\n",
            "500 0.5892533 [152.35722 184.17331 181.18689 195.19812 142.289  ]\n",
            "600 0.5844944 [152.34659 184.18031 181.18326 195.19824 142.29588]\n",
            "700 0.57991207 [152.3362  184.18715 181.17973 195.19844 142.30255]\n",
            "800 0.575504 [152.32608 184.19382 181.17628 195.19867 142.309  ]\n",
            "900 0.5712367 [152.3162  184.20032 181.17288 195.19894 142.31523]\n",
            "1000 0.5671219 [152.30653 184.20668 181.16957 195.19926 142.32127]\n",
            "1100 0.56314534 [152.2971  184.21286 181.16632 195.19965 142.3271 ]\n",
            "1200 0.55931294 [152.2879  184.21889 181.16315 195.20006 142.33275]\n",
            "1300 0.5556034 [152.27892 184.2248  181.16005 195.20052 142.33821]\n",
            "1400 0.55201447 [152.27014 184.23055 181.15701 195.20102 142.34349]\n",
            "1500 0.54853475 [152.26154 184.23616 181.15404 195.20155 142.34857]\n",
            "1600 0.54517025 [152.25316 184.24161 181.15111 195.20213 142.3535 ]\n",
            "1700 0.54191107 [152.245   184.24695 181.14825 195.20274 142.35825]\n",
            "1800 0.5387401 [152.23698 184.25218 181.14546 195.2034  142.36284]\n",
            "1900 0.535684 [152.22917 184.25725 181.14272 195.20406 142.36728]\n",
            "2000 0.5326941 [152.22156 184.26222 181.14005 195.20482 142.37155]\n",
            "2100 0.5298029 [152.21411 184.26706 181.1374  195.20555 142.37567]\n",
            "2200 0.5270054 [152.20682 184.27177 181.13484 195.20631 142.37965]\n",
            "2300 0.5242535 [152.19969 184.2764  181.1323  195.20714 142.3835 ]\n",
            "2400 0.5215903 [152.19275 184.28093 181.12984 195.208   142.3872 ]\n",
            "2500 0.51899683 [152.18591 184.2853  181.1274  195.20885 142.39073]\n",
            "2600 0.5164662 [152.17929 184.2896  181.12502 195.20976 142.39418]\n",
            "2700 0.51401454 [152.17279 184.29381 181.12271 195.21068 142.39749]\n",
            "2800 0.5116031 [152.16643 184.2979  181.1204  195.21164 142.40068]\n",
            "2900 0.50925446 [152.16019 184.30186 181.11813 195.21259 142.4037 ]\n",
            "3000 0.5069655 [152.15411 184.30577 181.11595 195.21361 142.40665]\n",
            "3100 0.5047258 [152.14818 184.30959 181.11378 195.21463 142.40948]\n",
            "3200 0.50253475 [152.14236 184.31334 181.1117  195.2157  142.4122 ]\n",
            "3300 0.5003979 [152.13669 184.31697 181.10962 195.21677 142.41481]\n",
            "3400 0.4982924 [152.13112 184.32053 181.10756 195.21786 142.41733]\n",
            "3500 0.49624243 [152.12569 184.32399 181.10556 195.21898 142.41972]\n",
            "3600 0.49422282 [152.12035 184.3274  181.10359 195.22012 142.42203]\n",
            "3700 0.49224678 [152.11514 184.3307  181.10167 195.2213  142.42422]\n",
            "3800 0.49031943 [152.11003 184.33394 181.09978 195.22246 142.42635]\n",
            "3900 0.48841476 [152.10503 184.33711 181.0979  195.22363 142.42836]\n",
            "4000 0.48655766 [152.10016 184.34021 181.09608 195.22484 142.4303 ]\n",
            "4100 0.4847209 [152.09535 184.34323 181.09428 195.22607 142.43214]\n",
            "4200 0.48291406 [152.09068 184.34619 181.09251 195.22733 142.4339 ]\n",
            "4300 0.48114252 [152.08607 184.34909 181.09079 195.22859 142.43558]\n",
            "4400 0.47940883 [152.08159 184.3519  181.08908 195.22984 142.43716]\n",
            "4500 0.47768974 [152.07718 184.35468 181.08742 195.23116 142.43869]\n",
            "4600 0.4759931 [152.07286 184.35738 181.08575 195.23247 142.44014]\n",
            "4700 0.4743325 [152.06865 184.36003 181.08415 195.2338  142.44151]\n",
            "4800 0.47269836 [152.0645  184.36261 181.08257 195.23512 142.44281]\n",
            "4900 0.47107568 [152.06046 184.36516 181.081   195.23648 142.44406]\n",
            "5000 0.46947965 [152.05649 184.36761 181.07944 195.23784 142.44522]\n",
            "5100 0.46791238 [152.05258 184.37004 181.07793 195.2392  142.44632]\n",
            "5200 0.466363 [152.04878 184.37244 181.07646 195.2406  142.44737]\n",
            "5300 0.464837 [152.045   184.37471 181.07495 195.24194 142.44833]\n",
            "5400 0.46332446 [152.04135 184.37701 181.07353 195.24338 142.44928]\n",
            "5500 0.46182972 [152.03773 184.37921 181.07208 195.24477 142.45012]\n",
            "5600 0.4603568 [152.03421 184.38141 181.07071 195.24622 142.45094]\n",
            "5700 0.45889372 [152.03075 184.38353 181.06932 195.24765 142.45168]\n",
            "5800 0.45745593 [152.02736 184.38564 181.06798 195.2491  142.4524 ]\n",
            "5900 0.45602384 [152.024   184.38765 181.0666  195.25053 142.45302]\n",
            "6000 0.45461935 [152.02075 184.38968 181.06532 195.25201 142.45363]\n",
            "6100 0.45322713 [152.01752 184.39162 181.06401 195.25346 142.45416]\n",
            "6200 0.45184675 [152.01439 184.39354 181.06274 195.25496 142.45467]\n",
            "6300 0.45048708 [152.01128 184.3954  181.06146 195.25641 142.45512]\n",
            "6400 0.44911557 [152.00824 184.39726 181.06023 195.25795 142.45552]\n",
            "6500 0.4477938 [152.00526 184.39906 181.05902 195.25943 142.4559 ]\n",
            "6600 0.44646534 [152.00232 184.40083 181.0578  195.26091 142.4562 ]\n",
            "6700 0.44513974 [151.99947 184.40257 181.05661 195.26245 142.45648]\n",
            "6800 0.44382912 [151.99663 184.40428 181.05542 195.26398 142.45674]\n",
            "6900 0.44254747 [151.99384 184.40591 181.05426 195.26547 142.45691]\n",
            "7000 0.44126529 [151.99112 184.40756 181.05313 195.26703 142.45709]\n",
            "7100 0.4399902 [151.98845 184.4092  181.05202 195.26859 142.45723]\n",
            "7200 0.43873662 [151.98578 184.41075 181.05087 195.27007 142.4573 ]\n",
            "7300 0.43748838 [151.98318 184.41228 181.04979 195.27164 142.45735]\n",
            "7400 0.43625277 [151.98065 184.4138  181.04872 195.2732  142.45738]\n",
            "7500 0.43502155 [151.97813 184.4153  181.04764 195.27473 142.45738]\n",
            "7600 0.433807 [151.97566 184.41673 181.04657 195.27628 142.45734]\n",
            "7700 0.43258137 [151.97324 184.41817 181.04552 195.27786 142.45724]\n",
            "7800 0.43138337 [151.97086 184.41959 181.0445  195.27943 142.45717]\n",
            "7900 0.43020138 [151.96849 184.42094 181.04347 195.28098 142.45703]\n",
            "8000 0.42901158 [151.96617 184.4223  181.04245 195.28255 142.45688]\n",
            "8100 0.4278338 [151.96391 184.42363 181.04146 195.28412 142.45667]\n",
            "8200 0.4266759 [151.96169 184.42493 181.04048 195.28569 142.45647]\n",
            "8300 0.42551818 [151.95947 184.42622 181.03952 195.28728 142.45624]\n",
            "8400 0.42436638 [151.95728 184.42749 181.03854 195.28885 142.456  ]\n",
            "8500 0.42321888 [151.95514 184.42871 181.03758 195.29044 142.45569]\n",
            "8600 0.42208132 [151.95305 184.42992 181.03664 195.29202 142.45537]\n",
            "8700 0.42094994 [151.95097 184.4311  181.0357  195.29362 142.45503]\n",
            "8800 0.41983548 [151.94894 184.4323  181.0348  195.29521 142.45468]\n",
            "8900 0.41871315 [151.94691 184.43347 181.03389 195.29683 142.45433]\n",
            "9000 0.4176143 [151.94492 184.43462 181.03299 195.2984  142.45395]\n",
            "9100 0.41651756 [151.94295 184.43571 181.03209 195.29997 142.4535 ]\n",
            "9200 0.41541344 [151.94101 184.43681 181.03119 195.30159 142.45308]\n",
            "9300 0.41433483 [151.93912 184.43788 181.03033 195.30318 142.45259]\n",
            "9400 0.4132544 [151.93726 184.43893 181.02946 195.30478 142.45212]\n",
            "9500 0.41216803 [151.93542 184.44    181.02861 195.30641 142.45163]\n",
            "9600 0.41110548 [151.93358 184.44102 181.02776 195.308   142.45113]\n",
            "9700 0.41005746 [151.93178 184.44205 181.02693 195.30959 142.45062]\n",
            "9800 0.40900412 [151.92998 184.44305 181.0261  195.31117 142.45009]\n",
            "9900 0.40795532 [151.92822 184.44405 181.02527 195.31277 142.44954]\n",
            "10000 0.40691504 [151.92647 184.445   181.02444 195.31436 142.44896]\n",
            "10100 0.40586525 [151.92476 184.44597 181.02362 195.31598 142.44836]\n",
            "10200 0.40484563 [151.92307 184.4469  181.02283 195.31757 142.44777]\n",
            "10300 0.40382648 [151.9214  184.4478  181.02203 195.31915 142.44713]\n",
            "10400 0.4028036 [151.91977 184.4487  181.02124 195.32077 142.4465 ]\n",
            "10500 0.40178865 [151.91814 184.44962 181.02048 195.32237 142.44583]\n",
            "10600 0.40077585 [151.91653 184.45049 181.01968 195.32397 142.44518]\n",
            "10700 0.3997671 [151.91496 184.45137 181.01892 195.32559 142.4445 ]\n",
            "10800 0.39877713 [151.91338 184.45221 181.01816 195.32718 142.44382]\n",
            "10900 0.3977818 [151.91185 184.45308 181.01741 195.3288  142.44315]\n",
            "11000 0.39680177 [151.91031 184.45392 181.01666 195.33038 142.44246]\n",
            "11100 0.39582455 [151.90878 184.45474 181.01591 195.33197 142.44176]\n",
            "11200 0.39485168 [151.90727 184.45558 181.0152  195.33357 142.44104]\n",
            "11300 0.39387625 [151.90578 184.45638 181.01443 195.33516 142.44032]\n",
            "11400 0.39291546 [151.9043  184.45718 181.01372 195.33675 142.43959]\n",
            "11500 0.39196032 [151.90283 184.45795 181.01299 195.33833 142.43886]\n",
            "11600 0.39100283 [151.9014  184.45876 181.01227 195.33992 142.43813]\n",
            "11700 0.39005992 [151.89996 184.45952 181.01157 195.34149 142.43735]\n",
            "11800 0.38912085 [151.89853 184.46028 181.01086 195.34308 142.4366 ]\n",
            "11900 0.3881814 [151.89713 184.46103 181.01015 195.34465 142.43584]\n",
            "12000 0.38724875 [151.89574 184.46176 181.00946 195.34625 142.43506]\n",
            "12100 0.3863193 [151.89438 184.46251 181.00877 195.34782 142.43427]\n",
            "12200 0.3853926 [151.89302 184.46323 181.00807 195.3494  142.43347]\n",
            "12300 0.38447118 [151.89168 184.46394 181.00739 195.35097 142.43266]\n",
            "12400 0.38356763 [151.89034 184.46465 181.00673 195.35254 142.43185]\n",
            "12500 0.38265505 [151.88902 184.46535 181.00606 195.35413 142.43105]\n",
            "12600 0.38174313 [151.88771 184.46603 181.00537 195.3557  142.43022]\n",
            "12700 0.3808498 [151.8864  184.46672 181.00471 195.35725 142.4294 ]\n",
            "12800 0.3799519 [151.88512 184.46739 181.00404 195.35881 142.42856]\n",
            "12900 0.37905487 [151.88383 184.46806 181.00337 195.36038 142.42773]\n",
            "13000 0.37817776 [151.88258 184.46872 181.00275 195.36195 142.4269 ]\n",
            "13100 0.37729192 [151.88133 184.46938 181.00209 195.36351 142.42604]\n",
            "13200 0.3764179 [151.88008 184.47003 181.00146 195.36507 142.42517]\n",
            "13300 0.3755402 [151.87886 184.47066 181.00081 195.36664 142.42433]\n",
            "13400 0.37467176 [151.87764 184.47127 181.00015 195.36816 142.42345]\n",
            "13500 0.3738148 [151.87642 184.47191 180.99954 195.36972 142.42259]\n",
            "13600 0.372939 [151.87521 184.47253 180.99889 195.37128 142.4217 ]\n",
            "13700 0.37209326 [151.87405 184.47314 180.99829 195.37283 142.42084]\n",
            "13800 0.3712411 [151.87288 184.47372 180.99765 195.37436 142.41994]\n",
            "13900 0.37038767 [151.87172 184.47435 180.99704 195.37592 142.41905]\n",
            "14000 0.36955047 [151.87056 184.47491 180.99643 195.37746 142.41815]\n",
            "14100 0.3687056 [151.86942 184.47551 180.99582 195.379   142.41724]\n",
            "14200 0.36786398 [151.86829 184.47609 180.99521 195.38055 142.41634]\n",
            "14300 0.3670388 [151.86716 184.47665 180.99461 195.38208 142.41542]\n",
            "14400 0.36620814 [151.86604 184.4772  180.994   195.38362 142.4145 ]\n",
            "14500 0.3653807 [151.86496 184.47777 180.99341 195.38516 142.41359]\n",
            "14600 0.36456665 [151.86386 184.47832 180.99283 195.38669 142.41266]\n",
            "14700 0.36374384 [151.86278 184.47887 180.99225 195.38824 142.41171]\n",
            "14800 0.36292404 [151.8617  184.4794  180.99164 195.38977 142.41078]\n",
            "14900 0.36211437 [151.86063 184.47995 180.99106 195.3913  142.40985]\n",
            "15000 0.36130983 [151.85956 184.48048 180.99048 195.39282 142.40892]\n",
            "15100 0.36051345 [151.85849 184.48102 180.9899  195.39433 142.408  ]\n",
            "15200 0.35971737 [151.85744 184.48157 180.98933 195.39586 142.40709]\n",
            "15300 0.35893196 [151.85638 184.48209 180.98877 195.39735 142.40614]\n",
            "15400 0.35813847 [151.85535 184.48262 180.98819 195.39888 142.40524]\n",
            "15500 0.3573594 [151.8543  184.48314 180.98763 195.40038 142.40431]\n",
            "15600 0.35657164 [151.85326 184.48366 180.98705 195.40189 142.40338]\n",
            "15700 0.35580328 [151.85222 184.48419 180.9865  195.40338 142.40248]\n",
            "15800 0.35503063 [151.85117 184.4847  180.98593 195.40488 142.40154]\n",
            "15900 0.35425967 [151.85016 184.4852  180.98535 195.40636 142.40062]\n",
            "16000 0.3535018 [151.84914 184.48569 180.9848  195.40784 142.39967]\n",
            "16100 0.3527345 [151.84813 184.48619 180.98424 195.40933 142.39874]\n",
            "16200 0.35197824 [151.84715 184.48666 180.98369 195.41081 142.39778]\n",
            "16300 0.35122046 [151.84616 184.48715 180.98314 195.41231 142.39684]\n",
            "16400 0.3504703 [151.84518 184.48763 180.98259 195.41379 142.39589]\n",
            "16500 0.3497188 [151.84421 184.48811 180.98204 195.41527 142.39494]\n",
            "16600 0.34897357 [151.84326 184.48857 180.9815  195.41676 142.39398]\n",
            "16700 0.34823152 [151.8423  184.48904 180.98097 195.41824 142.39302]\n",
            "16800 0.3475027 [151.84134 184.4895  180.98044 195.41971 142.39209]\n",
            "16900 0.3467721 [151.8404  184.48999 180.97992 195.42117 142.39114]\n",
            "17000 0.34603995 [151.83945 184.49045 180.97939 195.42265 142.3902 ]\n",
            "17100 0.34530932 [151.8385  184.49094 180.97885 195.42412 142.38927]\n",
            "17200 0.34459013 [151.83755 184.4914  180.97832 195.42557 142.38834]\n",
            "17300 0.34387892 [151.8366  184.49187 180.9778  195.42702 142.38742]\n",
            "17400 0.3431584 [151.83565 184.49232 180.97725 195.42845 142.38647]\n",
            "17500 0.34244826 [151.83472 184.49278 180.97673 195.42992 142.38556]\n",
            "17600 0.34175014 [151.8338  184.49323 180.97623 195.43135 142.38461]\n",
            "17700 0.34103456 [151.83287 184.49367 180.97568 195.43279 142.38365]\n",
            "17800 0.34033933 [151.83195 184.4941  180.97516 195.43422 142.38272]\n",
            "17900 0.33964092 [151.83104 184.49455 180.97466 195.43565 142.38176]\n",
            "18000 0.33893996 [151.83014 184.49498 180.97414 195.4371  142.3808 ]\n",
            "18100 0.33825788 [151.82925 184.49539 180.97363 195.43852 142.37985]\n",
            "18200 0.33756337 [151.82838 184.49583 180.97313 195.43997 142.3789 ]\n",
            "18300 0.33687812 [151.8275  184.49626 180.97263 195.4414  142.37796]\n",
            "18400 0.3361941 [151.82658 184.49669 180.9721  195.44283 142.37701]\n",
            "18500 0.33551747 [151.82573 184.49713 180.97162 195.44426 142.3761 ]\n",
            "18600 0.3348394 [151.82481 184.49757 180.9711  195.44566 142.37517]\n",
            "18700 0.3341778 [151.82393 184.498   180.9706  195.44705 142.37427]\n",
            "18800 0.33350927 [151.82304 184.49844 180.97011 195.44847 142.37334]\n",
            "18900 0.332846 [151.82216 184.49886 180.9696  195.44987 142.3724 ]\n",
            "19000 0.33218834 [151.8213  184.49928 180.96912 195.45126 142.37148]\n",
            "19100 0.3315265 [151.82042 184.49968 180.9686  195.45265 142.37053]\n",
            "19200 0.33087283 [151.81955 184.50009 180.96811 195.45406 142.3696 ]\n",
            "19300 0.33021632 [151.81871 184.5005  180.96762 195.45546 142.36865]\n",
            "19400 0.3295701 [151.81786 184.50087 180.96712 195.45683 142.36769]\n",
            "19500 0.32892528 [151.81705 184.50128 180.96666 195.45825 142.36676]\n",
            "19600 0.32827878 [151.81621 184.50168 180.96617 195.45966 142.36583]\n",
            "19700 0.3276393 [151.81538 184.50208 180.96568 195.46103 142.3649 ]\n",
            "19800 0.32700577 [151.81454 184.50249 180.96523 195.46242 142.36395]\n",
            "19900 0.32637158 [151.81372 184.50288 180.96474 195.46379 142.36304]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "pgwNMhR6pchO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "psW5zrwox07V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. 데이터는 2차원으로\n",
        "- 복잡한 데이터, 2차원으로 묶어주면 성능 up\n",
        "- matrix, 행렬로 처리하는 것 의미한다.\n",
        "- 앞으로 머신러닝에선 2차원 데이터를 많이 쓸 것\n",
        "- 테이블 형태로 많이 들어옴\n",
        "- pd.values로 풀어주면 자동으로 2차가 되는 것을 제 1원칙으로\n",
        "- 이 외 다른 여러 방안도 알아두기"
      ],
      "metadata": {
        "id": "YF_O8bi8zGMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "x1_data = [73., 93., 89., 96., 73.]\n",
        "x2_data = [80., 88., 91., 98., 66.]\n",
        "x3_data = [75., 93., 90., 100., 70.]\n",
        "y_data = [152., 185., 180., 196., 142.]\n",
        "을\n",
        "\n",
        "x_data = [[73., 80., 75.], [93., 88., 93.],\n",
        "         [89., 91., 90.], [96., 98., 100.], [73., 66., 70.]]\n",
        "y_data = [[152.], [185.], [180.], [196.], [142.]]\n",
        "으로\n",
        "\"\"\"\n",
        "\n",
        "### 코드로 바꿀 수 있어야 한다.\n",
        "x1_data = [73., 93., 89., 96., 73.]\n",
        "x2_data = [80., 88., 91., 98., 66.]\n",
        "x3_data = [75., 93., 90., 100., 70.]\n",
        "y1_data = [152., 185., 180., 196., 142.]\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(len(x1_data)):\n",
        "    x_data.append([x1_data[i], x2_data[i], x3_data[i]])\n",
        "    y_data.append([y1_data[i]])\n",
        "\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7duJwTOyzRZf",
        "outputId": "c0e5efb2-46e0-4adb-9d3f-718afe45a5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[73.0, 80.0, 75.0], [93.0, 88.0, 93.0], [89.0, 91.0, 90.0], [96.0, 98.0, 100.0], [73.0, 66.0, 70.0]]\n",
            "[[152.0], [185.0], [180.0], [196.0], [142.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1_data = [73., 93., 89., 96., 73.]\n",
        "x2_data = [80., 88., 91., 98., 66.]\n",
        "x3_data = [75., 93., 90., 100., 70.]\n",
        "y_data = [152., 185., 180., 96., 142.]\n",
        "\n",
        "x_data = np.dstack([x1_data, x2_data, x3_data])[0]\n",
        "\n",
        "y_data = np.array([y_data]).reshape((-1,1))\n",
        "\n",
        "\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tIfbeN61lPn",
        "outputId": "71a7c570-04c4-4f61-d4d2-c8a16be00067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 73.  80.  75.]\n",
            " [ 93.  88.  93.]\n",
            " [ 89.  91.  90.]\n",
            " [ 96.  98. 100.]\n",
            " [ 73.  66.  70.]]\n",
            "[[152.]\n",
            " [185.]\n",
            " [180.]\n",
            " [ 96.]\n",
            " [142.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1_data = [73., 93., 89., 96., 73.]\n",
        "x2_data = [80., 88., 91., 98., 66.]\n",
        "x3_data = [75., 93., 90., 100., 70.]\n",
        "y_data = [152., 185., 180., 96., 142.]\n",
        "\n",
        "x_data = np.dstack([x1_data, x2_data, x3_data])[0]\n",
        "\n",
        "y_data = np.array([y_data]).reshape((-1,1))\n",
        "\n",
        "\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "id": "M6xyBfHn2JF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 강사님이 알려주신 방법\n",
        "\n",
        "x1_data = [73., 93., 89., 96., 73.]\n",
        "x2_data = [80., 88., 91., 98., 66.]\n",
        "x3_data = [75., 93., 90., 100., 70.]\n",
        "y_data = [152., 185., 180., 96., 142.]\n",
        "\n",
        "x_data = pd.DataFrame(zip(x1_data, x2_data, x3_data)).values\n",
        "y_data = pd.DataFrame(y_data).values\n",
        "\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yogJPEdh22oe",
        "outputId": "bcf46e26-ca22-43a9-9848-7a678d85f471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 73.  80.  75.]\n",
            " [ 93.  88.  93.]\n",
            " [ 89.  91.  90.]\n",
            " [ 96.  98. 100.]\n",
            " [ 73.  66.  70.]]\n",
            "[[152.]\n",
            " [185.]\n",
            " [180.]\n",
            " [ 96.]\n",
            " [142.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1_data = [73., 93., 89., 96., 73.]\n",
        "x2_data = [80., 88., 91., 98., 66.]\n",
        "x3_data = [75., 93., 90., 100., 70.]\n",
        "y_data = [152., 185., 180., 96., 142.]\n",
        "\n",
        "x_data = np.reshape([x1_data, x2_data, x3_data], (3, 5)).T\n",
        "y_data = np.reshape([y_data], (-1, 1))\n",
        "\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2gHD75d2_-x",
        "outputId": "b48b389c-b45a-42cf-ce34-f2b149e41606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[152.],\n",
              "       [185.],\n",
              "       [180.],\n",
              "       [ 96.],\n",
              "       [142.]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "tf.stack([x1_data, x2_data, x3_data], axis=1).eval(session=sess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwnVqUQR3kKl",
        "outputId": "96df8872-3890-4887-dd0d-0558868a318b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 73.,  80.,  75.],\n",
              "       [ 93.,  88.,  93.],\n",
              "       [ 89.,  91.,  90.],\n",
              "       [ 96.,  98., 100.],\n",
              "       [ 73.,  66.,  70.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [[73., 80., 75.], [93., 88., 93.],\n",
        "         [89., 91., 90.], [96., 98., 100.], [73., 66., 70.]]\n",
        "y_data = [[152.], [185.], [180.], [196.], [142.]]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFxPB1Pj4V18",
        "outputId": "1d0b00a7-9b5e-47c2-f030-1860a9b12697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[152.0, 185.0, 180.0, 96.0, 142.0]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = [[73., 80., 75.], [93., 88., 93.],\n",
        "         [89., 91., 90.], [96., 98., 100.], [73., 66., 70.]]\n",
        "y_data = [[152.], [185.], [180.], [196.], [142.]]\n",
        "\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, 3]) #2차원 행렬 값이 들어올 땐 열의 개수는 지정해주고 행의 개수는 None으로 처리해줘야 유지보수 측면에서 유리\n",
        "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([3, 1])) #입력값 x의 개수와 가중치 개수를 맞쳐줘야. 그래서 x의 shape와 w는 3으로 한 것\n",
        "b = tf.Variable(tf.random_normal([1]))\n",
        "\n",
        "hypot = tf.matmul(X, W) + b\n",
        "\n",
        "#목표 설정 완료. 비용 계산\n",
        "cost = tf.reduce_mean(tf.square(hypot - y))\n",
        "\n",
        "#최소비용계산\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(cost)\n",
        "\n",
        "# 훈련시작\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(20000):\n",
        "  _, c, h = sess.run([train, cost, hypot], feed_dict={X:X_data, y:y_data})\n",
        "  if step % 100 == 0:\n",
        "    print(step, c, h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQIitNug8K5b",
        "outputId": "fc473568-7111-4c1a-c801-9b18ddf82c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 46.362328 [[151.0655 ]\n",
            " [194.38281]\n",
            " [185.06664]\n",
            " [201.14835]\n",
            " [151.52496]]\n",
            "100 13.92249 [[146.21764]\n",
            " [188.28046]\n",
            " [179.1972 ]\n",
            " [194.74069]\n",
            " [146.81508]]\n",
            "200 13.1989565 [[146.35973]\n",
            " [188.18301]\n",
            " [179.24069]\n",
            " [194.7725 ]\n",
            " [146.68692]]\n",
            "300 12.513565 [[146.49802]\n",
            " [188.08817]\n",
            " [179.28302]\n",
            " [194.80347]\n",
            " [146.56218]]\n",
            "400 11.864337 [[146.6326 ]\n",
            " [187.99586]\n",
            " [179.3242 ]\n",
            " [194.83359]\n",
            " [146.44075]]\n",
            "500 11.249362 [[146.76361]\n",
            " [187.90605]\n",
            " [179.36429]\n",
            " [194.86295]\n",
            " [146.3226 ]]\n",
            "600 10.666834 [[146.89108]\n",
            " [187.8186 ]\n",
            " [179.40332]\n",
            " [194.89153]\n",
            " [146.2076 ]]\n",
            "700 10.115023 [[147.01515]\n",
            " [187.7335 ]\n",
            " [179.44127]\n",
            " [194.91931]\n",
            " [146.09564]]\n",
            "800 9.592352 [[147.1359 ]\n",
            " [187.65071]\n",
            " [179.47823]\n",
            " [194.94638]\n",
            " [145.98668]]\n",
            "900 9.097231 [[147.2534 ]\n",
            " [187.5701 ]\n",
            " [179.51418]\n",
            " [194.97275]\n",
            " [145.88063]]\n",
            "1000 8.6282215 [[147.36778]\n",
            " [187.49165]\n",
            " [179.5492 ]\n",
            " [194.9984 ]\n",
            " [145.77742]]\n",
            "1100 8.184003 [[147.4791 ]\n",
            " [187.41533]\n",
            " [179.58325]\n",
            " [195.02336]\n",
            " [145.67697]]\n",
            "1200 7.7631807 [[147.58743]\n",
            " [187.34103]\n",
            " [179.6164 ]\n",
            " [195.04767]\n",
            " [145.5792 ]]\n",
            "1300 7.3645372 [[147.69287]\n",
            " [187.26872]\n",
            " [179.64867]\n",
            " [195.07137]\n",
            " [145.48402]]\n",
            "1400 6.9869857 [[147.79547]\n",
            " [187.19835]\n",
            " [179.68007]\n",
            " [195.09438]\n",
            " [145.3914 ]]\n",
            "1500 6.6292863 [[147.89534]\n",
            " [187.12985]\n",
            " [179.71062]\n",
            " [195.11682]\n",
            " [145.30124]]\n",
            "1600 6.2905054 [[147.99251]\n",
            " [187.06317]\n",
            " [179.74033]\n",
            " [195.13864]\n",
            " [145.21349]]\n",
            "1700 5.9695706 [[148.0871 ]\n",
            " [186.9983 ]\n",
            " [179.76926]\n",
            " [195.1599 ]\n",
            " [145.12807]]\n",
            "1800 5.665583 [[148.17917]\n",
            " [186.93518]\n",
            " [179.79745]\n",
            " [195.18062]\n",
            " [145.04497]]\n",
            "1900 5.3775926 [[148.26877]\n",
            " [186.87372]\n",
            " [179.82486]\n",
            " [195.20076]\n",
            " [144.96405]]\n",
            "2000 5.1048307 [[148.35596]\n",
            " [186.81392]\n",
            " [179.85152]\n",
            " [195.22038]\n",
            " [144.8853 ]]\n",
            "2100 4.846458 [[148.44081]\n",
            " [186.7557 ]\n",
            " [179.87747]\n",
            " [195.23947]\n",
            " [144.80865]]\n",
            "2200 4.6017103 [[148.52339]\n",
            " [186.69905]\n",
            " [179.90273]\n",
            " [195.25807]\n",
            " [144.73405]]\n",
            "2300 4.369857 [[148.60376]\n",
            " [186.6439 ]\n",
            " [179.92729]\n",
            " [195.27615]\n",
            " [144.66142]]\n",
            "2400 4.1502523 [[148.68199]\n",
            " [186.59027]\n",
            " [179.95123]\n",
            " [195.29381]\n",
            " [144.59076]]\n",
            "2500 3.9422467 [[148.7581 ]\n",
            " [186.53806]\n",
            " [179.97452]\n",
            " [195.31094]\n",
            " [144.52196]]\n",
            "2600 3.7451873 [[148.83221]\n",
            " [186.48723]\n",
            " [179.9972 ]\n",
            " [195.32767]\n",
            " [144.45503]]\n",
            "2700 3.558532 [[148.90431]\n",
            " [186.43777]\n",
            " [180.01924]\n",
            " [195.34392]\n",
            " [144.38985]]\n",
            "2800 3.3817182 [[148.9745 ]\n",
            " [186.38965]\n",
            " [180.04071]\n",
            " [195.35977]\n",
            " [144.32643]]\n",
            "2900 3.2142193 [[149.04279]\n",
            " [186.34277]\n",
            " [180.06157]\n",
            " [195.3752 ]\n",
            " [144.26468]]\n",
            "3000 3.0555787 [[149.10925]\n",
            " [186.29718]\n",
            " [180.08188]\n",
            " [195.39018]\n",
            " [144.20459]]\n",
            "3100 2.9053054 [[149.17393]\n",
            " [186.2528 ]\n",
            " [180.10167]\n",
            " [195.4048 ]\n",
            " [144.1461 ]]\n",
            "3200 2.762943 [[149.2369 ]\n",
            " [186.20961]\n",
            " [180.12091]\n",
            " [195.41902]\n",
            " [144.08917]]\n",
            "3300 2.628112 [[149.29816]\n",
            " [186.1676 ]\n",
            " [180.13965]\n",
            " [195.4329 ]\n",
            " [144.03377]]\n",
            "3400 2.5003614 [[149.35779]\n",
            " [186.12668]\n",
            " [180.15787]\n",
            " [195.4464 ]\n",
            " [143.97983]]\n",
            "3500 2.379378 [[149.41582]\n",
            " [186.08688]\n",
            " [180.17561]\n",
            " [195.45953]\n",
            " [143.92734]]\n",
            "3600 2.2647595 [[149.47229]\n",
            " [186.04813]\n",
            " [180.19289]\n",
            " [195.47232]\n",
            " [143.87624]]\n",
            "3700 2.1562028 [[149.52724]\n",
            " [186.01042]\n",
            " [180.20966]\n",
            " [195.48476]\n",
            " [143.82649]]\n",
            "3800 2.053351 [[149.58073]\n",
            " [185.97371]\n",
            " [180.22601]\n",
            " [195.4969 ]\n",
            " [143.77809]]\n",
            "3900 1.9559475 [[149.63278]\n",
            " [185.938  ]\n",
            " [180.24191]\n",
            " [195.5087 ]\n",
            " [143.73097]]\n",
            "4000 1.8636783 [[149.68344]\n",
            " [185.90324]\n",
            " [180.25742]\n",
            " [195.5202 ]\n",
            " [143.68512]]\n",
            "4100 1.7762604 [[149.73274]\n",
            " [185.86942]\n",
            " [180.27248]\n",
            " [195.53142]\n",
            " [143.64047]]\n",
            "4200 1.693462 [[149.78073]\n",
            " [185.8365 ]\n",
            " [180.28714]\n",
            " [195.54233]\n",
            " [143.59703]]\n",
            "4300 1.6150414 [[149.82742]\n",
            " [185.80446]\n",
            " [180.30139]\n",
            " [195.55292]\n",
            " [143.55475]]\n",
            "4400 1.5407412 [[149.87286]\n",
            " [185.77327]\n",
            " [180.31528]\n",
            " [195.56328]\n",
            " [143.51358]]\n",
            "4500 1.4703646 [[149.91708]\n",
            " [185.74294]\n",
            " [180.32878]\n",
            " [195.57333]\n",
            " [143.4735 ]]\n",
            "4600 1.4037169 [[149.96011]\n",
            " [185.7134 ]\n",
            " [180.34193]\n",
            " [195.58313]\n",
            " [143.43451]]\n",
            "4700 1.3405498 [[150.002  ]\n",
            " [185.68463]\n",
            " [180.3547 ]\n",
            " [195.59268]\n",
            " [143.39653]]\n",
            "4800 1.2807388 [[150.04277]\n",
            " [185.65668]\n",
            " [180.36717]\n",
            " [195.60197]\n",
            " [143.35959]]\n",
            "4900 1.2240597 [[150.08246]\n",
            " [185.62946]\n",
            " [180.37929]\n",
            " [195.61105]\n",
            " [143.32362]]\n",
            "5000 1.1703966 [[150.12105]\n",
            " [185.60295]\n",
            " [180.39107]\n",
            " [195.61987]\n",
            " [143.28862]]\n",
            "5100 1.1195549 [[150.15862]\n",
            " [185.57718]\n",
            " [180.40254]\n",
            " [195.62845]\n",
            " [143.25453]]\n",
            "5200 1.0714014 [[150.19518]\n",
            " [185.5521 ]\n",
            " [180.4137 ]\n",
            " [195.63681]\n",
            " [143.22137]]\n",
            "5300 1.0257629 [[150.23076]\n",
            " [185.52765]\n",
            " [180.42456]\n",
            " [195.64497]\n",
            " [143.18907]]\n",
            "5400 0.9825538 [[150.26538]\n",
            " [185.50388]\n",
            " [180.43512]\n",
            " [195.6529 ]\n",
            " [143.15765]]\n",
            "5500 0.9416057 [[150.29909]\n",
            " [185.48074]\n",
            " [180.44542]\n",
            " [195.66061]\n",
            " [143.12704]]\n",
            "5600 0.90284157 [[150.33188]\n",
            " [185.45825]\n",
            " [180.45543]\n",
            " [195.66815]\n",
            " [143.09729]]\n",
            "5700 0.8661032 [[150.3638 ]\n",
            " [185.43636]\n",
            " [180.46518]\n",
            " [195.67549]\n",
            " [143.0683 ]]\n",
            "5800 0.83129567 [[150.39487]\n",
            " [185.41502]\n",
            " [180.47464]\n",
            " [195.68263]\n",
            " [143.04008]]\n",
            "5900 0.79833925 [[150.42508]\n",
            " [185.39426]\n",
            " [180.48387]\n",
            " [195.68959]\n",
            " [143.01262]]\n",
            "6000 0.7671239 [[150.4545 ]\n",
            " [185.37408]\n",
            " [180.49286]\n",
            " [195.69637]\n",
            " [142.9859 ]]\n",
            "6100 0.7375351 [[150.48312]\n",
            " [185.35442]\n",
            " [180.50156]\n",
            " [195.70294]\n",
            " [142.95987]]\n",
            "6200 0.70951635 [[150.51099]\n",
            " [185.3353 ]\n",
            " [180.51007]\n",
            " [195.70938]\n",
            " [142.93454]]\n",
            "6300 0.68298316 [[150.53809]\n",
            " [185.31668]\n",
            " [180.51833]\n",
            " [195.71564]\n",
            " [142.9099 ]]\n",
            "6400 0.6578352 [[150.56448]\n",
            " [185.29858]\n",
            " [180.52638]\n",
            " [195.72174]\n",
            " [142.8859 ]]\n",
            "6500 0.634023 [[150.59015]\n",
            " [185.28094]\n",
            " [180.53421]\n",
            " [195.72768]\n",
            " [142.86253]]\n",
            "6600 0.6114623 [[150.61513]\n",
            " [185.2638 ]\n",
            " [180.54182]\n",
            " [195.73346]\n",
            " [142.83978]]\n",
            "6700 0.5900866 [[150.63945]\n",
            " [185.2471 ]\n",
            " [180.54922]\n",
            " [195.7391 ]\n",
            " [142.81766]]\n",
            "6800 0.569852 [[150.66309]\n",
            " [185.23083]\n",
            " [180.55644]\n",
            " [195.74458]\n",
            " [142.7961 ]]\n",
            "6900 0.5506632 [[150.68613]\n",
            " [185.21503]\n",
            " [180.56346]\n",
            " [195.74995]\n",
            " [142.77512]]\n",
            "7000 0.53249943 [[150.70853]\n",
            " [185.19963]\n",
            " [180.57028]\n",
            " [195.75516]\n",
            " [142.7547 ]]\n",
            "7100 0.5152899 [[150.73033]\n",
            " [185.18468]\n",
            " [180.57692]\n",
            " [195.76022]\n",
            " [142.73482]]\n",
            "7200 0.49899167 [[150.75156]\n",
            " [185.1701 ]\n",
            " [180.5834 ]\n",
            " [195.7652 ]\n",
            " [142.71548]]\n",
            "7300 0.48355174 [[150.77219]\n",
            " [185.15591]\n",
            " [180.58968]\n",
            " [195.77   ]\n",
            " [142.69664]]\n",
            "7400 0.46891984 [[150.7923 ]\n",
            " [185.14214]\n",
            " [180.59581]\n",
            " [195.77472]\n",
            " [142.67831]]\n",
            "7500 0.45505434 [[150.81186]\n",
            " [185.12868]\n",
            " [180.60176]\n",
            " [195.7793 ]\n",
            " [142.66046]]\n",
            "7600 0.44192386 [[150.83089]\n",
            " [185.11562]\n",
            " [180.60754]\n",
            " [195.78375]\n",
            " [142.64308]]\n",
            "7700 0.42949548 [[150.8494 ]\n",
            " [185.1029 ]\n",
            " [180.61319]\n",
            " [195.7881 ]\n",
            " [142.62617]]\n",
            "7800 0.41771317 [[150.86742]\n",
            " [185.09053]\n",
            " [180.61867]\n",
            " [195.79233]\n",
            " [142.60971]]\n",
            "7900 0.4065481 [[150.88495]\n",
            " [185.07848]\n",
            " [180.624  ]\n",
            " [195.79646]\n",
            " [142.59367]]\n",
            "8000 0.395979 [[150.90201]\n",
            " [185.06674]\n",
            " [180.62918]\n",
            " [195.80048]\n",
            " [142.57808]]\n",
            "8100 0.38596356 [[150.91861]\n",
            " [185.05533]\n",
            " [180.63423]\n",
            " [195.80438]\n",
            " [142.56288]]\n",
            "8200 0.37647963 [[150.93477]\n",
            " [185.04425]\n",
            " [180.63916]\n",
            " [195.80824]\n",
            " [142.54811]]\n",
            "8300 0.3674813 [[150.9505 ]\n",
            " [185.03343]\n",
            " [180.64392]\n",
            " [195.81195]\n",
            " [142.5337 ]]\n",
            "8400 0.35897344 [[150.96579]\n",
            " [185.0229 ]\n",
            " [180.64857]\n",
            " [195.81557]\n",
            " [142.5197 ]]\n",
            "8500 0.35090294 [[150.98068]\n",
            " [185.0127 ]\n",
            " [180.65309]\n",
            " [195.8191 ]\n",
            " [142.50606]]\n",
            "8600 0.34326363 [[150.99518]\n",
            " [185.00275]\n",
            " [180.65752]\n",
            " [195.82257]\n",
            " [142.49278]]\n",
            "8700 0.33602318 [[151.00926]\n",
            " [184.99304]\n",
            " [180.66179]\n",
            " [195.82591]\n",
            " [142.47983]]\n",
            "8800 0.32916605 [[151.02296]\n",
            " [184.9836 ]\n",
            " [180.66594]\n",
            " [195.8292 ]\n",
            " [142.46724]]\n",
            "8900 0.32266745 [[151.03633]\n",
            " [184.97444]\n",
            " [180.67001]\n",
            " [195.83238]\n",
            " [142.45499]]\n",
            "9000 0.31651145 [[151.04932]\n",
            " [184.9655 ]\n",
            " [180.67395]\n",
            " [195.8355 ]\n",
            " [142.44305]]\n",
            "9100 0.31068033 [[151.06195]\n",
            " [184.95682]\n",
            " [180.67778]\n",
            " [195.83853]\n",
            " [142.43144]]\n",
            "9200 0.30516174 [[151.07423]\n",
            " [184.94835]\n",
            " [180.68152]\n",
            " [195.84148]\n",
            " [142.42012]]\n",
            "9300 0.2999244 [[151.0862 ]\n",
            " [184.94011]\n",
            " [180.68513]\n",
            " [195.84436]\n",
            " [142.4091 ]]\n",
            "9400 0.2949658 [[151.09785]\n",
            " [184.93211]\n",
            " [180.68867]\n",
            " [195.84718]\n",
            " [142.3984 ]]\n",
            "9500 0.29027116 [[151.10916]\n",
            " [184.92432]\n",
            " [180.6921 ]\n",
            " [195.84993]\n",
            " [142.38797]]\n",
            "9600 0.2858161 [[151.1202 ]\n",
            " [184.91673]\n",
            " [180.69543]\n",
            " [195.85258]\n",
            " [142.37779]]\n",
            "9700 0.28160444 [[151.1309 ]\n",
            " [184.90935]\n",
            " [180.69868]\n",
            " [195.85521]\n",
            " [142.3679 ]]\n",
            "9800 0.2776068 [[151.14136]\n",
            " [184.90219]\n",
            " [180.70186]\n",
            " [195.85776]\n",
            " [142.35828]]\n",
            "9900 0.27381718 [[151.15152]\n",
            " [184.89522]\n",
            " [180.70493]\n",
            " [195.86024]\n",
            " [142.3489 ]]\n",
            "10000 0.27023822 [[151.16138]\n",
            " [184.88841]\n",
            " [180.70792]\n",
            " [195.86266]\n",
            " [142.33977]]\n",
            "10100 0.26684773 [[151.17099]\n",
            " [184.88179]\n",
            " [180.71085]\n",
            " [195.86502]\n",
            " [142.33089]]\n",
            "10200 0.26361737 [[151.18036]\n",
            " [184.87535]\n",
            " [180.71365]\n",
            " [195.8673 ]\n",
            " [142.32222]]\n",
            "10300 0.26056352 [[151.18947]\n",
            " [184.8691 ]\n",
            " [180.7164 ]\n",
            " [195.86957]\n",
            " [142.31381]]\n",
            "10400 0.25767824 [[151.1983 ]\n",
            " [184.86299]\n",
            " [180.71907]\n",
            " [195.87172]\n",
            " [142.30559]]\n",
            "10500 0.25494233 [[151.20692]\n",
            " [184.85709]\n",
            " [180.7217 ]\n",
            " [195.87389]\n",
            " [142.2976 ]]\n",
            "10600 0.25234982 [[151.2153 ]\n",
            " [184.8513 ]\n",
            " [180.72423]\n",
            " [195.87598]\n",
            " [142.28983]]\n",
            "10700 0.24988894 [[151.22346]\n",
            " [184.84569]\n",
            " [180.72668]\n",
            " [195.87799]\n",
            " [142.28226]]\n",
            "10800 0.2475626 [[151.23138]\n",
            " [184.84023]\n",
            " [180.72908]\n",
            " [195.87999]\n",
            " [142.27487]]\n",
            "10900 0.2453573 [[151.2391 ]\n",
            " [184.8349 ]\n",
            " [180.73141]\n",
            " [195.88191]\n",
            " [142.26768]]\n",
            "11000 0.24326476 [[151.24664]\n",
            " [184.82974]\n",
            " [180.7337 ]\n",
            " [195.88382]\n",
            " [142.26071]]\n",
            "11100 0.24128218 [[151.25394]\n",
            " [184.82469]\n",
            " [180.73589]\n",
            " [195.88564]\n",
            " [142.25389]]\n",
            "11200 0.2394073 [[151.26103]\n",
            " [184.8198 ]\n",
            " [180.73802]\n",
            " [195.88744]\n",
            " [142.24727]]\n",
            "11300 0.2376261 [[151.26796]\n",
            " [184.81502]\n",
            " [180.74011]\n",
            " [195.88919]\n",
            " [142.2408 ]]\n",
            "11400 0.235937 [[151.2747 ]\n",
            " [184.8104 ]\n",
            " [180.74214]\n",
            " [195.8909 ]\n",
            " [142.23453]]\n",
            "11500 0.23434813 [[151.28123]\n",
            " [184.8059 ]\n",
            " [180.74413]\n",
            " [195.89258]\n",
            " [142.22842]]\n",
            "11600 0.23283322 [[151.2876 ]\n",
            " [184.80148]\n",
            " [180.74603]\n",
            " [195.8942 ]\n",
            " [142.22244]]\n",
            "11700 0.23139842 [[151.29381]\n",
            " [184.79721]\n",
            " [180.74791]\n",
            " [195.8958 ]\n",
            " [142.21664]]\n",
            "11800 0.23003832 [[151.29982]\n",
            " [184.79308]\n",
            " [180.74971]\n",
            " [195.89735]\n",
            " [142.211  ]]\n",
            "11900 0.22874162 [[151.3057 ]\n",
            " [184.789  ]\n",
            " [180.75145]\n",
            " [195.89885]\n",
            " [142.20547]]\n",
            "12000 0.22751871 [[151.31142]\n",
            " [184.78508]\n",
            " [180.75317]\n",
            " [195.90034]\n",
            " [142.20013]]\n",
            "12100 0.22636576 [[151.31694]\n",
            " [184.78125]\n",
            " [180.75484]\n",
            " [195.90178]\n",
            " [142.19489]]\n",
            "12200 0.22527263 [[151.32234]\n",
            " [184.77753]\n",
            " [180.75647]\n",
            " [195.9032 ]\n",
            " [142.18982]]\n",
            "12300 0.2242246 [[151.3276 ]\n",
            " [184.7739 ]\n",
            " [180.75803]\n",
            " [195.90459]\n",
            " [142.18488]]\n",
            "12400 0.2232439 [[151.33272]\n",
            " [184.77039]\n",
            " [180.75958]\n",
            " [195.90594]\n",
            " [142.18005]]\n",
            "12500 0.22231188 [[151.33768]\n",
            " [184.76694]\n",
            " [180.76106]\n",
            " [195.90724]\n",
            " [142.17535]]\n",
            "12600 0.22142044 [[151.34253]\n",
            " [184.76361]\n",
            " [180.76251]\n",
            " [195.90854]\n",
            " [142.17078]]\n",
            "12700 0.22059067 [[151.34721]\n",
            " [184.76036]\n",
            " [180.76393]\n",
            " [195.90979]\n",
            " [142.16634]]\n",
            "12800 0.21978366 [[151.35179]\n",
            " [184.75722]\n",
            " [180.76527]\n",
            " [195.91103]\n",
            " [142.16199]]\n",
            "12900 0.21902928 [[151.35623]\n",
            " [184.75412]\n",
            " [180.76659]\n",
            " [195.9122 ]\n",
            " [142.15776]]\n",
            "13000 0.2183182 [[151.36057]\n",
            " [184.75113]\n",
            " [180.7679 ]\n",
            " [195.91338]\n",
            " [142.15364]]\n",
            "13100 0.21763793 [[151.36479]\n",
            " [184.74823]\n",
            " [180.76917]\n",
            " [195.91454]\n",
            " [142.14964]]\n",
            "13200 0.21699066 [[151.36888]\n",
            " [184.7454 ]\n",
            " [180.77037]\n",
            " [195.91565]\n",
            " [142.14574]]\n",
            "13300 0.2163867 [[151.37286]\n",
            " [184.74265]\n",
            " [180.77158]\n",
            " [195.91675]\n",
            " [142.14192]]\n",
            "13400 0.21580935 [[151.37674]\n",
            " [184.73997]\n",
            " [180.77274]\n",
            " [195.9178 ]\n",
            " [142.13821]]\n",
            "13500 0.21525967 [[151.3805 ]\n",
            " [184.73735]\n",
            " [180.77383]\n",
            " [195.91885]\n",
            " [142.13461]]\n",
            "13600 0.21473268 [[151.38417]\n",
            " [184.73483]\n",
            " [180.77492]\n",
            " [195.91986]\n",
            " [142.13109]]\n",
            "13700 0.21424404 [[151.38774]\n",
            " [184.73238]\n",
            " [180.776  ]\n",
            " [195.92088]\n",
            " [142.12767]]\n",
            "13800 0.21378167 [[151.39119]\n",
            " [184.72995]\n",
            " [180.77702]\n",
            " [195.92184]\n",
            " [142.12433]]\n",
            "13900 0.21333341 [[151.39456]\n",
            " [184.72763]\n",
            " [180.77802]\n",
            " [195.9228 ]\n",
            " [142.12108]]\n",
            "14000 0.21291542 [[151.39786]\n",
            " [184.72536]\n",
            " [180.779  ]\n",
            " [195.92374]\n",
            " [142.11792]]\n",
            "14100 0.21251532 [[151.40103]\n",
            " [184.72314]\n",
            " [180.77994]\n",
            " [195.92465]\n",
            " [142.11482]]\n",
            "14200 0.21213536 [[151.40414]\n",
            " [184.72098]\n",
            " [180.78085]\n",
            " [195.92554]\n",
            " [142.11182]]\n",
            "14300 0.2117747 [[151.40715]\n",
            " [184.7189 ]\n",
            " [180.78174]\n",
            " [195.9264 ]\n",
            " [142.10889]]\n",
            "14400 0.2114356 [[151.41008]\n",
            " [184.71687]\n",
            " [180.78261]\n",
            " [195.92725]\n",
            " [142.10603]]\n",
            "14500 0.21111071 [[151.41295]\n",
            " [184.7149 ]\n",
            " [180.78346]\n",
            " [195.9281 ]\n",
            " [142.10326]]\n",
            "14600 0.21079996 [[151.41571]\n",
            " [184.71297]\n",
            " [180.78426]\n",
            " [195.92892]\n",
            " [142.10054]]\n",
            "14700 0.21051177 [[151.4184 ]\n",
            " [184.71109]\n",
            " [180.78505]\n",
            " [195.92972]\n",
            " [142.0979 ]]\n",
            "14800 0.21023495 [[151.42102]\n",
            " [184.70929]\n",
            " [180.78583]\n",
            " [195.9305 ]\n",
            " [142.09534]]\n",
            "14900 0.2099679 [[151.42358]\n",
            " [184.70752]\n",
            " [180.78658]\n",
            " [195.93127]\n",
            " [142.09282]]\n",
            "15000 0.20972785 [[151.42604]\n",
            " [184.70578]\n",
            " [180.78731]\n",
            " [195.932  ]\n",
            " [142.09038]]\n",
            "15100 0.20948878 [[151.42845]\n",
            " [184.70412]\n",
            " [180.78801]\n",
            " [195.93274]\n",
            " [142.08801]]\n",
            "15200 0.2092628 [[151.4308 ]\n",
            " [184.70248]\n",
            " [180.7887 ]\n",
            " [195.93347]\n",
            " [142.0857 ]]\n",
            "15300 0.2090437 [[151.43309]\n",
            " [184.7009 ]\n",
            " [180.78935]\n",
            " [195.93417]\n",
            " [142.08342]]\n",
            "15400 0.20884605 [[151.4353 ]\n",
            " [184.69934]\n",
            " [180.79001]\n",
            " [195.93486]\n",
            " [142.0812 ]]\n",
            "15500 0.20865235 [[151.43747]\n",
            " [184.69786]\n",
            " [180.79065]\n",
            " [195.93555]\n",
            " [142.07907]]\n",
            "15600 0.20847213 [[151.43954]\n",
            " [184.69638]\n",
            " [180.79124]\n",
            " [195.93619]\n",
            " [142.07698]]\n",
            "15700 0.20829478 [[151.4416 ]\n",
            " [184.69499]\n",
            " [180.79185]\n",
            " [195.93686]\n",
            " [142.07495]]\n",
            "15800 0.20812976 [[151.44357]\n",
            " [184.6936 ]\n",
            " [180.79242]\n",
            " [195.9375 ]\n",
            " [142.07297]]\n",
            "15900 0.20796926 [[151.44548]\n",
            " [184.69226]\n",
            " [180.79295]\n",
            " [195.93811]\n",
            " [142.07103]]\n",
            "16000 0.20781946 [[151.44736]\n",
            " [184.69096]\n",
            " [180.7935 ]\n",
            " [195.93872]\n",
            " [142.06912]]\n",
            "16100 0.2076747 [[151.44919]\n",
            " [184.68971]\n",
            " [180.79404]\n",
            " [195.93935]\n",
            " [142.06729]]\n",
            "16200 0.20754433 [[151.45096]\n",
            " [184.68846]\n",
            " [180.79456]\n",
            " [195.93991]\n",
            " [142.06549]]\n",
            "16300 0.20741753 [[151.45265]\n",
            " [184.68727]\n",
            " [180.79504]\n",
            " [195.94049]\n",
            " [142.06374]]\n",
            "16400 0.20729685 [[151.45433]\n",
            " [184.6861 ]\n",
            " [180.79553]\n",
            " [195.94106]\n",
            " [142.06203]]\n",
            "16500 0.2071769 [[151.45595]\n",
            " [184.68498]\n",
            " [180.79599]\n",
            " [195.9416 ]\n",
            " [142.06038]]\n",
            "16600 0.20707044 [[151.45752]\n",
            " [184.68387]\n",
            " [180.79645]\n",
            " [195.94214]\n",
            " [142.05875]]\n",
            "16700 0.20696704 [[151.45905]\n",
            " [184.6828 ]\n",
            " [180.79689]\n",
            " [195.94269]\n",
            " [142.05717]]\n",
            "16800 0.20686576 [[151.46053]\n",
            " [184.68175]\n",
            " [180.7973 ]\n",
            " [195.94319]\n",
            " [142.05562]]\n",
            "16900 0.20676467 [[151.462  ]\n",
            " [184.68076]\n",
            " [180.79773]\n",
            " [195.94373]\n",
            " [142.05412]]\n",
            "17000 0.20667231 [[151.4634 ]\n",
            " [184.67976]\n",
            " [180.79811]\n",
            " [195.94423]\n",
            " [142.05266]]\n",
            "17100 0.20659053 [[151.46475]\n",
            " [184.6788 ]\n",
            " [180.79851]\n",
            " [195.94472]\n",
            " [142.05122]]\n",
            "17200 0.20649938 [[151.4661 ]\n",
            " [184.67789]\n",
            " [180.79887]\n",
            " [195.9452 ]\n",
            " [142.04984]]\n",
            "17300 0.2064245 [[151.46736]\n",
            " [184.67696]\n",
            " [180.79922]\n",
            " [195.94566]\n",
            " [142.04846]]\n",
            "17400 0.2063447 [[151.46863]\n",
            " [184.67612]\n",
            " [180.79959]\n",
            " [195.94617]\n",
            " [142.04715]]\n",
            "17500 0.20628223 [[151.46983]\n",
            " [184.67523]\n",
            " [180.79994]\n",
            " [195.94661]\n",
            " [142.04585]]\n",
            "17600 0.20621248 [[151.47101]\n",
            " [184.6744 ]\n",
            " [180.80026]\n",
            " [195.94705]\n",
            " [142.04457]]\n",
            "17700 0.20614395 [[151.47218]\n",
            " [184.6736 ]\n",
            " [180.8006 ]\n",
            " [195.94751]\n",
            " [142.04335]]\n",
            "17800 0.20608275 [[151.47328]\n",
            " [184.67282]\n",
            " [180.8009 ]\n",
            " [195.94792]\n",
            " [142.04216]]\n",
            "17900 0.20602329 [[151.47437]\n",
            " [184.67203]\n",
            " [180.8012 ]\n",
            " [195.94833]\n",
            " [142.04097]]\n",
            "18000 0.2059656 [[151.47543]\n",
            " [184.6713 ]\n",
            " [180.8015 ]\n",
            " [195.94878]\n",
            " [142.03984]]\n",
            "18100 0.20591104 [[151.47644]\n",
            " [184.67058]\n",
            " [180.80177]\n",
            " [195.94919]\n",
            " [142.03873]]\n",
            "18200 0.20585735 [[151.47742]\n",
            " [184.66988]\n",
            " [180.80203]\n",
            " [195.9496 ]\n",
            " [142.03763]]\n",
            "18300 0.20580044 [[151.47841]\n",
            " [184.66919]\n",
            " [180.80229]\n",
            " [195.95   ]\n",
            " [142.03656]]\n",
            "18400 0.20576489 [[151.47932]\n",
            " [184.66855]\n",
            " [180.80258]\n",
            " [195.9504 ]\n",
            " [142.03554]]\n",
            "18500 0.2057126 [[151.48022]\n",
            " [184.66792]\n",
            " [180.80281]\n",
            " [195.95078]\n",
            " [142.03455]]\n",
            "18600 0.2056733 [[151.4811 ]\n",
            " [184.66725]\n",
            " [180.80304]\n",
            " [195.95116]\n",
            " [142.03352]]\n",
            "18700 0.20563023 [[151.48198]\n",
            " [184.66667]\n",
            " [180.8033 ]\n",
            " [195.95154]\n",
            " [142.03256]]\n",
            "18800 0.20558664 [[151.4828 ]\n",
            " [184.66609]\n",
            " [180.80351]\n",
            " [195.9519 ]\n",
            " [142.03163]]\n",
            "18900 0.20554972 [[151.48361]\n",
            " [184.6655 ]\n",
            " [180.80373]\n",
            " [195.95226]\n",
            " [142.03073]]\n",
            "19000 0.20551534 [[151.4844 ]\n",
            " [184.66495]\n",
            " [180.80396]\n",
            " [195.9526 ]\n",
            " [142.02982]]\n",
            "19100 0.20548484 [[151.48514]\n",
            " [184.6644 ]\n",
            " [180.80415]\n",
            " [195.95296]\n",
            " [142.02893]]\n",
            "19200 0.20545109 [[151.48589]\n",
            " [184.6639 ]\n",
            " [180.80437]\n",
            " [195.95332]\n",
            " [142.0281 ]]\n",
            "19300 0.20541361 [[151.48657]\n",
            " [184.66338]\n",
            " [180.80452]\n",
            " [195.95364]\n",
            " [142.02727]]\n",
            "19400 0.20537925 [[151.48727]\n",
            " [184.66289]\n",
            " [180.8047 ]\n",
            " [195.95398]\n",
            " [142.02644]]\n",
            "19500 0.20535083 [[151.48796]\n",
            " [184.66238]\n",
            " [180.80489]\n",
            " [195.95432]\n",
            " [142.02563]]\n",
            "19600 0.20532325 [[151.48862]\n",
            " [184.66193]\n",
            " [180.80507]\n",
            " [195.95467]\n",
            " [142.02487]]\n",
            "19700 0.20529334 [[151.48926]\n",
            " [184.66148]\n",
            " [180.80524]\n",
            " [195.95497]\n",
            " [142.02412]]\n",
            "19800 0.20526333 [[151.48987]\n",
            " [184.66106]\n",
            " [180.80539]\n",
            " [195.95528]\n",
            " [142.02339]]\n",
            "19900 0.20524092 [[151.49046]\n",
            " [184.6606 ]\n",
            " [180.80554]\n",
            " [195.95558]\n",
            " [142.02264]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. 대용량 데이터 전처리 방법에 대해\n",
        "- Python에서 제공하는 DASK\n",
        "- Tensorflow에서 제공하는 Queue Runners(1.xx 버젼에서만)"
      ],
      "metadata": {
        "id": "A2aJlNCKBSYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DASK\n",
        "\n",
        "- 가상의 데이터프레임\n",
        "- 병렬처리용 작업 스케줄러"
      ],
      "metadata": {
        "id": "Bx9YxCxjBob1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"sample1.csv\"\n",
        "c1, c2, c3\n",
        "1, 1.11, one\n",
        "2, 2.11, two\n",
        "3, 3.11, three\n",
        "4, 4.11, four\n",
        "5, 5.11, five\n",
        "6, 6.11, six\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPktcSdRBREc",
        "outputId": "63e64fd4-ee3f-4e18-9a7a-23fd32cf07fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing \"sample1.csv\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'c1':[1,2,3,4,5,6], 'c2':[1.11,2.11,3.11,4.11,5.11,6.11], 'c3':['one', 'two', 'three', 'four', 'five', 'six']}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "hT-WTZDMCtGV",
        "outputId": "b0ebf27f-0638-4f58-b9a0-33626f7027db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c1</th>\n",
              "      <th>c2</th>\n",
              "      <th>c3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.11</td>\n",
              "      <td>one</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2.11</td>\n",
              "      <td>two</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3.11</td>\n",
              "      <td>three</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.11</td>\n",
              "      <td>four</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.11</td>\n",
              "      <td>five</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>6.11</td>\n",
              "      <td>six</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   c1    c2     c3\n",
              "0   1  1.11    one\n",
              "1   2  2.11    two\n",
              "2   3  3.11  three\n",
              "3   4  4.11   four\n",
              "4   5  5.11   five\n",
              "5   6  6.11    six"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install 'fsspec>=0.3.3'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOunoHleEx18",
        "outputId": "5894f824-d5be-4ffc-8f63-e7c6509d243a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec>=0.3.3\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 40 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 102 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 112 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 122 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 132 kB 4.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "Successfully installed fsspec-2021.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd"
      ],
      "metadata": {
        "id": "weMhGkBZE_xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"sample1.csv\"\n",
        "c1, c2, c3\n",
        "1, 1.11, one\n",
        "2, 2.11, two\n",
        "3, 3.11, three\n",
        "4, 4.11, four\n",
        "5, 5.11, five\n",
        "6, 6.11, six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20Ewt4mYFiG-",
        "outputId": "94e26047-8796-4635-f78c-8e884c12b09e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting \"sample1.csv\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df2 = dd.read_csv(\"sample1.csv\")\n",
        "\n",
        "# df2[' c2'].mean().compute()"
      ],
      "metadata": {
        "id": "lKv2V3XhFwpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# myfile = files.upload()"
      ],
      "metadata": {
        "id": "9xkkSOktGwFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Queue Runners\n",
        "- Runner란 쓰레드를 의미.\n",
        "- 여러 개를 동시에 처리하게끔 만든단 의미."
      ],
      "metadata": {
        "id": "D9Y9V-hfmWGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1)enqueue_many와 dequeue"
      ],
      "metadata": {
        "id": "_VXkUPwXmgAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = tf.FIFOQueue(20, 'float')\n",
        "\n",
        "# enc_ops = q.enqueue_many(([1.0, 2.0, 3.0, 4.0],))\n",
        "# qr = tf.train.QueueRunner(q, [enc_ops]*3) #동작 설정 명령어. 앞에는 공간 만들기과 설정만 해둔 것\n",
        "\n",
        "enc_ops1 = q.enqueue_many(([1.0, 2.0, 3.0],))\n",
        "enc_ops2 = q.enqueue_many(([4.0, 5.0, 6.0],))\n",
        "enc_ops3 = q.enqueue_many(([7.0, 8.0, 9.0],))\n",
        "qr = tf.train.QueueRunner(q, [enc_ops1, enc_ops2, enc_ops3]) #동작 설정 명령어. 앞에는 공간 만들기과 설정만 해둔 것\n",
        "\n",
        "sess = tf.Session()\n",
        "coordi = tf.train.Coordinator() #쓰레드, 병렬처리를 직접 지정하는건 전문가도 어려워하는 영역. tf에선 이를 대신해주는 중간관리자 함수가 있다.\n",
        "threads = qr.create_threads(sess, coord=coordi, start=True)\n",
        "\n",
        "for step in range(20):\n",
        "  print(sess.run(q.dequeue()))\n",
        "\n",
        "coordi.request_stop()\n",
        "coordi.join(threads)\n",
        "sess.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kraPUWDGJ_PK",
        "outputId": "66071a0c-4366-4e42-d017-7c2a3ae4a537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "2.0\n",
            "3.0\n",
            "4.0\n",
            "5.0\n",
            "6.0\n",
            "4.0\n",
            "5.0\n",
            "6.0\n",
            "7.0\n",
            "8.0\n",
            "9.0\n",
            "1.0\n",
            "2.0\n",
            "3.0\n",
            "4.0\n",
            "5.0\n",
            "6.0\n",
            "4.0\n",
            "5.0\n",
            "6.0\n",
            "4.0\n",
            "5.0\n",
            "6.0\n",
            "7.0\n",
            "8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) decoding"
      ],
      "metadata": {
        "id": "cPjX3RAgtmAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.string_input_producer([\"data//sample1.csv\", \"data/sample2.csv\"], shuffle=False, name=\"filename_queue\")\n",
        "reader = tf.TextLineReader() #Reader 세팅\n",
        "key, value = reader.read(filename_queue)\n",
        "\n",
        "a1, a2, a3 = tf.decode_csv(value, record_defaults==[[0], [0,0], [\"null\"]], field_delim=\",\")"
      ],
      "metadata": {
        "id": "nXMTis52rWlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "coordi = tf.train.Coordinator()\n",
        "threads = tf.train.start_queue_runners(sess=sess, coord=coordi)\n",
        "\n",
        "for i in range(30):\n",
        "  print(sess.run([a1, a2, a3]))\n",
        "\n",
        "coordi.request_stop()\n",
        "coordi.join(threads)\n",
        "sess.close()"
      ],
      "metadata": {
        "id": "A9lUZbsS0HwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) lab 슬라이드 내용"
      ],
      "metadata": {
        "id": "1oPPHDMY3uS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 1. Data\n",
        "\n",
        "filename_queue = tf.train.string_input_producer(\n",
        "   ['data-01-test-score.csv', 'data-02-test-score.csv', ... ], \n",
        "   shuffle=False, name='filename_queue')\n",
        "\n",
        "#### 2. Queue 방식 셋업 -> read 레이어까지\n",
        "reader = tf.TextLineReader()\n",
        "key, value = reader.read(filename_queue)\n",
        "\n",
        "\n",
        "#### 3. \n",
        "record_defaults = [[0.], [0.], [0.], [0.]]\n",
        "xy = tf.decode_csv(value, record_defaults=record_defaults)"
      ],
      "metadata": {
        "id": "hjIIxtHo3wWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch\n",
        "\n",
        "- 데이터 한뭉텅이를 한번에 돌리는 것 vs 데이터를 등분해서 섞어서 돌리는 것\n",
        "- 후자가 더 효율성이 좋다. 거기다 데이터를 균등하게 섞어서 훈련하므로 성능도 더 우수\n",
        "- Batch는 데이터 등분 후 섞어 돌리는 것을 의미\n",
        "- Batch엔 3가지 종류가 있다.\n",
        "  + 1. Full Batch or Batch : 데이터를 나누지 않고. 지금까지 우리가 한 것\n",
        "    * 샘플 전체를 계산하므로 시간이 많이 걸린다.\n",
        "    * 데이터가 많아 메모리에 데이터를 로드하지 못할 경우 사용 불가\n",
        "    * 오차율이 적다\n",
        "  + 2. Stochastic Gradient Descent : 한개의 데이터\n",
        "    * 데이터를 한개씩 추출해서 처리해보고 이를 모든 데이터에 반복\n",
        "    * 오차율이 클 수 있다.\n",
        "    * 하나씩 처리하다보니 속도가 매우 느리다.\n",
        "    * 하드웨어 성능을 제대로 발휘할 수 없는 구조\n",
        "  + 3. mini-batch :: full과 stoch를 적절히 섞은 방식\n",
        "    * 딥러닝에선 이 방식으로 데이터 처리함\n",
        "    * 전체 훈련 데이터를 배치 사이즈로 등분하여 각 배치셋을 순차적으로 순행\n",
        "    * Full Batch보다 빠르고 SGD보다 오차율 낮다.\n",
        "    * ML에서 cross_validation()와 비슷한 방식이라 이해하면 된다.\n",
        "  + 4. Epoch( = Iteration ) :: 배치의 수행횟수"
      ],
      "metadata": {
        "id": "_axKGelx5ocG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename_queue = tf.train.string_input_producer(\n",
        "   ['data-01-test-score.csv', 'data-02-test-score.csv', ... ], \n",
        "   shuffle=False, name='filename_queue')\n",
        "\n",
        "reader = tf.TextLineReader()\n",
        "key, value = reader.read(filename_queue)\n",
        "\n",
        " \n",
        "record_defaults = [[0.], [0.], [0.], [0.]]\n",
        "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
        "xy"
      ],
      "metadata": {
        "id": "X4nX46eS7gVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_batch, train_y_batch = tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)"
      ],
      "metadata": {
        "id": "qT7Gs65EtgRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([3,1], name='weight'))\n",
        "b = tf.Variable(tf.random_normal([1], name='bias'))\n",
        "\n",
        "# Hypothesis\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "# Simplified cost/loss function\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "# Minimize\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
        "train = optimizer.minimize(cost)\n"
      ],
      "metadata": {
        "id": "aouQJ_K8vYdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the graph in a session.\n",
        "sess = tf.Session()\n",
        "# Initializes global variables in the graph.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Start populating the filename queue.\n",
        "coord = tf.train.Coordinator()\n",
        "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "\n",
        "for step in range(2001):\n",
        "   x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
        "   cost_val, hy_val, _ = sess.run(\n",
        "       [cost, hypothesis, train], \n",
        "       feed_dict={X: x_batch, Y: y_batch})\n",
        "   if step % 10 == 0:\n",
        "       print(step, \"Cost: \", cost_val, \n",
        "                   \"\\nPrediction:\\n\", hy_val)\n",
        "\n",
        "coord.request_stop()\n",
        "coord.join(threads)"
      ],
      "metadata": {
        "id": "OB64M3ZzxLJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 로지스틱 회귀분석\n",
        "\n",
        "- 1. 분류 알고리즘\n",
        "- 2. 딥러닝에서 가장 기본이 되는 활성화 함수"
      ],
      "metadata": {
        "id": "N4W0A-pGzfiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. 2진 분류\n",
        "- Yes or No\n",
        "- Sigmoid"
      ],
      "metadata": {
        "id": "kdCoAc2FCzay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
        "y_data = [[0], [0] ,[0], [1], [1], [1]]"
      ],
      "metadata": {
        "id": "shs1WQ-gzraA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random.normal([2,1]))\n",
        "b = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "# 가설\n",
        "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "# 비용(잔차제곱합)\n",
        "cost = -tf.reduce_mean(y * tf.math.log(hypot) + (1 - y) * tf.math.log(1 - hypot))\n",
        "\n",
        "# 훈련방법(그라디언트디센트 방식 모델링)\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
      ],
      "metadata": {
        "id": "HjIPsaqR5QQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(15000):\n",
        "  _, h = sess.run([train, hypot], feed_dict = {X : X_data, y : y_data})\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    print(step, h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1v3_8m28ODa",
        "outputId": "fef9857a-5892-40b6-be1d-38a33568e1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [[0.20126384]\n",
            " [0.07761496]\n",
            " [0.36424482]\n",
            " [0.06545009]\n",
            " [0.06005477]\n",
            " [0.13736494]]\n",
            "500 [[0.31436762]\n",
            " [0.27344772]\n",
            " [0.8008981 ]\n",
            " [0.5839434 ]\n",
            " [0.73048294]\n",
            " [0.92488706]]\n",
            "1000 [[0.24670269]\n",
            " [0.23590282]\n",
            " [0.7820381 ]\n",
            " [0.59418625]\n",
            " [0.7612574 ]\n",
            " [0.9413185 ]]\n",
            "1500 [[0.20668615]\n",
            " [0.2210559 ]\n",
            " [0.7524033 ]\n",
            " [0.60709834]\n",
            " [0.782868  ]\n",
            " [0.9474307 ]]\n",
            "2000 [[0.1786963 ]\n",
            " [0.21457744]\n",
            " [0.7177814 ]\n",
            " [0.6208938 ]\n",
            " [0.80039924]\n",
            " [0.95035946]]\n",
            "2500 [[0.15692446]\n",
            " [0.21128565]\n",
            " [0.6812856 ]\n",
            " [0.63479847]\n",
            " [0.8157605 ]\n",
            " [0.95229495]]\n",
            "3000 [[0.13892727]\n",
            " [0.2090318 ]\n",
            " [0.6448132 ]\n",
            " [0.6483734 ]\n",
            " [0.82966167]\n",
            " [0.9540185 ]]\n",
            "3500 [[0.12357354]\n",
            " [0.20690696]\n",
            " [0.6095113 ]\n",
            " [0.6613707 ]\n",
            " [0.84236723]\n",
            " [0.9557941 ]]\n",
            "4000 [[0.11027618]\n",
            " [0.20456064]\n",
            " [0.57602566]\n",
            " [0.6736705 ]\n",
            " [0.8539925 ]\n",
            " [0.9576792 ]]\n",
            "4500 [[0.09868058]\n",
            " [0.2018948 ]\n",
            " [0.5446676 ]\n",
            " [0.6852284 ]\n",
            " [0.8646083 ]\n",
            " [0.95965177]]\n",
            "5000 [[0.08853716]\n",
            " [0.19892369]\n",
            " [0.51553935]\n",
            " [0.69605136]\n",
            " [0.87428236]\n",
            " [0.9616675 ]]\n",
            "5500 [[0.07964806]\n",
            " [0.19570148]\n",
            " [0.48861098]\n",
            " [0.70616734]\n",
            " [0.88308287]\n",
            " [0.96368086]]\n",
            "6000 [[0.07184731]\n",
            " [0.1922945 ]\n",
            " [0.463784  ]\n",
            " [0.71562093]\n",
            " [0.8910827 ]\n",
            " [0.96565586]]\n",
            "6500 [[0.06499102]\n",
            " [0.18876454]\n",
            " [0.44092125]\n",
            " [0.7244607 ]\n",
            " [0.8983546 ]\n",
            " [0.96756613]]\n",
            "7000 [[0.05895397]\n",
            " [0.18516536]\n",
            " [0.41987234]\n",
            " [0.7327373 ]\n",
            " [0.9049693 ]\n",
            " [0.9693945 ]]\n",
            "7500 [[0.05362713]\n",
            " [0.18153968]\n",
            " [0.40048185]\n",
            " [0.74049675]\n",
            " [0.91099167]\n",
            " [0.97113055]]\n",
            "8000 [[0.04891608]\n",
            " [0.17792174]\n",
            " [0.3826018 ]\n",
            " [0.7477841 ]\n",
            " [0.9164827 ]\n",
            " [0.97276944]]\n",
            "8500 [[0.04473928]\n",
            " [0.17433785]\n",
            " [0.3660925 ]\n",
            " [0.7546401 ]\n",
            " [0.92149734]\n",
            " [0.97431004]]\n",
            "9000 [[0.0410264 ]\n",
            " [0.17080733]\n",
            " [0.35082474]\n",
            " [0.76110125]\n",
            " [0.9260848 ]\n",
            " [0.975754  ]]\n",
            "9500 [[0.03771711]\n",
            " [0.16734481]\n",
            " [0.33668146]\n",
            " [0.7672013 ]\n",
            " [0.93028945]\n",
            " [0.9771045 ]]\n",
            "10000 [[0.03475955]\n",
            " [0.16396016]\n",
            " [0.32355604]\n",
            " [0.7729699 ]\n",
            " [0.9341503 ]\n",
            " [0.97836566]]\n",
            "10500 [[0.03210918]\n",
            " [0.16066052]\n",
            " [0.31135395]\n",
            " [0.7784348 ]\n",
            " [0.93770266]\n",
            " [0.97954273]]\n",
            "11000 [[0.02972775]\n",
            " [0.1574501 ]\n",
            " [0.29998884]\n",
            " [0.78361976]\n",
            " [0.94097704]\n",
            " [0.98064065]]\n",
            "11500 [[0.02758238]\n",
            " [0.15433162]\n",
            " [0.28938475]\n",
            " [0.788547  ]\n",
            " [0.9440011 ]\n",
            " [0.9816648 ]]\n",
            "12000 [[0.02564473]\n",
            " [0.1513062 ]\n",
            " [0.27947327]\n",
            " [0.7932364 ]\n",
            " [0.9467992 ]\n",
            " [0.98262006]]\n",
            "12500 [[0.02389026]\n",
            " [0.14837316]\n",
            " [0.2701926 ]\n",
            " [0.79770505]\n",
            " [0.94939274]\n",
            " [0.98351157]]\n",
            "13000 [[0.02229777]\n",
            " [0.14553148]\n",
            " [0.26148725]\n",
            " [0.80196863]\n",
            " [0.9518006 ]\n",
            " [0.98434377]]\n",
            "13500 [[0.020849  ]\n",
            " [0.14278002]\n",
            " [0.25330955]\n",
            " [0.8060427 ]\n",
            " [0.95404035]\n",
            " [0.98512137]]\n",
            "14000 [[0.0195279 ]\n",
            " [0.14011618]\n",
            " [0.24561428]\n",
            " [0.8099395 ]\n",
            " [0.95612687]\n",
            " [0.9858483 ]]\n",
            "14500 [[0.01832066]\n",
            " [0.13753822]\n",
            " [0.23836297]\n",
            " [0.81367195]\n",
            " [0.9580739 ]\n",
            " [0.9865284 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "predicted = tf.cast(hypot > 0.5, dtype=tf.float32) #tf.cast는 0과 1로 아니면  True, False로 값이 나오게 된다.\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))"
      ],
      "metadata": {
        "id": "GvsalSOw_WKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(15000):\n",
        "  _, h = sess.run([train, hypot], feed_dict = {X : X_data, y : y_data})\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    print(step, h)\n",
        "\n",
        "h, c, a = sess.run([hypot, predicted, accuracy],\n",
        "                      feed_dict={X: X_data, y: y_data})\n",
        "print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3jXKkr88At",
        "outputId": "bc9e576f-1485-48d8-9516-b635abc76d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [[0.49636456]\n",
            " [0.56362   ]\n",
            " [0.22430338]\n",
            " [0.40580353]\n",
            " [0.3318245 ]\n",
            " [0.1669275 ]]\n",
            "500 [[0.2824809 ]\n",
            " [0.39835677]\n",
            " [0.56013143]\n",
            " [0.6719139 ]\n",
            " [0.7826947 ]\n",
            " [0.8688397 ]]\n",
            "1000 [[0.18340896]\n",
            " [0.28411198]\n",
            " [0.5867284 ]\n",
            " [0.6647659 ]\n",
            " [0.8159261 ]\n",
            " [0.9261146 ]]\n",
            "1500 [[0.14177568]\n",
            " [0.24174811]\n",
            " [0.57894385]\n",
            " [0.66994125]\n",
            " [0.8366402 ]\n",
            " [0.94411635]]\n",
            "2000 [[0.11812194]\n",
            " [0.22187023]\n",
            " [0.5589127 ]\n",
            " [0.6785132 ]\n",
            " [0.8516779 ]\n",
            " [0.9523039 ]]\n",
            "2500 [[0.10205659]\n",
            " [0.21079536]\n",
            " [0.5345456 ]\n",
            " [0.6881486 ]\n",
            " [0.8638082 ]\n",
            " [0.9570764 ]]\n",
            "3000 [[0.08989657]\n",
            " [0.20357147]\n",
            " [0.5091884 ]\n",
            " [0.697939  ]\n",
            " [0.87416685]\n",
            " [0.9604243 ]]\n",
            "3500 [[0.08008153]\n",
            " [0.19812688]\n",
            " [0.48436013]\n",
            " [0.70748395]\n",
            " [0.88327485]\n",
            " [0.9630968 ]]\n",
            "4000 [[0.07185748]\n",
            " [0.19353041]\n",
            " [0.4607419 ]\n",
            " [0.71661067]\n",
            " [0.8914059 ]\n",
            " [0.9654056 ]]\n",
            "4500 [[0.06481484]\n",
            " [0.18935217]\n",
            " [0.43860278]\n",
            " [0.72525537]\n",
            " [0.8987248 ]\n",
            " [0.9674868 ]]\n",
            "5000 [[0.05870416]\n",
            " [0.18539393]\n",
            " [0.41800666]\n",
            " [0.7334085 ]\n",
            " [0.905346  ]\n",
            " [0.9694028 ]]\n",
            "5500 [[0.05335732]\n",
            " [0.18156762]\n",
            " [0.39891607]\n",
            " [0.74108607]\n",
            " [0.91135657]\n",
            " [0.97118294]]\n",
            "6000 [[0.0486511 ]\n",
            " [0.17783685]\n",
            " [0.38124532]\n",
            " [0.74831474]\n",
            " [0.91682756]\n",
            " [0.9728429 ]]\n",
            "6500 [[0.04448996]\n",
            " [0.17418972]\n",
            " [0.3648907 ]\n",
            " [0.7551264 ]\n",
            " [0.92181915]\n",
            " [0.9743925 ]]\n",
            "7000 [[0.0407967 ]\n",
            " [0.17062438]\n",
            " [0.34974265]\n",
            " [0.7615521 ]\n",
            " [0.92638326]\n",
            " [0.9758386 ]]\n",
            "7500 [[0.03750777]\n",
            " [0.16714361]\n",
            " [0.33569646]\n",
            " [0.7676227 ]\n",
            " [0.93056554]\n",
            " [0.9771878 ]]\n",
            "8000 [[0.03456974]\n",
            " [0.16375066]\n",
            " [0.3226526 ]\n",
            " [0.7733664 ]\n",
            " [0.93440545]\n",
            " [0.978446  ]]\n",
            "8500 [[0.03193741]\n",
            " [0.16044834]\n",
            " [0.3105195 ]\n",
            " [0.7788086 ]\n",
            " [0.9379382 ]\n",
            " [0.979619  ]]\n",
            "9000 [[0.02957243]\n",
            " [0.15723926]\n",
            " [0.29921585]\n",
            " [0.7839743 ]\n",
            " [0.9411952 ]\n",
            " [0.9807128 ]]\n",
            "9500 [[0.02744178]\n",
            " [0.15412389]\n",
            " [0.28866553]\n",
            " [0.78888345]\n",
            " [0.94420296]\n",
            " [0.98173255]]\n",
            "10000 [[0.02551721]\n",
            " [0.15110229]\n",
            " [0.27880144]\n",
            " [0.7935557 ]\n",
            " [0.94698614]\n",
            " [0.9826836 ]]\n",
            "10500 [[0.02377445]\n",
            " [0.14817427]\n",
            " [0.26956365]\n",
            " [0.798009  ]\n",
            " [0.94956607]\n",
            " [0.98357093]]\n",
            "11000 [[0.02219245]\n",
            " [0.14533834]\n",
            " [0.26089826]\n",
            " [0.80225927]\n",
            " [0.951962  ]\n",
            " [0.98439956]]\n",
            "11500 [[0.02075296]\n",
            " [0.14259237]\n",
            " [0.2527559 ]\n",
            " [0.8063203 ]\n",
            " [0.9541907 ]\n",
            " [0.9851736 ]]\n",
            "12000 [[0.01944023]\n",
            " [0.13993451]\n",
            " [0.24509384]\n",
            " [0.8102057 ]\n",
            " [0.9562672 ]\n",
            " [0.9858971 ]]\n",
            "12500 [[0.01824039]\n",
            " [0.1373618 ]\n",
            " [0.23787153]\n",
            " [0.8139263 ]\n",
            " [0.95820475]\n",
            " [0.986574  ]]\n",
            "13000 [[0.01714144]\n",
            " [0.13487203]\n",
            " [0.2310547 ]\n",
            " [0.817494  ]\n",
            " [0.9600158 ]\n",
            " [0.98720795]]\n",
            "13500 [[0.01613291]\n",
            " [0.13246249]\n",
            " [0.22461128]\n",
            " [0.820918  ]\n",
            " [0.96171093]\n",
            " [0.9878021 ]]\n",
            "14000 [[0.01520546]\n",
            " [0.13012983]\n",
            " [0.2185117 ]\n",
            " [0.82420665]\n",
            " [0.9632996 ]\n",
            " [0.9883592 ]]\n",
            "14500 [[0.01435101]\n",
            " [0.12787172]\n",
            " [0.21273087]\n",
            " [0.8273694 ]\n",
            " [0.9647907 ]\n",
            " [0.98888224]]\n",
            "\n",
            "Hypothesis:  [[0.0135624 ]\n",
            " [0.12568536]\n",
            " [0.20724513]\n",
            " [0.8304133 ]\n",
            " [0.96619207]\n",
            " [0.98937374]] \n",
            "Correct (Y):  [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]] \n",
            "Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Softmax regression\n",
        "- 원래 2진 분류는 다중분류를 지원하지 않는다.\n",
        "- 굳이 한다면 2진 분류로 고양이 Y/N을 분류하고 그런 다음 개 Y/N을 분류...\n",
        "- 분류할 내용이 많다면? 엄청 복잡해진다.\n",
        "- softmax는 로지스틱 회귀의 한 종류로 확률을 이용해 한번에 다중분류해준다."
      ],
      "metadata": {
        "id": "Y2ZQdE-3CC0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], \n",
        "                                                        [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
        "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]"
      ],
      "metadata": {
        "id": "S6xQKsyQCf06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, 4])\n",
        "y = tf.placeholder(tf.float32, [None, 3])\n",
        "\n",
        "W = tf.Variable(tf.random.normal([4,3]))\n",
        "b = tf.Variable(tf.random.normal([3]))\n",
        "\n",
        "# 가설\n",
        "hypot = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "# 비용(잔차제곱합)\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypot), axis=1))\n",
        "\n",
        "# 훈련방법(그라디언트디센트 방식 모델링)\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(30000):\n",
        "  _, c = sess.run([train, cost], feed_dict = {X : X_data, y : y_data})\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    print(step, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnL4-MvbGlOq",
        "outputId": "1aea40f0-4b47-44b7-c540-aaaf35ad9110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.0735668\n",
            "500 0.62975764\n",
            "1000 0.53878325\n",
            "1500 0.4973822\n",
            "2000 0.4670771\n",
            "2500 0.4424629\n",
            "3000 0.42147097\n",
            "3500 0.40302578\n",
            "4000 0.38648945\n",
            "4500 0.37145394\n",
            "5000 0.35764343\n",
            "5500 0.34486336\n",
            "6000 0.33297116\n",
            "6500 0.32185748\n",
            "7000 0.31143722\n",
            "7500 0.30164024\n",
            "8000 0.29240885\n",
            "8500 0.2836937\n",
            "9000 0.27545178\n",
            "9500 0.26764613\n",
            "10000 0.2602433\n",
            "10500 0.2532138\n",
            "11000 0.24653067\n",
            "11500 0.24016985\n",
            "12000 0.23410971\n",
            "12500 0.22832984\n",
            "13000 0.22281235\n",
            "13500 0.21754009\n",
            "14000 0.21249804\n",
            "14500 0.20767173\n",
            "15000 0.2030485\n",
            "15500 0.19861594\n",
            "16000 0.19436304\n",
            "16500 0.19027978\n",
            "17000 0.18635589\n",
            "17500 0.1825831\n",
            "18000 0.1789529\n",
            "18500 0.17545782\n",
            "19000 0.17209066\n",
            "19500 0.16884452\n",
            "20000 0.16571355\n",
            "20500 0.16269156\n",
            "21000 0.15977332\n",
            "21500 0.15695365\n",
            "22000 0.15422794\n",
            "22500 0.15159142\n",
            "23000 0.14904033\n",
            "23500 0.14657012\n",
            "24000 0.14417776\n",
            "24500 0.14185914\n",
            "25000 0.13961121\n",
            "25500 0.13743109\n",
            "26000 0.1353155\n",
            "26500 0.13326181\n",
            "27000 0.13126764\n",
            "27500 0.12933011\n",
            "28000 0.12744708\n",
            "28500 0.12561615\n",
            "29000 0.12383574\n",
            "29500 0.12210319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### argmax\n",
        "#### 최대값이 어디에 있는지 그 위치를 알려준다.\n",
        "#### softmax는 값의 크기가 아니라 최대값의 위치가 중요.\n",
        "#### 왜냐면 one-hot encoding이 되어 데이터가 처리되어\n",
        "#### 정확한 결과 분류는 위치로 나타내야 하기 때문\n",
        "\n",
        "a = tf.constant([3, 10, 1])\n",
        "\n",
        "print(a.eval(session=sess))\n",
        "print(tf.argmax(a, 0).eval(session=sess))\n",
        "\n",
        "b = tf.constant([[5, 10, 17], [4, 50, 6]])\n",
        "print(tf.argmax(b, 1).eval(session=sess))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ4jK4ArJa4V",
        "outputId": "2455ccb3-c806-4c75-a5cc-864f048f53ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 3 10  1]\n",
            "1\n",
            "[2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = tf.argmax(hypot, 1)\n",
        "sess.run(preds, feed_dict={X:X_data, y:y_data})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRv5Qu60MHuX",
        "outputId": "06f5cf69-c3ed-4dcb-c01b-f1090ee1f5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 1, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = tf.equal(preds, tf.argmax(y, 1))\n",
        "sess.run(correct, feed_dict={X:X_data, y:y_data})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1YDGkfYNCp1",
        "outputId": "2a9f532d-66d4-4226-fa4c-57b3c04fcf42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
        "sess.run(accuracy, feed_dict={X:X_data, y:y_data})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AgpJcwmOSxS",
        "outputId": "a1054321-c706-4f33-b914-fc89cea7ba3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 테스트 : [1, 11, 7, 9]\n",
        "test = sess.run(hypot, feed_dict={X:[[1, 11, 7, 9]]})\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkA648eFOqk0",
        "outputId": "7f128758-9a50-4c5e-e1b7-1534b3edefa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4993673e-03, 9.9849832e-01, 2.2861498e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sess.run(tf.argmax(test, 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIrCUVoOP_FV",
        "outputId": "8918bd6f-72f5-41a4-f34c-e6c26338ea41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "pPHPxO77Q3Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. softmax_cross_entropy_with_logits()\n",
        "- 비용공식을 쓰지 않고 함수로 바로 처리 가능"
      ],
      "metadata": {
        "id": "ZPhDVM3yVUwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, 4])\n",
        "y = tf.placeholder(tf.float32, [None, 3])\n",
        "\n",
        "W = tf.Variable(tf.random.normal([4,3]))\n",
        "b = tf.Variable(tf.random.normal([3]))\n",
        "\n",
        "# 가설\n",
        "# hypot = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "# 비용(잔차제곱합)\n",
        "# cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypot), axis=1))\n",
        "\n",
        "#### 가설과 비용을 한번에\n",
        "logit = tf.matmul(X, W) + b\n",
        "hypot = tf.nn.softmax(logit)\n",
        "cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y_data)\n",
        "\n",
        "\n",
        "# 훈련방법(그라디언트 디센트 방식 모델링)\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(30000):\n",
        "  _, c = sess.run([train, cost], feed_dict = {X : X_data, y : y_data})\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    print(step, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsQP903UUGkE",
        "outputId": "c4cc56a9-4ea3-48f2-8557-b6b7541f2986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [5.38149214e+00 2.39877343e+00 2.62030125e-01 1.59544039e+01\n",
            " 2.14863281e+01 1.76934357e+01 4.25568105e-05 1.20400655e-05]\n",
            "500 [0.05343698 0.29213205 0.47261924 0.4640928  0.8226139  0.29898807\n",
            " 0.4536415  0.32521433]\n",
            "1000 [0.01133396 0.18442304 0.35299632 0.32683873 0.6039823  0.24927597\n",
            " 0.39943734 0.23043454]\n",
            "1500 [0.0040733  0.13950656 0.27659628 0.25309053 0.47510654 0.20726758\n",
            " 0.3535584  0.16808732]\n",
            "2000 [0.00184711 0.11254618 0.22586754 0.20583914 0.38968882 0.1755884\n",
            " 0.31549236 0.12685832]\n",
            "2500 [0.00096156 0.09411837 0.19013259 0.17291228 0.32944575 0.15165088\n",
            " 0.28398994 0.09875278]\n",
            "3000 [0.00055036 0.08066153 0.16375047 0.1487046  0.28493568 0.13317756\n",
            " 0.25774172 0.07892797]\n",
            "3500 [0.0003379  0.07040965 0.14355108 0.13020755 0.25082    0.11858285\n",
            " 0.23566063 0.06449331]\n",
            "4000 [2.1920186e-04 6.2355220e-02 1.2763341e-01 1.1564940e-01 2.2389068e-01\n",
            " 1.0680148e-01 2.1689092e-01 5.3685226e-02]\n",
            "4500 [1.4864294e-04 5.5874269e-02 1.1479325e-01 1.0391595e-01 2.0211910e-01\n",
            " 9.7109936e-02 2.0077766e-01 4.5394842e-02]\n",
            "5000 [1.04421888e-04 5.05564883e-02 1.04232654e-01 9.42737311e-02\n",
            " 1.84165761e-01 8.90071839e-02 1.86814621e-01 3.89009379e-02]\n",
            "5500 [7.5695032e-05 4.6121836e-02 9.5404916e-02 8.6218961e-02 1.6911489e-01\n",
            " 8.2136907e-02 1.7461370e-01 3.3721380e-02]\n",
            "6000 [5.6146000e-05 4.2372115e-02 8.7922238e-02 7.9396941e-02 1.5631939e-01\n",
            " 7.6240823e-02 1.6386886e-01 2.9524399e-02]\n",
            "6500 [4.2676016e-05 3.9163709e-02 8.1503548e-02 7.3548749e-02 1.4531016e-01\n",
            " 7.1126685e-02 1.5434201e-01 2.6076205e-02]\n",
            "7000 [3.3020428e-05 3.6389925e-02 7.5939998e-02 6.8483576e-02 1.3573918e-01\n",
            " 6.6649921e-02 1.4583999e-01 2.3208408e-02]\n",
            "7500 [2.5987287e-05 3.3969957e-02 7.1074054e-02 6.4056106e-02 1.2734345e-01\n",
            " 6.2699348e-02 1.3820964e-01 2.0797161e-02]\n",
            "8000 [2.0742202e-05 3.1841744e-02 6.6783629e-02 6.0154520e-02 1.1991868e-01\n",
            " 5.9187241e-02 1.3132581e-01 1.8750207e-02]\n",
            "8500 [1.68083679e-05 2.99562514e-02 6.29731715e-02 5.66921383e-02\n",
            " 1.13307714e-01 5.60453795e-02 1.25084132e-01 1.69971418e-02]\n",
            "9000 [1.37089746e-05 2.82752346e-02 5.95677309e-02 5.35990186e-02\n",
            " 1.07383043e-01 5.32182902e-02 1.19401485e-01 1.54841151e-02]\n",
            "9500 [1.13248179e-05 2.67677009e-02 5.65062650e-02 5.08204065e-02\n",
            " 1.02043726e-01 5.06606363e-02 1.14207365e-01 1.41689731e-02]\n",
            "10000 [9.53669769e-06 2.54084021e-02 5.37396818e-02 4.83112372e-02\n",
            " 9.72076207e-02 4.83362265e-02 1.09441951e-01 1.30184945e-02]\n",
            "10500 [7.9869906e-06 2.4177006e-02 5.1227983e-02 4.6034180e-02 9.2806816e-02\n",
            " 4.6214722e-02 1.0505367e-01 1.2005807e-02]\n",
            "11000 [6.7949063e-06 2.3056865e-02 4.8937928e-02 4.3958664e-02 8.8785008e-02\n",
            " 4.4270631e-02 1.0099999e-01 1.1109756e-02]\n",
            "11500 [5.8412379e-06 2.2033306e-02 4.6841361e-02 4.2059444e-02 8.5095622e-02\n",
            " 4.2482488e-02 9.7245045e-02 1.0312960e-02]\n",
            "12000 [5.0067774e-06 2.1094844e-02 4.4914901e-02 4.0315542e-02 8.1699550e-02\n",
            " 4.0831916e-02 9.3756564e-02 9.6008945e-03]\n",
            "12500 [4.2915253e-06 2.0231320e-02 4.3138530e-02 3.8709089e-02 7.8562498e-02\n",
            " 3.9304119e-02 9.0508968e-02 8.9622848e-03]\n",
            "13000 [3.8146900e-06 1.9434277e-02 4.1496243e-02 3.7223760e-02 7.5656876e-02\n",
            " 3.7886705e-02 8.7476045e-02 8.3868671e-03]\n",
            "13500 [3.3378547e-06 1.8696506e-02 3.9973248e-02 3.5846408e-02 7.2956689e-02\n",
            " 3.6567733e-02 8.4639288e-02 7.8665921e-03]\n",
            "14000 [2.8610189e-06 1.8011581e-02 3.8556669e-02 3.4566548e-02 7.0442088e-02\n",
            " 3.5337124e-02 8.1979208e-02 7.3946812e-03]\n",
            "14500 [2.5033919e-06 1.7374104e-02 3.7236165e-02 3.3373941e-02 6.8093888e-02\n",
            " 3.4185067e-02 7.9481713e-02 6.9652842e-03]\n",
            "15000 [2.2649740e-06 1.6779711e-02 3.6002584e-02 3.2259442e-02 6.5897524e-02\n",
            " 3.3105262e-02 7.7129729e-02 6.5731267e-03]\n",
            "15500 [2.0265559e-06 1.6223557e-02 3.4846690e-02 3.1217301e-02 6.3836932e-02\n",
            " 3.2092638e-02 7.4912883e-02 6.2142247e-03]\n",
            "16000 [1.7881378e-06 1.5702648e-02 3.3762757e-02 3.0238615e-02 6.1901506e-02\n",
            " 3.1140113e-02 7.2818160e-02 5.8847032e-03]\n",
            "16500 [1.6689287e-06 1.5213640e-02 3.2743994e-02 2.9318823e-02 6.0078748e-02\n",
            " 3.0241854e-02 7.0837751e-02 5.5815089e-03]\n",
            "17000 [1.4305105e-06 1.4753524e-02 3.1783778e-02 2.8453453e-02 5.8360841e-02\n",
            " 2.9392794e-02 6.8961687e-02 5.3018201e-03]\n",
            "17500 [1.3113013e-06 1.4319990e-02 3.0877424e-02 2.7637428e-02 5.6738205e-02\n",
            " 2.8589470e-02 6.7182422e-02 5.0434032e-03]\n",
            "18000 [1.1920922e-06 1.3911192e-02 3.0021848e-02 2.6865652e-02 5.5202164e-02\n",
            " 2.7830599e-02 6.5491535e-02 4.8037837e-03]\n",
            "18500 [1.0728830e-06 1.3524337e-02 2.9211387e-02 2.6136363e-02 5.3747024e-02\n",
            " 2.7110601e-02 6.3884236e-02 4.5815497e-03]\n",
            "19000 [9.53673862e-07 1.31585095e-02 2.84442976e-02 2.54436173e-02\n",
            " 5.23653366e-02 2.64265407e-02 6.23538755e-02 4.37481515e-03]\n",
            "19500 [8.3446469e-07 1.2811497e-02 2.7715234e-02 2.4787487e-02 5.1054459e-02\n",
            " 2.5775352e-02 6.0894195e-02 4.1821646e-03]\n",
            "20000 [8.3446469e-07 1.2481670e-02 2.7021956e-02 2.4164788e-02 4.9806852e-02\n",
            " 2.5154667e-02 5.9501797e-02 4.0025371e-03]\n",
            "20500 [7.1525551e-07 1.2168811e-02 2.6363032e-02 2.3571394e-02 4.8618101e-02\n",
            " 2.4564184e-02 5.8171008e-02 3.8343971e-03]\n",
            "21000 [7.1525551e-07 1.1870579e-02 2.5734693e-02 2.3006886e-02 4.7484096e-02\n",
            " 2.4000935e-02 5.6898583e-02 3.6770368e-03]\n",
            "21500 [5.9604628e-07 1.1586988e-02 2.5136413e-02 2.2467123e-02 4.6403084e-02\n",
            " 2.3464154e-02 5.5678662e-02 3.5292739e-03]\n",
            "22000 [5.96046277e-07 1.13158105e-02 2.45636031e-02 2.19540074e-02\n",
            " 4.53690961e-02 2.29487531e-02 5.45123369e-02 3.39075597e-03]\n",
            "22500 [5.9604628e-07 1.1057410e-02 2.4017580e-02 2.1461980e-02 4.4380795e-02\n",
            " 2.2455119e-02 5.3393248e-02 3.2605364e-03]\n",
            "23000 [4.7683704e-07 1.0810147e-02 2.3494545e-02 2.0992242e-02 4.3434318e-02\n",
            " 2.1982463e-02 5.2318610e-02 3.1377864e-03]\n",
            "23500 [4.7683704e-07 1.0573674e-02 2.2993840e-02 2.0542134e-02 4.2527158e-02\n",
            " 2.1530233e-02 5.1286194e-02 3.0220337e-03]\n",
            "24000 [4.7683704e-07 1.0347293e-02 2.2514096e-02 2.0110520e-02 4.1656569e-02\n",
            " 2.1095894e-02 5.0293863e-02 2.9129237e-03]\n",
            "24500 [3.5762781e-07 1.0130064e-02 2.2053365e-02 1.9696834e-02 4.0820926e-02\n",
            " 2.0678762e-02 4.9339239e-02 2.8097460e-03]\n",
            "25000 [3.5762781e-07 9.9217575e-03 2.1611201e-02 1.9300049e-02 4.0017340e-02\n",
            " 2.0277580e-02 4.8420843e-02 2.7121450e-03]\n",
            "25500 [3.5762781e-07 9.7220270e-03 2.1186931e-02 1.8917847e-02 3.9246008e-02\n",
            " 1.9891895e-02 4.7534794e-02 2.6196470e-03]\n",
            "26000 [3.5762781e-07 9.5298141e-03 2.0777781e-02 1.8551650e-02 3.8503215e-02\n",
            " 1.9519506e-02 4.6681412e-02 2.5320156e-03]\n",
            "26500 [3.5762781e-07 9.3451217e-03 2.0384928e-02 1.8198308e-02 3.7788797e-02\n",
            " 1.9160658e-02 4.5858044e-02 2.4487767e-03]\n",
            "27000 [2.3841855e-07 9.1673648e-03 2.0006647e-02 1.7858427e-02 3.7099369e-02\n",
            " 1.8816192e-02 4.5061916e-02 2.3698122e-03]\n",
            "27500 [2.3841855e-07 8.9960732e-03 1.9641194e-02 1.7531432e-02 3.6433835e-02\n",
            " 1.8482840e-02 4.4295609e-02 2.2947663e-03]\n",
            "28000 [2.3841855e-07 8.8310158e-03 1.9289410e-02 1.7215697e-02 3.5793275e-02\n",
            " 1.8161785e-02 4.3552332e-02 2.2231645e-03]\n",
            "28500 [2.3841855e-07 8.6719589e-03 1.8949781e-02 1.6910996e-02 3.5173941e-02\n",
            " 1.7851986e-02 4.2834207e-02 2.1551256e-03]\n",
            "29000 [2.3841855e-07 8.5185496e-03 1.8622324e-02 1.6616050e-02 3.4576103e-02\n",
            " 1.7551932e-02 4.2139445e-02 2.0901754e-03]\n",
            "29500 [2.3841855e-07 8.3698453e-03 1.8304709e-02 1.6333332e-02 3.3997729e-02\n",
            " 1.7260924e-02 4.1467994e-02 2.0283142e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = sess.run(hypot, feed_dict={X:[[1, 11, 7, 9]]})\n",
        "print(test)\n",
        "print(sess.run(tf.argmax(test, 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oREGtKtWwBP",
        "outputId": "bd481c2b-b161-45f2-b133-d9ec6801ee84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.3057336e-09 1.0000000e+00 1.0381680e-09]]\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "1OBhegKoXAnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PqMsHnjIYHn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사례 1. 동물분류"
      ],
      "metadata": {
        "id": "EWRWpgCmYIPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H02t4sKMYKuQ",
        "outputId": "d80fc0a1-0355-4961-8e7b-ff09a7b29c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xy = np.loadtxt(\"/content/data/data-04-zoo.csv\", dtype=np.float32, delimiter=\",\")\n",
        "\n",
        "X_data = xy[:, :-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "print(X_data.shape, y_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb6oqhjiYv4U",
        "outputId": "20f5e0c0-c4c8-450b-d658-865ee1afa91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101, 16) (101, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, 16])\n",
        "y = tf.placeholder(tf.int32, [None, 1])\n",
        "\n",
        "y_one_hot = tf.one_hot(y, 7)  # one hot : tf.one_hot은 2차원을 3차원으로 만드는 특징 있어.\n",
        "y_one_hot = tf.reshape(y_one_hot, [-1, 7]) # 그래서 재조정해줘야한다.\n",
        "\n",
        "W = tf.Variable(tf.random.normal([16,7]))\n",
        "b = tf.Variable(tf.random.normal([7]))\n",
        "\n",
        "logit = tf.matmul(X, W) + b\n",
        "hypot = tf.nn.softmax(logit)\n",
        "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y_one_hot)\n",
        "cost = tf.reduce_mean(cost_i)\n",
        "\n",
        "# 훈련방법(그라디언트디센트 방식 모델링)\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# preds, acc modeling\n",
        "prediction = tf.argmax(hypot, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(y_one_hot, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "for step in range(30001):\n",
        "  _, c = sess.run([train, cost], feed_dict = {X : X_data, y : y_data})\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    print(step, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEH4Ceu0bNui",
        "outputId": "d6a48075-3fb9-4662-a2df-7e26a4fe4c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 9.498595\n",
            "500 0.9213073\n",
            "1000 0.65820026\n",
            "1500 0.52733916\n",
            "2000 0.4434341\n",
            "2500 0.38292816\n",
            "3000 0.3364861\n",
            "3500 0.29948586\n",
            "4000 0.2692616\n",
            "4500 0.2441106\n",
            "5000 0.22287194\n",
            "5500 0.20472075\n",
            "6000 0.1890527\n",
            "6500 0.17541406\n",
            "7000 0.16345611\n",
            "7500 0.1529055\n",
            "8000 0.14354426\n",
            "8500 0.13519593\n",
            "9000 0.12771596\n",
            "9500 0.12098467\n",
            "10000 0.11490231\n",
            "10500 0.10938502\n",
            "11000 0.104362\n",
            "11500 0.09977305\n",
            "12000 0.09556696\n",
            "12500 0.0916997\n",
            "13000 0.088133454\n",
            "13500 0.08483562\n",
            "14000 0.08177796\n",
            "14500 0.07893565\n",
            "15000 0.076287284\n",
            "15500 0.07381412\n",
            "16000 0.07149946\n",
            "16500 0.06932889\n",
            "17000 0.06728931\n",
            "17500 0.06536935\n",
            "18000 0.06355895\n",
            "18500 0.061848886\n",
            "19000 0.060231086\n",
            "19500 0.05869828\n",
            "20000 0.057243925\n",
            "20500 0.05586213\n",
            "21000 0.05454757\n",
            "21500 0.053295396\n",
            "22000 0.05210121\n",
            "22500 0.05096107\n",
            "23000 0.049871415\n",
            "23500 0.048828844\n",
            "24000 0.047830373\n",
            "24500 0.04687322\n",
            "25000 0.045954805\n",
            "25500 0.04507283\n",
            "26000 0.044225164\n",
            "26500 0.04340981\n",
            "27000 0.042624857\n",
            "27500 0.041868698\n",
            "28000 0.041139722\n",
            "28500 0.040436447\n",
            "29000 0.039757516\n",
            "29500 0.039101746\n",
            "30000 0.038467832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = sess.run(prediction, feed_dict={X:X_data})"
      ],
      "metadata": {
        "id": "jtZ2PpREvZm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fhir4VEv8m4",
        "outputId": "6b907651-31b4-4a5e-ef10-1bbaec765f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 3 0 0 0 0 3 3 0 0 1 3 6 6 6 1 0 3 0 1 1 0 1 5 4 4 0 0 0 5 0 0 1 3 0 0\n",
            " 1 3 5 5 1 5 1 0 0 6 0 0 0 0 5 4 6 0 0 1 1 1 1 3 3 2 0 0 0 0 0 0 0 0 1 6 3\n",
            " 0 0 2 6 1 1 2 6 3 1 0 6 3 1 5 4 2 2 3 0 0 1 0 5 0 6 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.run(accuracy, feed_dict={X:X_data, y:y_data})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXdEOaay04v6",
        "outputId": "d3daebeb-790b-4736-9d51-857f9a0e8d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tmYUUmks1Hmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사례 2. MNIST 이미지 데이터셋\n"
      ],
      "metadata": {
        "id": "ROSQlLd22s0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "metadata": {
        "id": "GZXezZv72y6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets(\"/content/data/mnist/\", one_hot = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbEn-ItH3y7H",
        "outputId": "bcf7e3cb-1eaf-4c24-9d71-9dc90c5cc6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-52f79432d5db>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /content/data/mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /content/data/mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /content/data/mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /content/data/mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(mnist))\n",
        "print(mnist.train.labels)\n",
        "print(mnist.train.num_examples)\n",
        "print(mnist.test.num_examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtaOM4Z-4AmD",
        "outputId": "085c4210-aa67-47e6-b83d-4b55f84cfcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n",
            "55000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mnist.train.images.shape)\n",
        "print(mnist.train.labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF3m0Vnz5KlT",
        "outputId": "963d1af8-dab5-4ebd-83e2-103c962e7013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(55000, 784)\n",
            "(55000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.int32, [None, 10])\n",
        "\n",
        "W = tf.Variable(tf.random.normal([784, 10]))\n",
        "b = tf.Variable(tf.random.normal([10]))\n",
        "\n",
        "logit = tf.matmul(X, W) + b\n",
        "hypot = tf.nn.softmax(logit)\n",
        "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y)\n",
        "cost = tf.reduce_mean(cost_i)\n",
        "\n",
        "# 훈련방법(그라디언트디센트 방식 모델링)\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# preds, acc modeling\n",
        "prediction = tf.argmax(hypot, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "for step in range(1001):\n",
        "  _, c = sess.run([train, cost], feed_dict = {X : mnist.train.images, y : mnist.train.labels})\n",
        "\n",
        "  if step % 50 == 0:\n",
        "    print(step, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDCfUx_V8xMI",
        "outputId": "1e2269a6-c2b5-49cc-c9df-4f30608da4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 14.377724\n",
            "50 2.042103\n",
            "100 1.388173\n",
            "150 1.1440251\n",
            "200 1.0068573\n",
            "250 0.91552246\n",
            "300 0.8488042\n",
            "350 0.79719114\n",
            "400 0.7557126\n",
            "450 0.72144663\n",
            "500 0.692531\n",
            "550 0.6677156\n",
            "600 0.646126\n",
            "650 0.62712806\n",
            "700 0.6102476\n",
            "750 0.59512186\n",
            "800 0.581468\n",
            "850 0.5690622\n",
            "900 0.5577253\n",
            "950 0.54731196\n",
            "1000 0.5377027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = sess.run(prediction, feed_dict={X:mnist.test.images})\n",
        "sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpiEumXj88GX",
        "outputId": "f3974ce6-3381-4286-f3cd-cd83689fecac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8756"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "8RmVNOy2Fdvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### mini-batch 활용해보기\n",
        "\n",
        "training_epoch = 150\n",
        "batch_size = 100\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for epoch in range(training_epoch):\n",
        "  total_batch = int(mnist.train.num_examples / batch_size)\n",
        "  avg_cost = 0\n",
        "\n",
        "  for i in range(total_batch):\n",
        "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "    \n",
        "    _, c = sess.run([train, cost], feed_dict = {X : batch_x, y : batch_y})\n",
        "\n",
        "    avg_cost += c / total_batch\n",
        "\n",
        "  print(epoch, avg_cost)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxb8IVcUAb5U",
        "outputId": "aa9c4d9c-0c01-40e1-82ce-09e5f6865c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.2894911776347584\n",
            "1 0.623784220828251\n",
            "2 0.5231581924178385\n",
            "3 0.46941496765071694\n",
            "4 0.43324000561779213\n",
            "5 0.41025176252831086\n",
            "6 0.3894206860932436\n",
            "7 0.3749512304636565\n",
            "8 0.36250294008038253\n",
            "9 0.35187102806839105\n",
            "10 0.34230279971252786\n",
            "11 0.3346638331697745\n",
            "12 0.3277166316319597\n",
            "13 0.32169956069100986\n",
            "14 0.31748263333331467\n",
            "15 0.312095958441496\n",
            "16 0.30828167439861687\n",
            "17 0.30395480533892466\n",
            "18 0.30071854680776594\n",
            "19 0.29763523723591434\n",
            "20 0.29492217000235227\n",
            "21 0.29208120249889136\n",
            "22 0.2895728697695518\n",
            "23 0.2874301116033034\n",
            "24 0.28484273271127186\n",
            "25 0.28287231914699074\n",
            "26 0.28157277598977054\n",
            "27 0.2799070935899558\n",
            "28 0.2773297780142589\n",
            "29 0.2760806230794298\n",
            "30 0.2755731570720674\n",
            "31 0.27422003372149045\n",
            "32 0.2729640596292233\n",
            "33 0.27158836233344935\n",
            "34 0.2697848037359388\n",
            "35 0.2690279626033523\n",
            "36 0.26802920497276533\n",
            "37 0.2674723100797694\n",
            "38 0.26631174099716276\n",
            "39 0.2647810330445116\n",
            "40 0.264835227484053\n",
            "41 0.26376237801530117\n",
            "42 0.26286132838238363\n",
            "43 0.2626670485328545\n",
            "44 0.26137011662125587\n",
            "45 0.2604872103712778\n",
            "46 0.2597356061095544\n",
            "47 0.259178973314437\n",
            "48 0.25866414034908447\n",
            "49 0.25846994440663945\n",
            "50 0.25755740288983686\n",
            "51 0.25648329163139494\n",
            "52 0.2567631403695454\n",
            "53 0.25585918149487547\n",
            "54 0.25512381958690533\n",
            "55 0.254671688188206\n",
            "56 0.2542795867134224\n",
            "57 0.25392510283399694\n",
            "58 0.25375717156312655\n",
            "59 0.25340906878763975\n",
            "60 0.25327479215508136\n",
            "61 0.2523968116397205\n",
            "62 0.25201598706570555\n",
            "63 0.25102387457408676\n",
            "64 0.2503570805219083\n",
            "65 0.2508899753202093\n",
            "66 0.2506328865479338\n",
            "67 0.25000341185791947\n",
            "68 0.2502208474278452\n",
            "69 0.2490969701653178\n",
            "70 0.24867630008946767\n",
            "71 0.24839156210422517\n",
            "72 0.24839023439044317\n",
            "73 0.24812198664654386\n",
            "74 0.24740313498811276\n",
            "75 0.24810993707992837\n",
            "76 0.24757386478510782\n",
            "77 0.24735941015183932\n",
            "78 0.24688336185433643\n",
            "79 0.2463192403587427\n",
            "80 0.24624086015603747\n",
            "81 0.24642150607976057\n",
            "82 0.2460338462482799\n",
            "83 0.2459360185807403\n",
            "84 0.24555944867432108\n",
            "85 0.24463621012189182\n",
            "86 0.244677333642136\n",
            "87 0.24440384705635632\n",
            "88 0.24420031995935892\n",
            "89 0.24359160810709\n",
            "90 0.24431107379496111\n",
            "91 0.24380708489905695\n",
            "92 0.24354621112346658\n",
            "93 0.24306344841014244\n",
            "94 0.24347715291109942\n",
            "95 0.24303309960798755\n",
            "96 0.24306620285592326\n",
            "97 0.24285243232141837\n",
            "98 0.2419943419166586\n",
            "99 0.2424786185540937\n",
            "100 0.2418143352866174\n",
            "101 0.24258575771342644\n",
            "102 0.2411059141700919\n",
            "103 0.2414184209298005\n",
            "104 0.24175995563241576\n",
            "105 0.24105380885980343\n",
            "106 0.24114945683966985\n",
            "107 0.24160095701840803\n",
            "108 0.24120552837848672\n",
            "109 0.24057037184184232\n",
            "110 0.2408703947338191\n",
            "111 0.2404989461600781\n",
            "112 0.2412443490732801\n",
            "113 0.24009363807060477\n",
            "114 0.2400524908033287\n",
            "115 0.23913867256858135\n",
            "116 0.2397529676556587\n",
            "117 0.23945021694356736\n",
            "118 0.23928367299112396\n",
            "119 0.23937742017209532\n",
            "120 0.23964390957220033\n",
            "121 0.23915951017629017\n",
            "122 0.2389872419766404\n",
            "123 0.23916651563210936\n",
            "124 0.23898540049791359\n",
            "125 0.23806195931001126\n",
            "126 0.23923270640048103\n",
            "127 0.23843658371405166\n",
            "128 0.23817596442320138\n",
            "129 0.23813450313427248\n",
            "130 0.23696312283927748\n",
            "131 0.23763727226040585\n",
            "132 0.2374773260138251\n",
            "133 0.23768148217688914\n",
            "134 0.23763370182026516\n",
            "135 0.2377729732204567\n",
            "136 0.2376719167422165\n",
            "137 0.23803810526024205\n",
            "138 0.23714903309941296\n",
            "139 0.23717035712166284\n",
            "140 0.23703919911926463\n",
            "141 0.23716178602115692\n",
            "142 0.23689831323244384\n",
            "143 0.23697829331864018\n",
            "144 0.23618788410316843\n",
            "145 0.23682591705837\n",
            "146 0.23680128461935313\n",
            "147 0.2359665210477331\n",
            "148 0.23608766100623402\n",
            "149 0.2363082153417848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = sess.run(prediction, feed_dict={X:mnist.test.images})\n",
        "sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUgM6asKGzkd",
        "outputId": "4785a5bb-001f-4217-ebf5-66bf1fcf2f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9228"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "r = random.randint(0, mnist.test.num_examples-1)\n",
        "r"
      ],
      "metadata": {
        "id": "i3jTsQPwLWYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b53d1b3-5f9a-4940-9e62-46bdcf6566f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5420"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lable : \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__SbtGB22p3o",
        "outputId": "34321190-3ba1-4a69-e8dc-77a4b37d0987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lable :  [5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(mnist.test.images[r:r+1].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "eGb3eqr82_pN",
        "outputId": "34ec83c6-5738-4d2b-b551-2ba2b920fc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efed2a1ed90>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"prediction : \", sess.run(tf.argmax(hypot, 1), feed_dict={X:mnist.test.images[r:r+1]}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hrU7-WZ394W",
        "outputId": "001e0588-8b43-4d5b-ade9-cc721d1f8367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction :  [5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "metadata": {
        "id": "3QCWC6afLCXz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}